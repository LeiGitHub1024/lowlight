{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://zhuanlan.zhihu.com/p/261701780"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg # mpimg 用于读取图片\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.util import random_noise\n",
    "from skimage.metrics import peak_signal_noise_ratio\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data \n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import STL10\n",
    "# import hiddenlayer as hl\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先，从上述链接中下载完STL10的数据集后，开始对数据做预处理。STL10中都是96*96的RGB图片，训练集放在了train_X.bin的文件中，可以用做自编码器的无监督学习，下面是第数据的预处理：\n",
    "def read_image(data_path):\n",
    "    with open(data_path, \"rb\") as f:\n",
    "        data1 = np.fromfile(f, dtype=np.uint8)\n",
    "        # 塑形成[batch, c, h, w]\n",
    "        images = np.reshape(data1, [-1, 3, 96, 96])\n",
    "        # 图像转化为RGB(即最后一个维度是通道维度)的形式，方便使用matplotlib进行可视化\n",
    "        images = np.transpose(images, [0, 3, 2, 1])\n",
    "    return images / 255\n",
    "data_path = \"./data/stl10_binary/train_X.bin\"\n",
    "images = read_image(data_path)\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面定义一个函数，为干净的图片添加高斯噪音，这部分添加了噪音的数据，将成为自编码器的输入。其中的random_noise是属于skimage.util下的一个方法。\n",
    "\n",
    "def gaussian_noise(images, sigma):\n",
    "    \"\"\"sigma: 噪声标准差\"\"\"\n",
    "    sigma2 = sigma**2 / (255 ** 2)   # 噪声方差\n",
    "    images_noisy = np.zeros_like(images)\n",
    "    for ii in range(images.shape[0]):\n",
    "        image = images[ii]\n",
    "        # 使用skimage中的函数增加噪音\n",
    "        noise_im = random_noise(image, mode=\"gaussian\", var=sigma2, clip=True)\n",
    "        images_noisy[ii] = noise_im\n",
    "    return images_noisy\n",
    "def gaussian_gamma(images, sigma):\n",
    "    \"\"\"sigma: 噪声标准差, gamma: gamma矫正\"\"\"\n",
    "    sigma2 = sigma**2 / (255 ** 2)   # 噪声方差\n",
    "    res = np.zeros_like(images)\n",
    "    for ii in tqdm(range(images.shape[0])):\n",
    "        image = images[ii]\n",
    "        # 使用skimage中的函数增加噪音,gamma,在1.5-4.5之间\n",
    "        gamma = random.uniform(2.5,3)\n",
    "        noise_im = random_noise(image, mode=\"gaussian\", var=sigma2, clip=True)\n",
    "        # 使用gamma来让图像变暗\n",
    "        gamma_noise_im = np.power(noise_im, gamma)\n",
    "\n",
    "        res[ii] = gamma_noise_im\n",
    "    return res\n",
    "\n",
    "images_noise = gaussian_gamma(images, 25,)\n",
    "print(\"image_noise:\", images_noise.min(), \"~\", images_noise.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面可视化一些添加了噪音后的图片，其中iamges[ii, ...]中的...是numpy库中的语法糖，作用等价于若干个:,:,:的组合，也就是数组剩余的每个维度都全取。\n",
    "plt.figure(figsize=[6, 6])\n",
    "for ii in np.arange(36):\n",
    "    plt.subplot(6, 6, ii + 1)\n",
    "    plt.imshow(images[ii, ...])\n",
    "    plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# 带噪音的图片\n",
    "plt.figure(figsize=[6, 6])\n",
    "for ii in np.arange(36):\n",
    "    plt.subplot(6, 6, ii + 1)\n",
    "    plt.imshow(images_noise[ii, ...])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据与处理完后，老规矩，构建loader，以便于训练时直接拿出一个batch的数据出来训练。\n",
    "# 首先将数据集切分为训练集和验证集，并转换为torch张量。\n",
    "\n",
    "# 数据集准备为PyTorch可用的形式，转化为[样本, 通道, 高, 宽]\n",
    "data_X = np.transpose(images_noise, (0, 3, 2, 1))\n",
    "data_Y = np.transpose(images, (0, 3, 2, 1))\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(data_X, data_Y, \n",
    "                                                  test_size=0.2, random_state=123)\n",
    "# 转化为torch张量\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "# 将X与y整合在一起\n",
    "train_data = Data.TensorDataset(X_train, y_train)\n",
    "val_data = Data.TensorDataset(X_val, y_val)\n",
    "\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"y_train.shape:\", y_train.shape)\n",
    "print(\"X_val.shape:\", X_val.shape)\n",
    "print(\"y_val.shape:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据加载器\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = Data.DataLoader(\n",
    "    dataset=val_data,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建网络\n",
    "# 接下来定义一个DenoiseAutoEncoder类，其中包括了自编码器的encoder和decoder，forward返回原数据编码后的特征映射和特征映射解码后的重构图像。\n",
    "class DenoiseAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenoiseAutoEncoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.Encoder = nn.Sequential(\n",
    "            # param [input_c, output_c, kernel_size, stride, padding]\n",
    "            nn.Conv2d(3, 64, 3, 1, 1),   # [, 64, 96, 96]\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, 3, 1, 1), # [, 64, 96, 96]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),             # [, 64, 48, 48]\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, 3, 1, 1),  # [, 64, 48, 48]\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 128, 3, 1, 1), # [, 128, 48, 48]\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 128, 3, 1, 1), # [, 128, 48, 48]\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 256, 3, 1, 1), # [, 256, 48, 48]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),                 # [, 256, 24, 24]\n",
    "            nn.BatchNorm2d(256)   \n",
    "        )\n",
    "        \n",
    "        # decoder\n",
    "        self.Decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 3 ,1, 1),   # [, 128, 24, 24]\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ConvTranspose2d(128, 128, 3, 2, 1, 1),   # [, 128, 48, 48]\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ConvTranspose2d(128, 64, 3, 1, 1),    # [, 64, 48, 48]\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ConvTranspose2d(64, 32, 3, 1, 1),      # [, 32, 48, 48]\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ConvTranspose2d(32, 32, 3, 1, 1),      # [, 32, 48, 48]\n",
    "            nn.ConvTranspose2d(32, 16, 3, 2, 1, 1),  # [, 16, 96, 96]\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ConvTranspose2d(16, 3, 3, 1, 1),         # [, 3, 96, 96]\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoder = self.Encoder(x)\n",
    "        decoder = self.Decoder(encoder)\n",
    "        return encoder, decoder\n",
    "        \n",
    "# 输出网络结构\n",
    "DAEmodel = DenoiseAutoEncoder().to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchviz import make_dot\n",
    "# x = torch.randn(1, 3, 96, 96).requires_grad_(True)  # 模拟输入\n",
    "# _, y = DAEmodel(x)\n",
    "\n",
    "# vis_net = make_dot(y, params=dict(list(DAEmodel.named_parameters()) + [(\"x\", x)]))\n",
    "# vis_net.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有了网络和数据加载器后，可以开始训练了，其中，我们使用hiddenlayer库来做训练过程动态可视化：\n",
    "LR = 0.0002\n",
    "epoch_num = 10\n",
    "optimizer = optim.Adam(DAEmodel.parameters(), lr=LR)\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "history = hl.History()\n",
    "canvas = hl.Canvas()\n",
    "\n",
    "train_num, val_num = 0, 0\n",
    "\n",
    "for epoch in tqdm(range(epoch_num)):\n",
    "    train_loss_epoch, val_loss_epoch = 0, 0\n",
    "    \n",
    "    # 训练\n",
    "    for step, (b_x, b_y) in enumerate(train_loader):\n",
    "        DAEmodel.train()\n",
    "        \n",
    "        _, output = DAEmodel(b_x)\n",
    "        loss = loss_func(output, b_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_epoch += loss.item() * b_x.size(0)\n",
    "        train_num += b_x.size(0)\n",
    "    \n",
    "    # 验证\n",
    "    for step, (b_x, b_y) in enumerate(val_loader):\n",
    "        DAEmodel.eval()\n",
    "        _, output = DAEmodel(b_x)\n",
    "        loss = loss_func(output, b_y)\n",
    "        val_loss_epoch += loss.item() * b_x.size(0)\n",
    "        val_num += b_x.size(0)\n",
    "    \n",
    "    # 计算一个epoch的损失\n",
    "    train_loss = train_loss_epoch / train_num\n",
    "    val_loss = val_loss_epoch / val_num\n",
    "    \n",
    "    # 记录hiddenlayer的记录并动态可视化训练过程\n",
    "    history.log(epoch, train_loss = train_loss, val_loss = val_loss)\n",
    "    with canvas:\n",
    "        canvas.draw_plot([\n",
    "            history[\"train_loss\"],\n",
    "            history[\"val_loss\"]\n",
    "        ])\n",
    "    #保存模型\n",
    "    torch.save(DAEmodel.state_dict(), \"autodecode.mdl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网络训练完后，我们可以随便挑一张图片来测试一下去噪的效果如何，此处我们使用PSNR（峰值信噪比）来度量干净的原图和自编码器输出的去噪图之间的相似性，PSNR越大说明两个图片之间越相似。\n",
    "imageindex = 1\n",
    "im = X_val[imageindex, ...]\n",
    "im = im.unsqueeze(0)\n",
    "im_noise = np.transpose(im.cpu().data.numpy(), (0, 3, 2, 1))\n",
    "im_noise = im_noise[0, ...]\n",
    "# 去噪\n",
    "DAEmodel.eval()\n",
    "_, output = DAEmodel(im)\n",
    "im_denoise= np.transpose(output.cpu().data.numpy(), (0, 3, 2, 1))\n",
    "im_denoise = im_denoise[0, ...]\n",
    "# 输出\n",
    "im = y_val[imageindex, ...]\n",
    "im_origin = im.unsqueeze(0)\n",
    "im_origin = np.transpose(im_origin.cpu().data.numpy(), (0, 3, 2, 1))\n",
    "im_origin = im_origin[0, ...]\n",
    "\n",
    "# 计算去噪后的PSNR\n",
    "print(\"加躁后的PSNR:\", peak_signal_noise_ratio(im_origin, im_noise))\n",
    "print(\"去噪后的PSNR:\", peak_signal_noise_ratio(im_origin, im_denoise))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将图片可视化\n",
    "plt.figure(figsize=[12, 4])\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(im_origin)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Origin image\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(im_noise)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Noise image $\\sigma=30$\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(im_denoise)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Denoise image\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 接下来，我们试着在整个验证集上计算模型的得到的去噪图片与带噪图片相比，PSNR的平均提升量：\n",
    "\n",
    "PSNR_val = []\n",
    "DAEmodel.eval()\n",
    "for ii in range(X_val.shape[0]):\n",
    "    imageindex = ii\n",
    "    # 输入\n",
    "    im = X_val[imageindex, ...]\n",
    "    im = im.unsqueeze(0)\n",
    "    im_noise = np.transpose(im.cpu().data.numpy(), (0, 3, 2, 1))\n",
    "    im_noise = im_noise[0, ...]\n",
    "    # 去噪\n",
    "    _, output = DAEmodel(im)\n",
    "    im_denoise = np.transpose(output.cpu().data.numpy(), (0, 3, 2, 1))\n",
    "    im_denoise = im_denoise[0, ...]\n",
    "    # 输出\n",
    "    im = y_val[imageindex, ...] \n",
    "    im_origin = im.unsqueeze(0)\n",
    "    im_origin = np.transpose(im_origin.cpu().data.numpy(), (0, 3, 2, 1))\n",
    "    im_origin = im_origin[0, ...]\n",
    "    \n",
    "    PSNR_val.append(peak_signal_noise_ratio(im_origin, im_denoise) - peak_signal_noise_ratio(im_origin, im_noise))\n",
    "    \n",
    "print(\"PSNR的平均提升量：\", np.mean(PSNR_val), \"dB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取一张lol图片，看效果\n",
    "high = mpimg.imread('data/lol485/high/2.png') # \n",
    "low = mpimg.imread('data/lol485/low/2.png') # \n",
    "\n",
    "input = np.transpose(low, (2, 1, 0))\n",
    "input = torch.tensor(input, dtype=torch.float32).to(device)\n",
    "input = input.unsqueeze(0)\n",
    "DAEmodel.eval()\n",
    "_, output = DAEmodel(input)\n",
    "im_enhance= np.transpose(output.cpu().data.numpy(), (0, 3, 2, 1))\n",
    "im_enhance = im_enhance[0, ...]\n",
    "\n",
    "plt.figure(figsize=[20, 20])\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(low)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Lowlight image\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(im_enhance)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"LowlightEnhance image\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(high)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Ground truth\")\n",
    "print(\"原始的PSNR:\", peak_signal_noise_ratio(low, high))\n",
    "print(\"暗光增强后的PSNR:\", peak_signal_noise_ratio(im_enhance, high))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在loldataset上计算平均psnr\n",
    "\n",
    "#获取lol485目录信息\n",
    "\n",
    "lolList = os.listdir('./data/lol15/high')\n",
    "DAEmodel.eval()\n",
    "PSNR_init = []\n",
    "PSNR_enhance = []\n",
    "PSNR_plus = []\n",
    "for imgName in lolList:\n",
    "    high = mpimg.imread('data/lol15/high/'+imgName) # \n",
    "    low = mpimg.imread('data/lol15/low/'+imgName) # \n",
    "    input = np.transpose(low, (2, 1, 0))\n",
    "    input = torch.tensor(input, dtype=torch.float32).to(device)\n",
    "    input = input.unsqueeze(0)\n",
    "    _, output = DAEmodel(input)\n",
    "    im_enhance= np.transpose(output.cpu().data.numpy(), (0, 3, 2, 1))\n",
    "    im_enhance = im_enhance[0, ...]\n",
    "    PSNR_init.append(peak_signal_noise_ratio(low, high))\n",
    "    PSNR_enhance.append(peak_signal_noise_ratio(im_enhance, high))\n",
    "    PSNR_plus.append(peak_signal_noise_ratio(im_enhance, high)-peak_signal_noise_ratio(low, high))\n",
    "    # print(\"原始的PSNR:\", peak_signal_noise_ratio(low, high))\n",
    "    # print(\"暗光增强后的PSNR:\", peak_signal_noise_ratio(im_enhance, high))\n",
    "print(np.mean(PSNR_init))\n",
    "print(np.mean(PSNR_enhance))\n",
    "print(np.mean(PSNR_plus))\n",
    "\n",
    "# PSNR_val = []\n",
    "# DAEmodel.eval()\n",
    "# for ii in range(X_val.shape[0]):\n",
    "#     imageindex = ii\n",
    "#     # 输入\n",
    "#     im = X_val[imageindex, ...]\n",
    "#     im = im.unsqueeze(0)\n",
    "#     im_noise = np.transpose(im.cpu().data.numpy(), (0, 3, 2, 1))\n",
    "#     im_noise = im_noise[0, ...]\n",
    "#     # 去噪\n",
    "#     _, output = DAEmodel(im)\n",
    "#     im_denoise = np.transpose(output.cpu().data.numpy(), (0, 3, 2, 1))\n",
    "#     im_denoise = im_denoise[0, ...]\n",
    "#     # 输出\n",
    "#     im = y_val[imageindex, ...] \n",
    "#     im_origin = im.unsqueeze(0)\n",
    "#     im_origin = np.transpose(im_origin.cpu().data.numpy(), (0, 3, 2, 1))\n",
    "#     im_origin = im_origin[0, ...]\n",
    "    \n",
    "#     PSNR_val.append(peak_signal_noise_ratio(im_origin, im_denoise) - peak_signal_noise_ratio(im_origin, im_noise))\n",
    "    \n",
    "# print(\"PSNR的平均提升量：\", np.mean(PSNR_val), \"dB\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a08e6890f1e37f6640bcf115006b91805bef7115f2496f3efee946545d5d6ffd"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('mxy': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
