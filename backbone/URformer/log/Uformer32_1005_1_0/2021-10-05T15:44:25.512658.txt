Namespace(arch='Uformer', att_se=False, batch_size=32, checkpoint=50, dataset='SIDD', embed_dim=32, env='32_1005_1_0', eval_workers=8, global_skip=False, gpu='0', local_skip=False, lr_initial=0.0002, mode='denoising', nepoch=50, norm_layer='nn.LayerNorm', optimizer='adamw', pretrain_weights='./log/Uformer32/models/model_best.pth', resume=False, save_dir='/home/ma-user/work/deNoTr/log', save_images=False, token_mlp='leff', token_projection='linear', train_dir='/home/mist/lowlight/datasets/lol_stage0/train', train_ps=64, train_workers=16, val_dir='/home/mist/lowlight/datasets/lol_stage0/valid', vit_depth=12, vit_dim=256, vit_mlp_dim=512, vit_nheads=8, vit_patch_size=16, vit_share=False, warmup=True, warmup_epochs=3, weight_decay=0.02, win_size=8)
Uformer(
  embed_dim=32, token_projection=linear, token_mlp=leff,win_size=8
  (pos_drop): Dropout(p=0.0, inplace=False)
  (input_proj): InputProj(
    (proj): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): LeakyReLU(negative_slope=0.01, inplace=True)
    )
  )
  (output_proj): OutputProj(
    (proj): Sequential(
      (0): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (encoderlayer_0): BasicUformerLayer(
    dim=32, input_resolution=(64, 64), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=32, input_resolution=(64, 64), num_heads=1, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=32, win_size=(8, 8), num_heads=1
          (qkv): LinearProjection(
            (to_q): Linear(in_features=32, out_features=32, bias=True)
            (to_kv): Linear(in_features=32, out_features=64, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=32, out_features=32, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=32, out_features=128, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=128, out_features=32, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=32, input_resolution=(64, 64), num_heads=1, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=32, win_size=(8, 8), num_heads=1
          (qkv): LinearProjection(
            (to_q): Linear(in_features=32, out_features=32, bias=True)
            (to_kv): Linear(in_features=32, out_features=64, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=32, out_features=32, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=32, out_features=128, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=128, out_features=32, bias=True)
          )
        )
      )
    )
  )
  (dowsample_0): Downsample(
    (conv): Sequential(
      (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (encoderlayer_1): BasicUformerLayer(
    dim=64, input_resolution=(32, 32), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=64, input_resolution=(32, 32), num_heads=2, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=64, win_size=(8, 8), num_heads=2
          (qkv): LinearProjection(
            (to_q): Linear(in_features=64, out_features=64, bias=True)
            (to_kv): Linear(in_features=64, out_features=128, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=64, out_features=256, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=256, out_features=64, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=64, input_resolution=(32, 32), num_heads=2, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=64, win_size=(8, 8), num_heads=2
          (qkv): LinearProjection(
            (to_q): Linear(in_features=64, out_features=64, bias=True)
            (to_kv): Linear(in_features=64, out_features=128, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=64, out_features=256, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=256, out_features=64, bias=True)
          )
        )
      )
    )
  )
  (dowsample_1): Downsample(
    (conv): Sequential(
      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (encoderlayer_2): BasicUformerLayer(
    dim=128, input_resolution=(16, 16), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=128, input_resolution=(16, 16), num_heads=4, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=128, win_size=(8, 8), num_heads=4
          (qkv): LinearProjection(
            (to_q): Linear(in_features=128, out_features=128, bias=True)
            (to_kv): Linear(in_features=128, out_features=256, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=128, out_features=512, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=128, input_resolution=(16, 16), num_heads=4, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=128, win_size=(8, 8), num_heads=4
          (qkv): LinearProjection(
            (to_q): Linear(in_features=128, out_features=128, bias=True)
            (to_kv): Linear(in_features=128, out_features=256, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=128, out_features=512, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
          )
        )
      )
    )
  )
  (dowsample_2): Downsample(
    (conv): Sequential(
      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (encoderlayer_3): BasicUformerLayer(
    dim=256, input_resolution=(8, 8), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=256, input_resolution=(8, 8), num_heads=8, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=256, win_size=(8, 8), num_heads=8
          (qkv): LinearProjection(
            (to_q): Linear(in_features=256, out_features=256, bias=True)
            (to_kv): Linear(in_features=256, out_features=512, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=1024, out_features=256, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=256, input_resolution=(8, 8), num_heads=8, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=256, win_size=(8, 8), num_heads=8
          (qkv): LinearProjection(
            (to_q): Linear(in_features=256, out_features=256, bias=True)
            (to_kv): Linear(in_features=256, out_features=512, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=1024, out_features=256, bias=True)
          )
        )
      )
    )
  )
  (dowsample_3): Downsample(
    (conv): Sequential(
      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (resize_0): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (resize_1): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (resize_2): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (resize_3): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (resize_4): OutputProj(
    (proj): Sequential(
      (0): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (recover_0): InputProj(
    (proj): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): LeakyReLU(negative_slope=0.01, inplace=True)
    )
  )
  (conv): BasicUformerLayer(
    dim=32, input_resolution=(64, 64), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=32, input_resolution=(64, 64), num_heads=16, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=32, win_size=(8, 8), num_heads=16
          (qkv): LinearProjection(
            (to_q): Linear(in_features=32, out_features=32, bias=True)
            (to_kv): Linear(in_features=32, out_features=64, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=32, out_features=32, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=32, out_features=128, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=128, out_features=32, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=32, input_resolution=(64, 64), num_heads=16, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=32, win_size=(8, 8), num_heads=16
          (qkv): LinearProjection(
            (to_q): Linear(in_features=32, out_features=32, bias=True)
            (to_kv): Linear(in_features=32, out_features=64, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=32, out_features=32, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=32, out_features=128, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=128, out_features=32, bias=True)
          )
        )
      )
    )
  )
  (recover_1): Downsample(
    (conv): Sequential(
      (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (recover_2): Downsample(
    (conv): Sequential(
      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (recover_3): Downsample(
    (conv): Sequential(
      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (recover_4): Downsample(
    (conv): Sequential(
      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (upsample_0): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (decoderlayer_0): BasicUformerLayer(
    dim=512, input_resolution=(8, 8), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=512, input_resolution=(8, 8), num_heads=16, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=512, win_size=(8, 8), num_heads=16
          (qkv): LinearProjection(
            (to_q): Linear(in_features=512, out_features=512, bias=True)
            (to_kv): Linear(in_features=512, out_features=1024, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=512, out_features=2048, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=512, input_resolution=(8, 8), num_heads=16, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=512, win_size=(8, 8), num_heads=16
          (qkv): LinearProjection(
            (to_q): Linear(in_features=512, out_features=512, bias=True)
            (to_kv): Linear(in_features=512, out_features=1024, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=512, out_features=2048, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
      )
    )
  )
  (upsample_1): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(512, 128, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (decoderlayer_1): BasicUformerLayer(
    dim=256, input_resolution=(16, 16), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=256, input_resolution=(16, 16), num_heads=8, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=256, win_size=(8, 8), num_heads=8
          (qkv): LinearProjection(
            (to_q): Linear(in_features=256, out_features=256, bias=True)
            (to_kv): Linear(in_features=256, out_features=512, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=1024, out_features=256, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=256, input_resolution=(16, 16), num_heads=8, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=256, win_size=(8, 8), num_heads=8
          (qkv): LinearProjection(
            (to_q): Linear(in_features=256, out_features=256, bias=True)
            (to_kv): Linear(in_features=256, out_features=512, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=1024, out_features=256, bias=True)
          )
        )
      )
    )
  )
  (upsample_2): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (decoderlayer_2): BasicUformerLayer(
    dim=128, input_resolution=(32, 32), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=128, input_resolution=(32, 32), num_heads=4, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=128, win_size=(8, 8), num_heads=4
          (qkv): LinearProjection(
            (to_q): Linear(in_features=128, out_features=128, bias=True)
            (to_kv): Linear(in_features=128, out_features=256, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=128, out_features=512, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=128, input_resolution=(32, 32), num_heads=4, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=128, win_size=(8, 8), num_heads=4
          (qkv): LinearProjection(
            (to_q): Linear(in_features=128, out_features=128, bias=True)
            (to_kv): Linear(in_features=128, out_features=256, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=128, out_features=512, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
          )
        )
      )
    )
  )
  (upsample_3): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(128, 32, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (decoderlayer_3): BasicUformerLayer(
    dim=64, input_resolution=(64, 64), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=64, input_resolution=(64, 64), num_heads=2, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=64, win_size=(8, 8), num_heads=2
          (qkv): LinearProjection(
            (to_q): Linear(in_features=64, out_features=64, bias=True)
            (to_kv): Linear(in_features=64, out_features=128, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=64, out_features=256, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=256, out_features=64, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=64, input_resolution=(64, 64), num_heads=2, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=64, win_size=(8, 8), num_heads=2
          (qkv): LinearProjection(
            (to_q): Linear(in_features=64, out_features=64, bias=True)
            (to_kv): Linear(in_features=64, out_features=128, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=64, out_features=256, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=256, out_features=64, bias=True)
          )
        )
      )
    )
  )
)
[Ep 1 it 29	 PSNR SIDD: 8.4248	] ----  [best_Ep_SIDD 1 best_it_SIDD 29 Best_PSNR_SIDD 8.4248] 
[Ep 1 it 59	 PSNR SIDD: 7.0598	] ----  [best_Ep_SIDD 1 best_it_SIDD 29 Best_PSNR_SIDD 8.4248] 
[Ep 1 it 89	 PSNR SIDD: 8.2731	] ----  [best_Ep_SIDD 1 best_it_SIDD 29 Best_PSNR_SIDD 8.4248] 
[Ep 1 it 119	 PSNR SIDD: 10.2456	] ----  [best_Ep_SIDD 1 best_it_SIDD 119 Best_PSNR_SIDD 10.2456] 
Epoch: 1	Time: 25.0338	Loss: 249.1085	LearningRate 0.000133
[Ep 2 it 29	 PSNR SIDD: 12.4539	] ----  [best_Ep_SIDD 2 best_it_SIDD 29 Best_PSNR_SIDD 12.4539] 
[Ep 2 it 59	 PSNR SIDD: 13.2651	] ----  [best_Ep_SIDD 2 best_it_SIDD 59 Best_PSNR_SIDD 13.2651] 
[Ep 2 it 89	 PSNR SIDD: 13.5092	] ----  [best_Ep_SIDD 2 best_it_SIDD 89 Best_PSNR_SIDD 13.5092] 
[Ep 2 it 119	 PSNR SIDD: 13.4547	] ----  [best_Ep_SIDD 2 best_it_SIDD 89 Best_PSNR_SIDD 13.5092] 
Epoch: 2	Time: 24.8662	Loss: 86.2475	LearningRate 0.000200
[Ep 3 it 29	 PSNR SIDD: 14.8190	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 14.8190] 
[Ep 3 it 59	 PSNR SIDD: 14.9877	] ----  [best_Ep_SIDD 3 best_it_SIDD 59 Best_PSNR_SIDD 14.9877] 
[Ep 3 it 89	 PSNR SIDD: 14.1868	] ----  [best_Ep_SIDD 3 best_it_SIDD 59 Best_PSNR_SIDD 14.9877] 
[Ep 3 it 119	 PSNR SIDD: 15.0846	] ----  [best_Ep_SIDD 3 best_it_SIDD 119 Best_PSNR_SIDD 15.0846] 
Epoch: 3	Time: 26.0109	Loss: 71.6515	LearningRate 0.000200
[Ep 4 it 29	 PSNR SIDD: 15.2983	] ----  [best_Ep_SIDD 4 best_it_SIDD 29 Best_PSNR_SIDD 15.2983] 
[Ep 4 it 59	 PSNR SIDD: 14.1923	] ----  [best_Ep_SIDD 4 best_it_SIDD 29 Best_PSNR_SIDD 15.2983] 
[Ep 4 it 89	 PSNR SIDD: 15.2929	] ----  [best_Ep_SIDD 4 best_it_SIDD 29 Best_PSNR_SIDD 15.2983] 
[Ep 4 it 119	 PSNR SIDD: 15.3509	] ----  [best_Ep_SIDD 4 best_it_SIDD 119 Best_PSNR_SIDD 15.3509] 
Epoch: 4	Time: 26.0079	Loss: 64.6532	LearningRate 0.000200
[Ep 5 it 29	 PSNR SIDD: 15.2995	] ----  [best_Ep_SIDD 4 best_it_SIDD 119 Best_PSNR_SIDD 15.3509] 
[Ep 5 it 59	 PSNR SIDD: 15.1806	] ----  [best_Ep_SIDD 4 best_it_SIDD 119 Best_PSNR_SIDD 15.3509] 
[Ep 5 it 89	 PSNR SIDD: 16.0285	] ----  [best_Ep_SIDD 5 best_it_SIDD 89 Best_PSNR_SIDD 16.0285] 
[Ep 5 it 119	 PSNR SIDD: 15.9149	] ----  [best_Ep_SIDD 5 best_it_SIDD 89 Best_PSNR_SIDD 16.0285] 
Epoch: 5	Time: 25.0269	Loss: 55.9269	LearningRate 0.000198
[Ep 6 it 29	 PSNR SIDD: 16.0156	] ----  [best_Ep_SIDD 5 best_it_SIDD 89 Best_PSNR_SIDD 16.0285] 
[Ep 6 it 59	 PSNR SIDD: 15.8375	] ----  [best_Ep_SIDD 5 best_it_SIDD 89 Best_PSNR_SIDD 16.0285] 
[Ep 6 it 89	 PSNR SIDD: 15.6265	] ----  [best_Ep_SIDD 5 best_it_SIDD 89 Best_PSNR_SIDD 16.0285] 
[Ep 6 it 119	 PSNR SIDD: 16.1437	] ----  [best_Ep_SIDD 6 best_it_SIDD 119 Best_PSNR_SIDD 16.1437] 
Epoch: 6	Time: 25.0390	Loss: 44.6994	LearningRate 0.000197
[Ep 7 it 29	 PSNR SIDD: 15.6759	] ----  [best_Ep_SIDD 6 best_it_SIDD 119 Best_PSNR_SIDD 16.1437] 
[Ep 7 it 59	 PSNR SIDD: 15.8457	] ----  [best_Ep_SIDD 6 best_it_SIDD 119 Best_PSNR_SIDD 16.1437] 
[Ep 7 it 89	 PSNR SIDD: 16.2437	] ----  [best_Ep_SIDD 7 best_it_SIDD 89 Best_PSNR_SIDD 16.2437] 
[Ep 7 it 119	 PSNR SIDD: 16.1339	] ----  [best_Ep_SIDD 7 best_it_SIDD 89 Best_PSNR_SIDD 16.2437] 
Epoch: 7	Time: 25.7485	Loss: 41.5802	LearningRate 0.000195
[Ep 8 it 29	 PSNR SIDD: 16.1356	] ----  [best_Ep_SIDD 7 best_it_SIDD 89 Best_PSNR_SIDD 16.2437] 
[Ep 8 it 59	 PSNR SIDD: 16.1839	] ----  [best_Ep_SIDD 7 best_it_SIDD 89 Best_PSNR_SIDD 16.2437] 
[Ep 8 it 89	 PSNR SIDD: 16.2813	] ----  [best_Ep_SIDD 8 best_it_SIDD 89 Best_PSNR_SIDD 16.2813] 
[Ep 8 it 119	 PSNR SIDD: 15.2230	] ----  [best_Ep_SIDD 8 best_it_SIDD 89 Best_PSNR_SIDD 16.2813] 
Epoch: 8	Time: 25.6375	Loss: 39.9002	LearningRate 0.000193
[Ep 9 it 29	 PSNR SIDD: 16.2696	] ----  [best_Ep_SIDD 8 best_it_SIDD 89 Best_PSNR_SIDD 16.2813] 
[Ep 9 it 59	 PSNR SIDD: 16.2671	] ----  [best_Ep_SIDD 8 best_it_SIDD 89 Best_PSNR_SIDD 16.2813] 
[Ep 9 it 89	 PSNR SIDD: 16.1107	] ----  [best_Ep_SIDD 8 best_it_SIDD 89 Best_PSNR_SIDD 16.2813] 
[Ep 9 it 119	 PSNR SIDD: 16.4150	] ----  [best_Ep_SIDD 9 best_it_SIDD 119 Best_PSNR_SIDD 16.4150] 
Epoch: 9	Time: 25.0305	Loss: 40.3351	LearningRate 0.000190
[Ep 10 it 29	 PSNR SIDD: 16.3875	] ----  [best_Ep_SIDD 9 best_it_SIDD 119 Best_PSNR_SIDD 16.4150] 
[Ep 10 it 59	 PSNR SIDD: 16.2310	] ----  [best_Ep_SIDD 9 best_it_SIDD 119 Best_PSNR_SIDD 16.4150] 
[Ep 10 it 89	 PSNR SIDD: 16.4700	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 16.4700] 
[Ep 10 it 119	 PSNR SIDD: 16.4607	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 16.4700] 
Epoch: 10	Time: 25.0090	Loss: 38.3910	LearningRate 0.000187
[Ep 11 it 29	 PSNR SIDD: 16.1039	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 16.4700] 
[Ep 11 it 59	 PSNR SIDD: 16.2414	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 16.4700] 
[Ep 11 it 89	 PSNR SIDD: 15.2996	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 16.4700] 
[Ep 11 it 119	 PSNR SIDD: 16.1509	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 16.4700] 
Epoch: 11	Time: 24.4060	Loss: 38.6823	LearningRate 0.000183
[Ep 12 it 29	 PSNR SIDD: 15.8707	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 16.4700] 
[Ep 12 it 59	 PSNR SIDD: 15.6978	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 16.4700] 
[Ep 12 it 89	 PSNR SIDD: 16.5598	] ----  [best_Ep_SIDD 12 best_it_SIDD 89 Best_PSNR_SIDD 16.5598] 
[Ep 12 it 119	 PSNR SIDD: 16.4626	] ----  [best_Ep_SIDD 12 best_it_SIDD 89 Best_PSNR_SIDD 16.5598] 
Epoch: 12	Time: 26.0397	Loss: 37.2801	LearningRate 0.000179
[Ep 13 it 29	 PSNR SIDD: 16.5424	] ----  [best_Ep_SIDD 12 best_it_SIDD 89 Best_PSNR_SIDD 16.5598] 
[Ep 13 it 59	 PSNR SIDD: 16.4358	] ----  [best_Ep_SIDD 12 best_it_SIDD 89 Best_PSNR_SIDD 16.5598] 
[Ep 13 it 89	 PSNR SIDD: 16.5152	] ----  [best_Ep_SIDD 12 best_it_SIDD 89 Best_PSNR_SIDD 16.5598] 
[Ep 13 it 119	 PSNR SIDD: 16.4936	] ----  [best_Ep_SIDD 12 best_it_SIDD 89 Best_PSNR_SIDD 16.5598] 
Epoch: 13	Time: 25.0430	Loss: 36.3248	LearningRate 0.000175
[Ep 14 it 29	 PSNR SIDD: 16.8538	] ----  [best_Ep_SIDD 14 best_it_SIDD 29 Best_PSNR_SIDD 16.8538] 
[Ep 14 it 59	 PSNR SIDD: 16.7015	] ----  [best_Ep_SIDD 14 best_it_SIDD 29 Best_PSNR_SIDD 16.8538] 
[Ep 14 it 89	 PSNR SIDD: 16.7869	] ----  [best_Ep_SIDD 14 best_it_SIDD 29 Best_PSNR_SIDD 16.8538] 
[Ep 14 it 119	 PSNR SIDD: 16.4022	] ----  [best_Ep_SIDD 14 best_it_SIDD 29 Best_PSNR_SIDD 16.8538] 
Epoch: 14	Time: 24.1572	Loss: 36.4472	LearningRate 0.000170
[Ep 15 it 29	 PSNR SIDD: 16.7989	] ----  [best_Ep_SIDD 14 best_it_SIDD 29 Best_PSNR_SIDD 16.8538] 
[Ep 15 it 59	 PSNR SIDD: 16.7148	] ----  [best_Ep_SIDD 14 best_it_SIDD 29 Best_PSNR_SIDD 16.8538] 
[Ep 15 it 89	 PSNR SIDD: 16.8914	] ----  [best_Ep_SIDD 15 best_it_SIDD 89 Best_PSNR_SIDD 16.8914] 
[Ep 15 it 119	 PSNR SIDD: 16.6005	] ----  [best_Ep_SIDD 15 best_it_SIDD 89 Best_PSNR_SIDD 16.8914] 
Epoch: 15	Time: 24.7269	Loss: 36.8261	LearningRate 0.000165
[Ep 16 it 29	 PSNR SIDD: 16.9595	] ----  [best_Ep_SIDD 16 best_it_SIDD 29 Best_PSNR_SIDD 16.9595] 
[Ep 16 it 59	 PSNR SIDD: 16.8307	] ----  [best_Ep_SIDD 16 best_it_SIDD 29 Best_PSNR_SIDD 16.9595] 
[Ep 16 it 89	 PSNR SIDD: 16.6782	] ----  [best_Ep_SIDD 16 best_it_SIDD 29 Best_PSNR_SIDD 16.9595] 
[Ep 16 it 119	 PSNR SIDD: 16.6400	] ----  [best_Ep_SIDD 16 best_it_SIDD 29 Best_PSNR_SIDD 16.9595] 
Epoch: 16	Time: 24.7584	Loss: 36.5097	LearningRate 0.000160
[Ep 17 it 29	 PSNR SIDD: 16.9233	] ----  [best_Ep_SIDD 16 best_it_SIDD 29 Best_PSNR_SIDD 16.9595] 
[Ep 17 it 59	 PSNR SIDD: 16.7060	] ----  [best_Ep_SIDD 16 best_it_SIDD 29 Best_PSNR_SIDD 16.9595] 
[Ep 17 it 89	 PSNR SIDD: 15.8686	] ----  [best_Ep_SIDD 16 best_it_SIDD 29 Best_PSNR_SIDD 16.9595] 
[Ep 17 it 119	 PSNR SIDD: 16.9240	] ----  [best_Ep_SIDD 16 best_it_SIDD 29 Best_PSNR_SIDD 16.9595] 
Epoch: 17	Time: 25.4295	Loss: 36.5016	LearningRate 0.000154
[Ep 18 it 29	 PSNR SIDD: 17.1762	] ----  [best_Ep_SIDD 18 best_it_SIDD 29 Best_PSNR_SIDD 17.1762] 
[Ep 18 it 59	 PSNR SIDD: 17.0783	] ----  [best_Ep_SIDD 18 best_it_SIDD 29 Best_PSNR_SIDD 17.1762] 
[Ep 18 it 89	 PSNR SIDD: 17.1847	] ----  [best_Ep_SIDD 18 best_it_SIDD 89 Best_PSNR_SIDD 17.1847] 
[Ep 18 it 119	 PSNR SIDD: 17.3538	] ----  [best_Ep_SIDD 18 best_it_SIDD 119 Best_PSNR_SIDD 17.3538] 
Epoch: 18	Time: 27.5980	Loss: 32.9437	LearningRate 0.000149
[Ep 19 it 29	 PSNR SIDD: 17.6793	] ----  [best_Ep_SIDD 19 best_it_SIDD 29 Best_PSNR_SIDD 17.6793] 
[Ep 19 it 59	 PSNR SIDD: 16.9700	] ----  [best_Ep_SIDD 19 best_it_SIDD 29 Best_PSNR_SIDD 17.6793] 
[Ep 19 it 89	 PSNR SIDD: 17.5476	] ----  [best_Ep_SIDD 19 best_it_SIDD 29 Best_PSNR_SIDD 17.6793] 
[Ep 19 it 119	 PSNR SIDD: 17.9039	] ----  [best_Ep_SIDD 19 best_it_SIDD 119 Best_PSNR_SIDD 17.9039] 
Epoch: 19	Time: 25.3484	Loss: 30.2546	LearningRate 0.000143
[Ep 20 it 29	 PSNR SIDD: 17.4373	] ----  [best_Ep_SIDD 19 best_it_SIDD 119 Best_PSNR_SIDD 17.9039] 
[Ep 20 it 59	 PSNR SIDD: 17.2793	] ----  [best_Ep_SIDD 19 best_it_SIDD 119 Best_PSNR_SIDD 17.9039] 
[Ep 20 it 89	 PSNR SIDD: 16.9652	] ----  [best_Ep_SIDD 19 best_it_SIDD 119 Best_PSNR_SIDD 17.9039] 
[Ep 20 it 119	 PSNR SIDD: 17.8960	] ----  [best_Ep_SIDD 19 best_it_SIDD 119 Best_PSNR_SIDD 17.9039] 
Epoch: 20	Time: 24.1108	Loss: 30.6488	LearningRate 0.000137
[Ep 21 it 29	 PSNR SIDD: 17.8844	] ----  [best_Ep_SIDD 19 best_it_SIDD 119 Best_PSNR_SIDD 17.9039] 
[Ep 21 it 59	 PSNR SIDD: 17.7986	] ----  [best_Ep_SIDD 19 best_it_SIDD 119 Best_PSNR_SIDD 17.9039] 
[Ep 21 it 89	 PSNR SIDD: 18.1077	] ----  [best_Ep_SIDD 21 best_it_SIDD 89 Best_PSNR_SIDD 18.1077] 
[Ep 21 it 119	 PSNR SIDD: 17.0949	] ----  [best_Ep_SIDD 21 best_it_SIDD 89 Best_PSNR_SIDD 18.1077] 
Epoch: 21	Time: 24.5098	Loss: 29.9015	LearningRate 0.000130
[Ep 22 it 29	 PSNR SIDD: 16.9647	] ----  [best_Ep_SIDD 21 best_it_SIDD 89 Best_PSNR_SIDD 18.1077] 
[Ep 22 it 59	 PSNR SIDD: 17.0352	] ----  [best_Ep_SIDD 21 best_it_SIDD 89 Best_PSNR_SIDD 18.1077] 
[Ep 22 it 89	 PSNR SIDD: 17.1608	] ----  [best_Ep_SIDD 21 best_it_SIDD 89 Best_PSNR_SIDD 18.1077] 
[Ep 22 it 119	 PSNR SIDD: 18.0366	] ----  [best_Ep_SIDD 21 best_it_SIDD 89 Best_PSNR_SIDD 18.1077] 
Epoch: 22	Time: 23.1098	Loss: 29.9944	LearningRate 0.000124
[Ep 23 it 29	 PSNR SIDD: 17.3017	] ----  [best_Ep_SIDD 21 best_it_SIDD 89 Best_PSNR_SIDD 18.1077] 
[Ep 23 it 59	 PSNR SIDD: 18.1497	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 18.1497] 
[Ep 23 it 89	 PSNR SIDD: 16.5987	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 18.1497] 
[Ep 23 it 119	 PSNR SIDD: 17.3583	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 18.1497] 
Epoch: 23	Time: 25.3062	Loss: 30.0120	LearningRate 0.000117
[Ep 24 it 29	 PSNR SIDD: 18.1671	] ----  [best_Ep_SIDD 24 best_it_SIDD 29 Best_PSNR_SIDD 18.1671] 
[Ep 24 it 59	 PSNR SIDD: 17.7625	] ----  [best_Ep_SIDD 24 best_it_SIDD 29 Best_PSNR_SIDD 18.1671] 
[Ep 24 it 89	 PSNR SIDD: 18.4550	] ----  [best_Ep_SIDD 24 best_it_SIDD 89 Best_PSNR_SIDD 18.4550] 
[Ep 24 it 119	 PSNR SIDD: 17.7726	] ----  [best_Ep_SIDD 24 best_it_SIDD 89 Best_PSNR_SIDD 18.4550] 
Epoch: 24	Time: 26.1874	Loss: 27.7072	LearningRate 0.000111
[Ep 25 it 29	 PSNR SIDD: 18.2609	] ----  [best_Ep_SIDD 24 best_it_SIDD 89 Best_PSNR_SIDD 18.4550] 
[Ep 25 it 59	 PSNR SIDD: 17.6483	] ----  [best_Ep_SIDD 24 best_it_SIDD 89 Best_PSNR_SIDD 18.4550] 
[Ep 25 it 89	 PSNR SIDD: 17.8757	] ----  [best_Ep_SIDD 24 best_it_SIDD 89 Best_PSNR_SIDD 18.4550] 
[Ep 25 it 119	 PSNR SIDD: 18.0465	] ----  [best_Ep_SIDD 24 best_it_SIDD 89 Best_PSNR_SIDD 18.4550] 
Epoch: 25	Time: 24.4252	Loss: 27.0079	LearningRate 0.000104
[Ep 26 it 29	 PSNR SIDD: 17.9445	] ----  [best_Ep_SIDD 24 best_it_SIDD 89 Best_PSNR_SIDD 18.4550] 
[Ep 26 it 59	 PSNR SIDD: 17.9456	] ----  [best_Ep_SIDD 24 best_it_SIDD 89 Best_PSNR_SIDD 18.4550] 
[Ep 26 it 89	 PSNR SIDD: 17.4377	] ----  [best_Ep_SIDD 24 best_it_SIDD 89 Best_PSNR_SIDD 18.4550] 
[Ep 26 it 119	 PSNR SIDD: 17.5957	] ----  [best_Ep_SIDD 24 best_it_SIDD 89 Best_PSNR_SIDD 18.4550] 
Epoch: 26	Time: 24.4171	Loss: 27.6518	LearningRate 0.000098
[Ep 27 it 29	 PSNR SIDD: 18.6531	] ----  [best_Ep_SIDD 27 best_it_SIDD 29 Best_PSNR_SIDD 18.6531] 
[Ep 27 it 59	 PSNR SIDD: 18.1160	] ----  [best_Ep_SIDD 27 best_it_SIDD 29 Best_PSNR_SIDD 18.6531] 
[Ep 27 it 89	 PSNR SIDD: 17.7685	] ----  [best_Ep_SIDD 27 best_it_SIDD 29 Best_PSNR_SIDD 18.6531] 
[Ep 27 it 119	 PSNR SIDD: 17.3427	] ----  [best_Ep_SIDD 27 best_it_SIDD 29 Best_PSNR_SIDD 18.6531] 
Epoch: 27	Time: 24.6333	Loss: 27.2354	LearningRate 0.000091
[Ep 28 it 29	 PSNR SIDD: 18.3344	] ----  [best_Ep_SIDD 27 best_it_SIDD 29 Best_PSNR_SIDD 18.6531] 
[Ep 28 it 59	 PSNR SIDD: 18.2800	] ----  [best_Ep_SIDD 27 best_it_SIDD 29 Best_PSNR_SIDD 18.6531] 
[Ep 28 it 89	 PSNR SIDD: 17.8443	] ----  [best_Ep_SIDD 27 best_it_SIDD 29 Best_PSNR_SIDD 18.6531] 
[Ep 28 it 119	 PSNR SIDD: 18.3859	] ----  [best_Ep_SIDD 27 best_it_SIDD 29 Best_PSNR_SIDD 18.6531] 
Epoch: 28	Time: 24.5249	Loss: 26.4803	LearningRate 0.000084
[Ep 29 it 29	 PSNR SIDD: 18.2791	] ----  [best_Ep_SIDD 27 best_it_SIDD 29 Best_PSNR_SIDD 18.6531] 
[Ep 29 it 59	 PSNR SIDD: 17.8860	] ----  [best_Ep_SIDD 27 best_it_SIDD 29 Best_PSNR_SIDD 18.6531] 
[Ep 29 it 89	 PSNR SIDD: 17.9126	] ----  [best_Ep_SIDD 27 best_it_SIDD 29 Best_PSNR_SIDD 18.6531] 
[Ep 29 it 119	 PSNR SIDD: 18.6693	] ----  [best_Ep_SIDD 29 best_it_SIDD 119 Best_PSNR_SIDD 18.6693] 
Epoch: 29	Time: 24.4027	Loss: 26.8662	LearningRate 0.000078
[Ep 30 it 29	 PSNR SIDD: 18.1868	] ----  [best_Ep_SIDD 29 best_it_SIDD 119 Best_PSNR_SIDD 18.6693] 
[Ep 30 it 59	 PSNR SIDD: 18.2936	] ----  [best_Ep_SIDD 29 best_it_SIDD 119 Best_PSNR_SIDD 18.6693] 
[Ep 30 it 89	 PSNR SIDD: 17.1799	] ----  [best_Ep_SIDD 29 best_it_SIDD 119 Best_PSNR_SIDD 18.6693] 
[Ep 30 it 119	 PSNR SIDD: 18.0586	] ----  [best_Ep_SIDD 29 best_it_SIDD 119 Best_PSNR_SIDD 18.6693] 
Epoch: 30	Time: 23.4367	Loss: 26.7755	LearningRate 0.000071
[Ep 31 it 29	 PSNR SIDD: 17.2777	] ----  [best_Ep_SIDD 29 best_it_SIDD 119 Best_PSNR_SIDD 18.6693] 
[Ep 31 it 59	 PSNR SIDD: 18.0561	] ----  [best_Ep_SIDD 29 best_it_SIDD 119 Best_PSNR_SIDD 18.6693] 
[Ep 31 it 89	 PSNR SIDD: 18.2382	] ----  [best_Ep_SIDD 29 best_it_SIDD 119 Best_PSNR_SIDD 18.6693] 
[Ep 31 it 119	 PSNR SIDD: 17.5898	] ----  [best_Ep_SIDD 29 best_it_SIDD 119 Best_PSNR_SIDD 18.6693] 
Epoch: 31	Time: 24.7793	Loss: 26.4428	LearningRate 0.000065
[Ep 32 it 29	 PSNR SIDD: 18.0456	] ----  [best_Ep_SIDD 29 best_it_SIDD 119 Best_PSNR_SIDD 18.6693] 
[Ep 32 it 59	 PSNR SIDD: 17.5654	] ----  [best_Ep_SIDD 29 best_it_SIDD 119 Best_PSNR_SIDD 18.6693] 
[Ep 32 it 89	 PSNR SIDD: 18.1381	] ----  [best_Ep_SIDD 29 best_it_SIDD 119 Best_PSNR_SIDD 18.6693] 
[Ep 32 it 119	 PSNR SIDD: 18.1918	] ----  [best_Ep_SIDD 29 best_it_SIDD 119 Best_PSNR_SIDD 18.6693] 
Epoch: 32	Time: 24.4939	Loss: 25.4557	LearningRate 0.000059
[Ep 33 it 29	 PSNR SIDD: 18.5754	] ----  [best_Ep_SIDD 29 best_it_SIDD 119 Best_PSNR_SIDD 18.6693] 
[Ep 33 it 59	 PSNR SIDD: 18.2229	] ----  [best_Ep_SIDD 29 best_it_SIDD 119 Best_PSNR_SIDD 18.6693] 
[Ep 33 it 89	 PSNR SIDD: 17.7567	] ----  [best_Ep_SIDD 29 best_it_SIDD 119 Best_PSNR_SIDD 18.6693] 
[Ep 33 it 119	 PSNR SIDD: 17.8782	] ----  [best_Ep_SIDD 29 best_it_SIDD 119 Best_PSNR_SIDD 18.6693] 
Epoch: 33	Time: 25.6344	Loss: 25.7748	LearningRate 0.000053
[Ep 34 it 29	 PSNR SIDD: 18.1511	] ----  [best_Ep_SIDD 29 best_it_SIDD 119 Best_PSNR_SIDD 18.6693] 
[Ep 34 it 59	 PSNR SIDD: 18.2980	] ----  [best_Ep_SIDD 29 best_it_SIDD 119 Best_PSNR_SIDD 18.6693] 
[Ep 34 it 89	 PSNR SIDD: 18.0307	] ----  [best_Ep_SIDD 29 best_it_SIDD 119 Best_PSNR_SIDD 18.6693] 
[Ep 34 it 119	 PSNR SIDD: 18.3098	] ----  [best_Ep_SIDD 29 best_it_SIDD 119 Best_PSNR_SIDD 18.6693] 
Epoch: 34	Time: 24.6474	Loss: 25.6544	LearningRate 0.000047
[Ep 35 it 29	 PSNR SIDD: 18.4538	] ----  [best_Ep_SIDD 29 best_it_SIDD 119 Best_PSNR_SIDD 18.6693] 
[Ep 35 it 59	 PSNR SIDD: 18.5838	] ----  [best_Ep_SIDD 29 best_it_SIDD 119 Best_PSNR_SIDD 18.6693] 
[Ep 35 it 89	 PSNR SIDD: 17.7316	] ----  [best_Ep_SIDD 29 best_it_SIDD 119 Best_PSNR_SIDD 18.6693] 
[Ep 35 it 119	 PSNR SIDD: 18.7594	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
Epoch: 35	Time: 24.3805	Loss: 25.5461	LearningRate 0.000042
[Ep 36 it 29	 PSNR SIDD: 17.9661	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 36 it 59	 PSNR SIDD: 18.1367	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 36 it 89	 PSNR SIDD: 18.1515	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 36 it 119	 PSNR SIDD: 18.1692	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
Epoch: 36	Time: 23.9849	Loss: 25.0273	LearningRate 0.000037
[Ep 37 it 29	 PSNR SIDD: 18.1646	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 37 it 59	 PSNR SIDD: 18.2772	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 37 it 89	 PSNR SIDD: 18.4666	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 37 it 119	 PSNR SIDD: 18.1394	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
Epoch: 37	Time: 24.8848	Loss: 24.6780	LearningRate 0.000032
[Ep 38 it 29	 PSNR SIDD: 18.6187	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 38 it 59	 PSNR SIDD: 18.5781	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 38 it 89	 PSNR SIDD: 18.3570	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 38 it 119	 PSNR SIDD: 18.1086	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
Epoch: 38	Time: 24.7842	Loss: 24.8847	LearningRate 0.000027
[Ep 39 it 29	 PSNR SIDD: 18.4392	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 39 it 59	 PSNR SIDD: 17.9054	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 39 it 89	 PSNR SIDD: 18.4473	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 39 it 119	 PSNR SIDD: 18.2918	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
Epoch: 39	Time: 24.1747	Loss: 25.4366	LearningRate 0.000023
[Ep 40 it 29	 PSNR SIDD: 18.0457	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 40 it 59	 PSNR SIDD: 18.1189	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 40 it 89	 PSNR SIDD: 18.0605	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 40 it 119	 PSNR SIDD: 18.4875	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
Epoch: 40	Time: 23.8997	Loss: 24.9450	LearningRate 0.000019
[Ep 41 it 29	 PSNR SIDD: 18.1771	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 41 it 59	 PSNR SIDD: 18.1927	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 41 it 89	 PSNR SIDD: 18.4568	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 41 it 119	 PSNR SIDD: 18.5011	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
Epoch: 41	Time: 24.8948	Loss: 24.3823	LearningRate 0.000015
[Ep 42 it 29	 PSNR SIDD: 18.4825	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 42 it 59	 PSNR SIDD: 18.6061	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 42 it 89	 PSNR SIDD: 18.5289	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 42 it 119	 PSNR SIDD: 18.2905	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
Epoch: 42	Time: 24.2444	Loss: 24.1997	LearningRate 0.000012
[Ep 43 it 29	 PSNR SIDD: 18.2129	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 43 it 59	 PSNR SIDD: 18.5600	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 43 it 89	 PSNR SIDD: 18.5126	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 43 it 119	 PSNR SIDD: 18.2789	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
Epoch: 43	Time: 24.4562	Loss: 24.9693	LearningRate 0.000009
[Ep 44 it 29	 PSNR SIDD: 18.6341	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 44 it 59	 PSNR SIDD: 18.6679	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 44 it 89	 PSNR SIDD: 18.3947	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 44 it 119	 PSNR SIDD: 18.3325	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
Epoch: 44	Time: 24.3841	Loss: 24.3391	LearningRate 0.000007
[Ep 45 it 29	 PSNR SIDD: 18.4210	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 45 it 59	 PSNR SIDD: 18.2951	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 45 it 89	 PSNR SIDD: 18.3791	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 45 it 119	 PSNR SIDD: 18.5705	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
Epoch: 45	Time: 25.2492	Loss: 24.9542	LearningRate 0.000005
[Ep 46 it 29	 PSNR SIDD: 18.4715	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 46 it 59	 PSNR SIDD: 18.3708	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 46 it 89	 PSNR SIDD: 18.3974	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 46 it 119	 PSNR SIDD: 18.4587	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
Epoch: 46	Time: 24.6911	Loss: 23.9499	LearningRate 0.000003
[Ep 47 it 29	 PSNR SIDD: 18.5523	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 47 it 59	 PSNR SIDD: 18.4203	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 47 it 89	 PSNR SIDD: 18.4867	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 47 it 119	 PSNR SIDD: 18.5979	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
Epoch: 47	Time: 23.7208	Loss: 23.8541	LearningRate 0.000002
[Ep 48 it 29	 PSNR SIDD: 18.6755	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 48 it 59	 PSNR SIDD: 18.4170	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 48 it 89	 PSNR SIDD: 18.5911	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 48 it 119	 PSNR SIDD: 18.3750	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
Epoch: 48	Time: 24.2715	Loss: 23.8924	LearningRate 0.000001
[Ep 49 it 29	 PSNR SIDD: 18.5965	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 49 it 59	 PSNR SIDD: 18.5447	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 49 it 89	 PSNR SIDD: 18.5733	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 49 it 119	 PSNR SIDD: 18.4769	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
Epoch: 49	Time: 24.5722	Loss: 24.4971	LearningRate 0.000001
[Ep 50 it 29	 PSNR SIDD: 18.5088	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 50 it 59	 PSNR SIDD: 18.5950	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 50 it 89	 PSNR SIDD: 18.5897	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
[Ep 50 it 119	 PSNR SIDD: 18.5426	] ----  [best_Ep_SIDD 35 best_it_SIDD 119 Best_PSNR_SIDD 18.7594] 
Epoch: 50	Time: 23.6559	Loss: 24.5000	LearningRate 0.000001
