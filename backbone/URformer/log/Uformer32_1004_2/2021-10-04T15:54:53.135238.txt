Namespace(arch='Uformer', att_se=False, batch_size=32, checkpoint=50, dataset='SIDD', embed_dim=32, env='32_1004_2', eval_workers=8, global_skip=False, gpu='0', local_skip=False, lr_initial=0.0002, mode='denoising', nepoch=200, norm_layer='nn.LayerNorm', optimizer='adamw', pretrain_weights='./log/Uformer32/models/model_best.pth', resume=False, save_dir='/home/ma-user/work/deNoTr/log', save_images=False, token_mlp='leff', token_projection='linear', train_dir='/home/mist/lowlight/datasets/lol/train', train_ps=64, train_workers=16, val_dir='/home/mist/lowlight/datasets/lol/valid', vit_depth=12, vit_dim=256, vit_mlp_dim=512, vit_nheads=8, vit_patch_size=16, vit_share=False, warmup=True, warmup_epochs=3, weight_decay=0.02, win_size=8)
Uformer(
  embed_dim=32, token_projection=linear, token_mlp=leff,win_size=8
  (pos_drop): Dropout(p=0.0, inplace=False)
  (input_proj): InputProj(
    (proj): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): LeakyReLU(negative_slope=0.01, inplace=True)
    )
  )
  (output_proj): OutputProj(
    (proj): Sequential(
      (0): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (encoderlayer_0): BasicUformerLayer(
    dim=32, input_resolution=(64, 64), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=32, input_resolution=(64, 64), num_heads=1, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=32, win_size=(8, 8), num_heads=1
          (qkv): LinearProjection(
            (to_q): Linear(in_features=32, out_features=32, bias=True)
            (to_kv): Linear(in_features=32, out_features=64, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=32, out_features=32, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=32, out_features=128, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=128, out_features=32, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=32, input_resolution=(64, 64), num_heads=1, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=32, win_size=(8, 8), num_heads=1
          (qkv): LinearProjection(
            (to_q): Linear(in_features=32, out_features=32, bias=True)
            (to_kv): Linear(in_features=32, out_features=64, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=32, out_features=32, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=32, out_features=128, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=128, out_features=32, bias=True)
          )
        )
      )
    )
  )
  (dowsample_0): Downsample(
    (conv): Sequential(
      (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (encoderlayer_1): BasicUformerLayer(
    dim=64, input_resolution=(32, 32), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=64, input_resolution=(32, 32), num_heads=2, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=64, win_size=(8, 8), num_heads=2
          (qkv): LinearProjection(
            (to_q): Linear(in_features=64, out_features=64, bias=True)
            (to_kv): Linear(in_features=64, out_features=128, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=64, out_features=256, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=256, out_features=64, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=64, input_resolution=(32, 32), num_heads=2, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=64, win_size=(8, 8), num_heads=2
          (qkv): LinearProjection(
            (to_q): Linear(in_features=64, out_features=64, bias=True)
            (to_kv): Linear(in_features=64, out_features=128, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=64, out_features=256, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=256, out_features=64, bias=True)
          )
        )
      )
    )
  )
  (dowsample_1): Downsample(
    (conv): Sequential(
      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (encoderlayer_2): BasicUformerLayer(
    dim=128, input_resolution=(16, 16), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=128, input_resolution=(16, 16), num_heads=4, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=128, win_size=(8, 8), num_heads=4
          (qkv): LinearProjection(
            (to_q): Linear(in_features=128, out_features=128, bias=True)
            (to_kv): Linear(in_features=128, out_features=256, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=128, out_features=512, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=128, input_resolution=(16, 16), num_heads=4, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=128, win_size=(8, 8), num_heads=4
          (qkv): LinearProjection(
            (to_q): Linear(in_features=128, out_features=128, bias=True)
            (to_kv): Linear(in_features=128, out_features=256, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=128, out_features=512, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
          )
        )
      )
    )
  )
  (dowsample_2): Downsample(
    (conv): Sequential(
      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (encoderlayer_3): BasicUformerLayer(
    dim=256, input_resolution=(8, 8), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=256, input_resolution=(8, 8), num_heads=8, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=256, win_size=(8, 8), num_heads=8
          (qkv): LinearProjection(
            (to_q): Linear(in_features=256, out_features=256, bias=True)
            (to_kv): Linear(in_features=256, out_features=512, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=1024, out_features=256, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=256, input_resolution=(8, 8), num_heads=8, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=256, win_size=(8, 8), num_heads=8
          (qkv): LinearProjection(
            (to_q): Linear(in_features=256, out_features=256, bias=True)
            (to_kv): Linear(in_features=256, out_features=512, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=1024, out_features=256, bias=True)
          )
        )
      )
    )
  )
  (dowsample_3): Downsample(
    (conv): Sequential(
      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (resize_0): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (resize_1): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (resize_2): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (resize_3): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (resize_4): OutputProj(
    (proj): Sequential(
      (0): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (conv): BasicUformerLayer(
    dim=512, input_resolution=(4, 4), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=512, input_resolution=(4, 4), num_heads=16, win_size=4, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=512, win_size=(4, 4), num_heads=16
          (qkv): LinearProjection(
            (to_q): Linear(in_features=512, out_features=512, bias=True)
            (to_kv): Linear(in_features=512, out_features=1024, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=512, out_features=2048, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=512, input_resolution=(4, 4), num_heads=16, win_size=4, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=512, win_size=(4, 4), num_heads=16
          (qkv): LinearProjection(
            (to_q): Linear(in_features=512, out_features=512, bias=True)
            (to_kv): Linear(in_features=512, out_features=1024, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=512, out_features=2048, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
      )
    )
  )
  (upsample_0): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (decoderlayer_0): BasicUformerLayer(
    dim=512, input_resolution=(8, 8), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=512, input_resolution=(8, 8), num_heads=16, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=512, win_size=(8, 8), num_heads=16
          (qkv): LinearProjection(
            (to_q): Linear(in_features=512, out_features=512, bias=True)
            (to_kv): Linear(in_features=512, out_features=1024, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=512, out_features=2048, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=512, input_resolution=(8, 8), num_heads=16, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=512, win_size=(8, 8), num_heads=16
          (qkv): LinearProjection(
            (to_q): Linear(in_features=512, out_features=512, bias=True)
            (to_kv): Linear(in_features=512, out_features=1024, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=512, out_features=2048, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
      )
    )
  )
  (upsample_1): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(512, 128, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (decoderlayer_1): BasicUformerLayer(
    dim=256, input_resolution=(16, 16), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=256, input_resolution=(16, 16), num_heads=8, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=256, win_size=(8, 8), num_heads=8
          (qkv): LinearProjection(
            (to_q): Linear(in_features=256, out_features=256, bias=True)
            (to_kv): Linear(in_features=256, out_features=512, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=1024, out_features=256, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=256, input_resolution=(16, 16), num_heads=8, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=256, win_size=(8, 8), num_heads=8
          (qkv): LinearProjection(
            (to_q): Linear(in_features=256, out_features=256, bias=True)
            (to_kv): Linear(in_features=256, out_features=512, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=1024, out_features=256, bias=True)
          )
        )
      )
    )
  )
  (upsample_2): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (decoderlayer_2): BasicUformerLayer(
    dim=128, input_resolution=(32, 32), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=128, input_resolution=(32, 32), num_heads=4, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=128, win_size=(8, 8), num_heads=4
          (qkv): LinearProjection(
            (to_q): Linear(in_features=128, out_features=128, bias=True)
            (to_kv): Linear(in_features=128, out_features=256, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=128, out_features=512, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=128, input_resolution=(32, 32), num_heads=4, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=128, win_size=(8, 8), num_heads=4
          (qkv): LinearProjection(
            (to_q): Linear(in_features=128, out_features=128, bias=True)
            (to_kv): Linear(in_features=128, out_features=256, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=128, out_features=512, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
          )
        )
      )
    )
  )
  (upsample_3): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(128, 32, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (decoderlayer_3): BasicUformerLayer(
    dim=64, input_resolution=(64, 64), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=64, input_resolution=(64, 64), num_heads=2, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=64, win_size=(8, 8), num_heads=2
          (qkv): LinearProjection(
            (to_q): Linear(in_features=64, out_features=64, bias=True)
            (to_kv): Linear(in_features=64, out_features=128, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=64, out_features=256, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=256, out_features=64, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=64, input_resolution=(64, 64), num_heads=2, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=64, win_size=(8, 8), num_heads=2
          (qkv): LinearProjection(
            (to_q): Linear(in_features=64, out_features=64, bias=True)
            (to_kv): Linear(in_features=64, out_features=128, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=64, out_features=256, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=256, out_features=64, bias=True)
          )
        )
      )
    )
  )
)
[Ep 1 it 29	 PSNR SIDD: 11.4973	] ----  [best_Ep_SIDD 1 best_it_SIDD 29 Best_PSNR_SIDD 11.4973] 
[Ep 1 it 59	 PSNR SIDD: 13.3403	] ----  [best_Ep_SIDD 1 best_it_SIDD 59 Best_PSNR_SIDD 13.3403] 
[Ep 1 it 89	 PSNR SIDD: 14.0935	] ----  [best_Ep_SIDD 1 best_it_SIDD 89 Best_PSNR_SIDD 14.0935] 
[Ep 1 it 119	 PSNR SIDD: 15.2350	] ----  [best_Ep_SIDD 1 best_it_SIDD 119 Best_PSNR_SIDD 15.2350] 
Epoch: 1	Time: 28.7759	Loss: 122.0289	LearningRate 0.000133
[Ep 2 it 29	 PSNR SIDD: 15.5996	] ----  [best_Ep_SIDD 2 best_it_SIDD 29 Best_PSNR_SIDD 15.5996] 
[Ep 2 it 59	 PSNR SIDD: 15.4167	] ----  [best_Ep_SIDD 2 best_it_SIDD 29 Best_PSNR_SIDD 15.5996] 
[Ep 2 it 89	 PSNR SIDD: 15.6146	] ----  [best_Ep_SIDD 2 best_it_SIDD 89 Best_PSNR_SIDD 15.6146] 
[Ep 2 it 119	 PSNR SIDD: 16.1979	] ----  [best_Ep_SIDD 2 best_it_SIDD 119 Best_PSNR_SIDD 16.1979] 
Epoch: 2	Time: 26.6000	Loss: 66.3596	LearningRate 0.000200
[Ep 3 it 29	 PSNR SIDD: 15.4495	] ----  [best_Ep_SIDD 2 best_it_SIDD 119 Best_PSNR_SIDD 16.1979] 
[Ep 3 it 59	 PSNR SIDD: 15.3786	] ----  [best_Ep_SIDD 2 best_it_SIDD 119 Best_PSNR_SIDD 16.1979] 
[Ep 3 it 89	 PSNR SIDD: 16.0456	] ----  [best_Ep_SIDD 2 best_it_SIDD 119 Best_PSNR_SIDD 16.1979] 
[Ep 3 it 119	 PSNR SIDD: 15.0373	] ----  [best_Ep_SIDD 2 best_it_SIDD 119 Best_PSNR_SIDD 16.1979] 
Epoch: 3	Time: 26.0264	Loss: 62.7742	LearningRate 0.000200
[Ep 4 it 29	 PSNR SIDD: 16.1386	] ----  [best_Ep_SIDD 2 best_it_SIDD 119 Best_PSNR_SIDD 16.1979] 
[Ep 4 it 59	 PSNR SIDD: 16.3251	] ----  [best_Ep_SIDD 4 best_it_SIDD 59 Best_PSNR_SIDD 16.3251] 
[Ep 4 it 89	 PSNR SIDD: 16.3224	] ----  [best_Ep_SIDD 4 best_it_SIDD 59 Best_PSNR_SIDD 16.3251] 
[Ep 4 it 119	 PSNR SIDD: 16.2230	] ----  [best_Ep_SIDD 4 best_it_SIDD 59 Best_PSNR_SIDD 16.3251] 
Epoch: 4	Time: 26.6408	Loss: 55.5030	LearningRate 0.000200
[Ep 5 it 29	 PSNR SIDD: 15.1209	] ----  [best_Ep_SIDD 4 best_it_SIDD 59 Best_PSNR_SIDD 16.3251] 
[Ep 5 it 59	 PSNR SIDD: 15.3541	] ----  [best_Ep_SIDD 4 best_it_SIDD 59 Best_PSNR_SIDD 16.3251] 
[Ep 5 it 89	 PSNR SIDD: 16.2531	] ----  [best_Ep_SIDD 4 best_it_SIDD 59 Best_PSNR_SIDD 16.3251] 
[Ep 5 it 119	 PSNR SIDD: 15.9478	] ----  [best_Ep_SIDD 4 best_it_SIDD 59 Best_PSNR_SIDD 16.3251] 
Epoch: 5	Time: 25.9356	Loss: 53.3934	LearningRate 0.000200
[Ep 6 it 29	 PSNR SIDD: 16.1607	] ----  [best_Ep_SIDD 4 best_it_SIDD 59 Best_PSNR_SIDD 16.3251] 
[Ep 6 it 59	 PSNR SIDD: 16.0390	] ----  [best_Ep_SIDD 4 best_it_SIDD 59 Best_PSNR_SIDD 16.3251] 
[Ep 6 it 89	 PSNR SIDD: 15.7168	] ----  [best_Ep_SIDD 4 best_it_SIDD 59 Best_PSNR_SIDD 16.3251] 
[Ep 6 it 119	 PSNR SIDD: 16.4794	] ----  [best_Ep_SIDD 6 best_it_SIDD 119 Best_PSNR_SIDD 16.4794] 
Epoch: 6	Time: 27.7366	Loss: 39.7491	LearningRate 0.000200
[Ep 7 it 29	 PSNR SIDD: 16.6392	] ----  [best_Ep_SIDD 7 best_it_SIDD 29 Best_PSNR_SIDD 16.6392] 
[Ep 7 it 59	 PSNR SIDD: 16.7821	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.7821] 
[Ep 7 it 89	 PSNR SIDD: 16.3814	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.7821] 
[Ep 7 it 119	 PSNR SIDD: 15.9988	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.7821] 
Epoch: 7	Time: 28.0529	Loss: 37.9380	LearningRate 0.000200
[Ep 8 it 29	 PSNR SIDD: 16.6735	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.7821] 
[Ep 8 it 59	 PSNR SIDD: 16.6090	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.7821] 
[Ep 8 it 89	 PSNR SIDD: 16.9279	] ----  [best_Ep_SIDD 8 best_it_SIDD 89 Best_PSNR_SIDD 16.9279] 
[Ep 8 it 119	 PSNR SIDD: 16.2624	] ----  [best_Ep_SIDD 8 best_it_SIDD 89 Best_PSNR_SIDD 16.9279] 
Epoch: 8	Time: 26.7410	Loss: 37.1102	LearningRate 0.000200
[Ep 9 it 29	 PSNR SIDD: 16.6115	] ----  [best_Ep_SIDD 8 best_it_SIDD 89 Best_PSNR_SIDD 16.9279] 
[Ep 9 it 59	 PSNR SIDD: 16.5612	] ----  [best_Ep_SIDD 8 best_it_SIDD 89 Best_PSNR_SIDD 16.9279] 
[Ep 9 it 89	 PSNR SIDD: 16.4630	] ----  [best_Ep_SIDD 8 best_it_SIDD 89 Best_PSNR_SIDD 16.9279] 
[Ep 9 it 119	 PSNR SIDD: 16.5784	] ----  [best_Ep_SIDD 8 best_it_SIDD 89 Best_PSNR_SIDD 16.9279] 
Epoch: 9	Time: 26.0485	Loss: 36.3608	LearningRate 0.000199
[Ep 10 it 29	 PSNR SIDD: 16.7827	] ----  [best_Ep_SIDD 8 best_it_SIDD 89 Best_PSNR_SIDD 16.9279] 
[Ep 10 it 59	 PSNR SIDD: 16.2572	] ----  [best_Ep_SIDD 8 best_it_SIDD 89 Best_PSNR_SIDD 16.9279] 
[Ep 10 it 89	 PSNR SIDD: 16.5942	] ----  [best_Ep_SIDD 8 best_it_SIDD 89 Best_PSNR_SIDD 16.9279] 
[Ep 10 it 119	 PSNR SIDD: 16.4064	] ----  [best_Ep_SIDD 8 best_it_SIDD 89 Best_PSNR_SIDD 16.9279] 
Epoch: 10	Time: 26.8550	Loss: 36.9631	LearningRate 0.000199
[Ep 11 it 29	 PSNR SIDD: 16.0814	] ----  [best_Ep_SIDD 8 best_it_SIDD 89 Best_PSNR_SIDD 16.9279] 
[Ep 11 it 59	 PSNR SIDD: 16.5065	] ----  [best_Ep_SIDD 8 best_it_SIDD 89 Best_PSNR_SIDD 16.9279] 
[Ep 11 it 89	 PSNR SIDD: 16.9631	] ----  [best_Ep_SIDD 11 best_it_SIDD 89 Best_PSNR_SIDD 16.9631] 
[Ep 11 it 119	 PSNR SIDD: 17.0357	] ----  [best_Ep_SIDD 11 best_it_SIDD 119 Best_PSNR_SIDD 17.0357] 
Epoch: 11	Time: 27.1284	Loss: 34.6342	LearningRate 0.000199
[Ep 12 it 29	 PSNR SIDD: 16.0540	] ----  [best_Ep_SIDD 11 best_it_SIDD 119 Best_PSNR_SIDD 17.0357] 
[Ep 12 it 59	 PSNR SIDD: 16.7131	] ----  [best_Ep_SIDD 11 best_it_SIDD 119 Best_PSNR_SIDD 17.0357] 
[Ep 12 it 89	 PSNR SIDD: 16.7171	] ----  [best_Ep_SIDD 11 best_it_SIDD 119 Best_PSNR_SIDD 17.0357] 
[Ep 12 it 119	 PSNR SIDD: 16.5898	] ----  [best_Ep_SIDD 11 best_it_SIDD 119 Best_PSNR_SIDD 17.0357] 
Epoch: 12	Time: 26.6785	Loss: 36.9862	LearningRate 0.000199
[Ep 13 it 29	 PSNR SIDD: 16.5761	] ----  [best_Ep_SIDD 11 best_it_SIDD 119 Best_PSNR_SIDD 17.0357] 
[Ep 13 it 59	 PSNR SIDD: 16.4108	] ----  [best_Ep_SIDD 11 best_it_SIDD 119 Best_PSNR_SIDD 17.0357] 
[Ep 13 it 89	 PSNR SIDD: 16.4892	] ----  [best_Ep_SIDD 11 best_it_SIDD 119 Best_PSNR_SIDD 17.0357] 
[Ep 13 it 119	 PSNR SIDD: 16.6887	] ----  [best_Ep_SIDD 11 best_it_SIDD 119 Best_PSNR_SIDD 17.0357] 
Epoch: 13	Time: 27.2081	Loss: 35.7458	LearningRate 0.000198
[Ep 14 it 29	 PSNR SIDD: 16.3399	] ----  [best_Ep_SIDD 11 best_it_SIDD 119 Best_PSNR_SIDD 17.0357] 
[Ep 14 it 59	 PSNR SIDD: 16.2079	] ----  [best_Ep_SIDD 11 best_it_SIDD 119 Best_PSNR_SIDD 17.0357] 
[Ep 14 it 89	 PSNR SIDD: 16.7074	] ----  [best_Ep_SIDD 11 best_it_SIDD 119 Best_PSNR_SIDD 17.0357] 
[Ep 14 it 119	 PSNR SIDD: 16.7371	] ----  [best_Ep_SIDD 11 best_it_SIDD 119 Best_PSNR_SIDD 17.0357] 
Epoch: 14	Time: 27.8640	Loss: 34.8297	LearningRate 0.000198
[Ep 15 it 29	 PSNR SIDD: 16.5983	] ----  [best_Ep_SIDD 11 best_it_SIDD 119 Best_PSNR_SIDD 17.0357] 
[Ep 15 it 59	 PSNR SIDD: 16.6739	] ----  [best_Ep_SIDD 11 best_it_SIDD 119 Best_PSNR_SIDD 17.0357] 
[Ep 15 it 89	 PSNR SIDD: 16.7097	] ----  [best_Ep_SIDD 11 best_it_SIDD 119 Best_PSNR_SIDD 17.0357] 
[Ep 15 it 119	 PSNR SIDD: 16.2370	] ----  [best_Ep_SIDD 11 best_it_SIDD 119 Best_PSNR_SIDD 17.0357] 
Epoch: 15	Time: 28.1505	Loss: 33.7873	LearningRate 0.000198
[Ep 16 it 29	 PSNR SIDD: 16.9735	] ----  [best_Ep_SIDD 11 best_it_SIDD 119 Best_PSNR_SIDD 17.0357] 
[Ep 16 it 59	 PSNR SIDD: 16.4394	] ----  [best_Ep_SIDD 11 best_it_SIDD 119 Best_PSNR_SIDD 17.0357] 
[Ep 16 it 89	 PSNR SIDD: 16.4639	] ----  [best_Ep_SIDD 11 best_it_SIDD 119 Best_PSNR_SIDD 17.0357] 
[Ep 16 it 119	 PSNR SIDD: 16.9788	] ----  [best_Ep_SIDD 11 best_it_SIDD 119 Best_PSNR_SIDD 17.0357] 
Epoch: 16	Time: 28.6925	Loss: 34.9024	LearningRate 0.000198
[Ep 17 it 29	 PSNR SIDD: 16.8262	] ----  [best_Ep_SIDD 11 best_it_SIDD 119 Best_PSNR_SIDD 17.0357] 
[Ep 17 it 59	 PSNR SIDD: 16.4751	] ----  [best_Ep_SIDD 11 best_it_SIDD 119 Best_PSNR_SIDD 17.0357] 
[Ep 17 it 89	 PSNR SIDD: 17.0880	] ----  [best_Ep_SIDD 17 best_it_SIDD 89 Best_PSNR_SIDD 17.0880] 
[Ep 17 it 119	 PSNR SIDD: 16.6852	] ----  [best_Ep_SIDD 17 best_it_SIDD 89 Best_PSNR_SIDD 17.0880] 
Epoch: 17	Time: 28.5050	Loss: 34.0376	LearningRate 0.000197
[Ep 18 it 29	 PSNR SIDD: 16.8683	] ----  [best_Ep_SIDD 17 best_it_SIDD 89 Best_PSNR_SIDD 17.0880] 
[Ep 18 it 59	 PSNR SIDD: 16.9132	] ----  [best_Ep_SIDD 17 best_it_SIDD 89 Best_PSNR_SIDD 17.0880] 
[Ep 18 it 89	 PSNR SIDD: 16.9503	] ----  [best_Ep_SIDD 17 best_it_SIDD 89 Best_PSNR_SIDD 17.0880] 
[Ep 18 it 119	 PSNR SIDD: 16.5423	] ----  [best_Ep_SIDD 17 best_it_SIDD 89 Best_PSNR_SIDD 17.0880] 
Epoch: 18	Time: 28.4703	Loss: 33.6851	LearningRate 0.000197
[Ep 19 it 29	 PSNR SIDD: 16.1961	] ----  [best_Ep_SIDD 17 best_it_SIDD 89 Best_PSNR_SIDD 17.0880] 
[Ep 19 it 59	 PSNR SIDD: 16.2848	] ----  [best_Ep_SIDD 17 best_it_SIDD 89 Best_PSNR_SIDD 17.0880] 
[Ep 19 it 89	 PSNR SIDD: 16.9055	] ----  [best_Ep_SIDD 17 best_it_SIDD 89 Best_PSNR_SIDD 17.0880] 
[Ep 19 it 119	 PSNR SIDD: 16.4693	] ----  [best_Ep_SIDD 17 best_it_SIDD 89 Best_PSNR_SIDD 17.0880] 
Epoch: 19	Time: 28.6628	Loss: 34.8844	LearningRate 0.000196
[Ep 20 it 29	 PSNR SIDD: 16.0123	] ----  [best_Ep_SIDD 17 best_it_SIDD 89 Best_PSNR_SIDD 17.0880] 
[Ep 20 it 59	 PSNR SIDD: 16.3650	] ----  [best_Ep_SIDD 17 best_it_SIDD 89 Best_PSNR_SIDD 17.0880] 
[Ep 20 it 89	 PSNR SIDD: 16.8429	] ----  [best_Ep_SIDD 17 best_it_SIDD 89 Best_PSNR_SIDD 17.0880] 
[Ep 20 it 119	 PSNR SIDD: 16.6272	] ----  [best_Ep_SIDD 17 best_it_SIDD 89 Best_PSNR_SIDD 17.0880] 
Epoch: 20	Time: 29.2139	Loss: 34.7197	LearningRate 0.000196
[Ep 21 it 29	 PSNR SIDD: 16.5328	] ----  [best_Ep_SIDD 17 best_it_SIDD 89 Best_PSNR_SIDD 17.0880] 
[Ep 21 it 59	 PSNR SIDD: 16.3112	] ----  [best_Ep_SIDD 17 best_it_SIDD 89 Best_PSNR_SIDD 17.0880] 
[Ep 21 it 89	 PSNR SIDD: 16.2422	] ----  [best_Ep_SIDD 17 best_it_SIDD 89 Best_PSNR_SIDD 17.0880] 
[Ep 21 it 119	 PSNR SIDD: 17.1840	] ----  [best_Ep_SIDD 21 best_it_SIDD 119 Best_PSNR_SIDD 17.1840] 
Epoch: 21	Time: 29.1896	Loss: 34.0335	LearningRate 0.000195
[Ep 22 it 29	 PSNR SIDD: 16.7415	] ----  [best_Ep_SIDD 21 best_it_SIDD 119 Best_PSNR_SIDD 17.1840] 
[Ep 22 it 59	 PSNR SIDD: 16.8702	] ----  [best_Ep_SIDD 21 best_it_SIDD 119 Best_PSNR_SIDD 17.1840] 
[Ep 22 it 89	 PSNR SIDD: 16.7520	] ----  [best_Ep_SIDD 21 best_it_SIDD 119 Best_PSNR_SIDD 17.1840] 
[Ep 22 it 119	 PSNR SIDD: 16.5202	] ----  [best_Ep_SIDD 21 best_it_SIDD 119 Best_PSNR_SIDD 17.1840] 
Epoch: 22	Time: 27.9448	Loss: 34.1361	LearningRate 0.000195
[Ep 23 it 29	 PSNR SIDD: 17.1064	] ----  [best_Ep_SIDD 21 best_it_SIDD 119 Best_PSNR_SIDD 17.1840] 
[Ep 23 it 59	 PSNR SIDD: 16.2421	] ----  [best_Ep_SIDD 21 best_it_SIDD 119 Best_PSNR_SIDD 17.1840] 
[Ep 23 it 89	 PSNR SIDD: 16.6050	] ----  [best_Ep_SIDD 21 best_it_SIDD 119 Best_PSNR_SIDD 17.1840] 
[Ep 23 it 119	 PSNR SIDD: 16.8571	] ----  [best_Ep_SIDD 21 best_it_SIDD 119 Best_PSNR_SIDD 17.1840] 
Epoch: 23	Time: 29.3030	Loss: 34.2404	LearningRate 0.000194
[Ep 24 it 29	 PSNR SIDD: 16.7785	] ----  [best_Ep_SIDD 21 best_it_SIDD 119 Best_PSNR_SIDD 17.1840] 
[Ep 24 it 59	 PSNR SIDD: 16.9108	] ----  [best_Ep_SIDD 21 best_it_SIDD 119 Best_PSNR_SIDD 17.1840] 
[Ep 24 it 89	 PSNR SIDD: 17.2557	] ----  [best_Ep_SIDD 24 best_it_SIDD 89 Best_PSNR_SIDD 17.2557] 
[Ep 24 it 119	 PSNR SIDD: 16.9384	] ----  [best_Ep_SIDD 24 best_it_SIDD 89 Best_PSNR_SIDD 17.2557] 
Epoch: 24	Time: 28.7150	Loss: 34.2288	LearningRate 0.000194
[Ep 25 it 29	 PSNR SIDD: 16.7545	] ----  [best_Ep_SIDD 24 best_it_SIDD 89 Best_PSNR_SIDD 17.2557] 
[Ep 25 it 59	 PSNR SIDD: 16.5657	] ----  [best_Ep_SIDD 24 best_it_SIDD 89 Best_PSNR_SIDD 17.2557] 
[Ep 25 it 89	 PSNR SIDD: 16.6472	] ----  [best_Ep_SIDD 24 best_it_SIDD 89 Best_PSNR_SIDD 17.2557] 
[Ep 25 it 119	 PSNR SIDD: 16.9783	] ----  [best_Ep_SIDD 24 best_it_SIDD 89 Best_PSNR_SIDD 17.2557] 
Epoch: 25	Time: 28.9947	Loss: 33.5648	LearningRate 0.000193
[Ep 26 it 29	 PSNR SIDD: 16.8255	] ----  [best_Ep_SIDD 24 best_it_SIDD 89 Best_PSNR_SIDD 17.2557] 
[Ep 26 it 59	 PSNR SIDD: 16.7242	] ----  [best_Ep_SIDD 24 best_it_SIDD 89 Best_PSNR_SIDD 17.2557] 
[Ep 26 it 89	 PSNR SIDD: 16.7431	] ----  [best_Ep_SIDD 24 best_it_SIDD 89 Best_PSNR_SIDD 17.2557] 
[Ep 26 it 119	 PSNR SIDD: 16.7597	] ----  [best_Ep_SIDD 24 best_it_SIDD 89 Best_PSNR_SIDD 17.2557] 
Epoch: 26	Time: 28.0681	Loss: 34.6528	LearningRate 0.000193
[Ep 27 it 29	 PSNR SIDD: 16.8584	] ----  [best_Ep_SIDD 24 best_it_SIDD 89 Best_PSNR_SIDD 17.2557] 
[Ep 27 it 59	 PSNR SIDD: 16.7275	] ----  [best_Ep_SIDD 24 best_it_SIDD 89 Best_PSNR_SIDD 17.2557] 
[Ep 27 it 89	 PSNR SIDD: 16.8884	] ----  [best_Ep_SIDD 24 best_it_SIDD 89 Best_PSNR_SIDD 17.2557] 
[Ep 27 it 119	 PSNR SIDD: 16.8446	] ----  [best_Ep_SIDD 24 best_it_SIDD 89 Best_PSNR_SIDD 17.2557] 
Epoch: 27	Time: 28.1283	Loss: 32.4317	LearningRate 0.000192
[Ep 28 it 29	 PSNR SIDD: 17.2264	] ----  [best_Ep_SIDD 24 best_it_SIDD 89 Best_PSNR_SIDD 17.2557] 
[Ep 28 it 59	 PSNR SIDD: 16.9628	] ----  [best_Ep_SIDD 24 best_it_SIDD 89 Best_PSNR_SIDD 17.2557] 
[Ep 28 it 89	 PSNR SIDD: 16.8339	] ----  [best_Ep_SIDD 24 best_it_SIDD 89 Best_PSNR_SIDD 17.2557] 
[Ep 28 it 119	 PSNR SIDD: 16.9750	] ----  [best_Ep_SIDD 24 best_it_SIDD 89 Best_PSNR_SIDD 17.2557] 
Epoch: 28	Time: 28.4240	Loss: 33.6858	LearningRate 0.000192
[Ep 29 it 29	 PSNR SIDD: 16.6877	] ----  [best_Ep_SIDD 24 best_it_SIDD 89 Best_PSNR_SIDD 17.2557] 
[Ep 29 it 59	 PSNR SIDD: 16.6410	] ----  [best_Ep_SIDD 24 best_it_SIDD 89 Best_PSNR_SIDD 17.2557] 
[Ep 29 it 89	 PSNR SIDD: 16.9595	] ----  [best_Ep_SIDD 24 best_it_SIDD 89 Best_PSNR_SIDD 17.2557] 
[Ep 29 it 119	 PSNR SIDD: 17.2018	] ----  [best_Ep_SIDD 24 best_it_SIDD 89 Best_PSNR_SIDD 17.2557] 
Epoch: 29	Time: 28.5802	Loss: 33.5356	LearningRate 0.000191
[Ep 30 it 29	 PSNR SIDD: 16.2498	] ----  [best_Ep_SIDD 24 best_it_SIDD 89 Best_PSNR_SIDD 17.2557] 
[Ep 30 it 59	 PSNR SIDD: 16.7221	] ----  [best_Ep_SIDD 24 best_it_SIDD 89 Best_PSNR_SIDD 17.2557] 
[Ep 30 it 89	 PSNR SIDD: 16.9100	] ----  [best_Ep_SIDD 24 best_it_SIDD 89 Best_PSNR_SIDD 17.2557] 
[Ep 30 it 119	 PSNR SIDD: 16.8683	] ----  [best_Ep_SIDD 24 best_it_SIDD 89 Best_PSNR_SIDD 17.2557] 
Epoch: 30	Time: 28.6551	Loss: 33.3269	LearningRate 0.000190
[Ep 31 it 29	 PSNR SIDD: 17.5028	] ----  [best_Ep_SIDD 31 best_it_SIDD 29 Best_PSNR_SIDD 17.5028] 
[Ep 31 it 59	 PSNR SIDD: 16.4094	] ----  [best_Ep_SIDD 31 best_it_SIDD 29 Best_PSNR_SIDD 17.5028] 
[Ep 31 it 89	 PSNR SIDD: 16.6562	] ----  [best_Ep_SIDD 31 best_it_SIDD 29 Best_PSNR_SIDD 17.5028] 
[Ep 31 it 119	 PSNR SIDD: 16.8714	] ----  [best_Ep_SIDD 31 best_it_SIDD 29 Best_PSNR_SIDD 17.5028] 
Epoch: 31	Time: 29.3718	Loss: 33.3420	LearningRate 0.000190
[Ep 32 it 29	 PSNR SIDD: 16.9736	] ----  [best_Ep_SIDD 31 best_it_SIDD 29 Best_PSNR_SIDD 17.5028] 
[Ep 32 it 59	 PSNR SIDD: 17.1237	] ----  [best_Ep_SIDD 31 best_it_SIDD 29 Best_PSNR_SIDD 17.5028] 
[Ep 32 it 89	 PSNR SIDD: 17.0406	] ----  [best_Ep_SIDD 31 best_it_SIDD 29 Best_PSNR_SIDD 17.5028] 
[Ep 32 it 119	 PSNR SIDD: 16.8997	] ----  [best_Ep_SIDD 31 best_it_SIDD 29 Best_PSNR_SIDD 17.5028] 
Epoch: 32	Time: 26.6303	Loss: 32.3698	LearningRate 0.000189
[Ep 33 it 29	 PSNR SIDD: 16.4692	] ----  [best_Ep_SIDD 31 best_it_SIDD 29 Best_PSNR_SIDD 17.5028] 
[Ep 33 it 59	 PSNR SIDD: 17.0097	] ----  [best_Ep_SIDD 31 best_it_SIDD 29 Best_PSNR_SIDD 17.5028] 
[Ep 33 it 89	 PSNR SIDD: 16.7298	] ----  [best_Ep_SIDD 31 best_it_SIDD 29 Best_PSNR_SIDD 17.5028] 
[Ep 33 it 119	 PSNR SIDD: 17.0935	] ----  [best_Ep_SIDD 31 best_it_SIDD 29 Best_PSNR_SIDD 17.5028] 
Epoch: 33	Time: 27.7621	Loss: 33.5384	LearningRate 0.000188
[Ep 34 it 29	 PSNR SIDD: 16.6615	] ----  [best_Ep_SIDD 31 best_it_SIDD 29 Best_PSNR_SIDD 17.5028] 
[Ep 34 it 59	 PSNR SIDD: 17.2013	] ----  [best_Ep_SIDD 31 best_it_SIDD 29 Best_PSNR_SIDD 17.5028] 
[Ep 34 it 89	 PSNR SIDD: 17.0917	] ----  [best_Ep_SIDD 31 best_it_SIDD 29 Best_PSNR_SIDD 17.5028] 
[Ep 34 it 119	 PSNR SIDD: 16.4487	] ----  [best_Ep_SIDD 31 best_it_SIDD 29 Best_PSNR_SIDD 17.5028] 
Epoch: 34	Time: 28.6817	Loss: 33.2842	LearningRate 0.000187
[Ep 35 it 29	 PSNR SIDD: 16.9890	] ----  [best_Ep_SIDD 31 best_it_SIDD 29 Best_PSNR_SIDD 17.5028] 
[Ep 35 it 59	 PSNR SIDD: 17.1698	] ----  [best_Ep_SIDD 31 best_it_SIDD 29 Best_PSNR_SIDD 17.5028] 
[Ep 35 it 89	 PSNR SIDD: 17.1078	] ----  [best_Ep_SIDD 31 best_it_SIDD 29 Best_PSNR_SIDD 17.5028] 
[Ep 35 it 119	 PSNR SIDD: 16.9752	] ----  [best_Ep_SIDD 31 best_it_SIDD 29 Best_PSNR_SIDD 17.5028] 
Epoch: 35	Time: 29.2131	Loss: 33.5595	LearningRate 0.000187
