Namespace(arch='Uformer', att_se=False, batch_size=32, checkpoint=50, dataset='SIDD', embed_dim=32, env='32_1009_3_0', eval_workers=8, global_skip=False, gpu='0', local_skip=False, lr_initial=2e-05, mode='denoising', nepoch=150, norm_layer='nn.LayerNorm', optimizer='adamw', pretrain_weights='./log/Uformer32/models/model_best.pth', resume=False, save_dir='/home/ma-user/work/deNoTr/log', save_images=False, token_mlp='leff', token_projection='linear', train_dir='/home/mist/lowlight/datasets/lol_stage0/train', train_ps=64, train_workers=16, val_dir='/home/mist/lowlight/datasets/lol_stage0/valid', vit_depth=12, vit_dim=256, vit_mlp_dim=512, vit_nheads=8, vit_patch_size=16, vit_share=False, warmup=True, warmup_epochs=3, weight_decay=0.02, win_size=8)
Uformer(
  embed_dim=32, token_projection=linear, token_mlp=leff,win_size=8
  (pos_drop): Dropout(p=0.0, inplace=False)
  (input_proj): InputProj(
    (proj): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): LeakyReLU(negative_slope=0.01, inplace=True)
    )
  )
  (output_proj): OutputProj(
    (proj): Sequential(
      (0): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (encoderlayer_0): ConvBlock(
    (block): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): LeakyReLU(negative_slope=0.01, inplace=True)
      (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (conv11): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
  )
  (skipconnection_0): BasicUformerLayer(
    dim=32, input_resolution=(64, 64), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=32, input_resolution=(64, 64), num_heads=1, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=32, win_size=(8, 8), num_heads=1
          (qkv): LinearProjection(
            (to_q): Linear(in_features=32, out_features=32, bias=True)
            (to_kv): Linear(in_features=32, out_features=64, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=32, out_features=32, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=32, out_features=128, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=128, out_features=32, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=32, input_resolution=(64, 64), num_heads=1, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=32, win_size=(8, 8), num_heads=1
          (qkv): LinearProjection(
            (to_q): Linear(in_features=32, out_features=32, bias=True)
            (to_kv): Linear(in_features=32, out_features=64, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=32, out_features=32, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=32, out_features=128, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=128, out_features=32, bias=True)
          )
        )
      )
    )
  )
  (dowsample_0): Downsample(
    (conv): Sequential(
      (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (encoderlayer_1): ConvBlock(
    (block): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): LeakyReLU(negative_slope=0.01, inplace=True)
      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (conv11): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (skipconnection_1): BasicUformerLayer(
    dim=64, input_resolution=(32, 32), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=64, input_resolution=(32, 32), num_heads=2, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=64, win_size=(8, 8), num_heads=2
          (qkv): LinearProjection(
            (to_q): Linear(in_features=64, out_features=64, bias=True)
            (to_kv): Linear(in_features=64, out_features=128, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=64, out_features=256, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=256, out_features=64, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=64, input_resolution=(32, 32), num_heads=2, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=64, win_size=(8, 8), num_heads=2
          (qkv): LinearProjection(
            (to_q): Linear(in_features=64, out_features=64, bias=True)
            (to_kv): Linear(in_features=64, out_features=128, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=64, out_features=256, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=256, out_features=64, bias=True)
          )
        )
      )
    )
  )
  (dowsample_1): Downsample(
    (conv): Sequential(
      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (encoderlayer_2): ConvBlock(
    (block): Sequential(
      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): LeakyReLU(negative_slope=0.01, inplace=True)
      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (conv11): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
  )
  (skipconnection_2): BasicUformerLayer(
    dim=128, input_resolution=(16, 16), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=128, input_resolution=(16, 16), num_heads=4, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=128, win_size=(8, 8), num_heads=4
          (qkv): LinearProjection(
            (to_q): Linear(in_features=128, out_features=128, bias=True)
            (to_kv): Linear(in_features=128, out_features=256, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=128, out_features=512, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=128, input_resolution=(16, 16), num_heads=4, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=128, win_size=(8, 8), num_heads=4
          (qkv): LinearProjection(
            (to_q): Linear(in_features=128, out_features=128, bias=True)
            (to_kv): Linear(in_features=128, out_features=256, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=128, out_features=512, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
          )
        )
      )
    )
  )
  (dowsample_2): Downsample(
    (conv): Sequential(
      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (encoderlayer_3): ConvBlock(
    (block): Sequential(
      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): LeakyReLU(negative_slope=0.01, inplace=True)
      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (conv11): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
  )
  (skipconnection_3): BasicUformerLayer(
    dim=256, input_resolution=(8, 8), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=256, input_resolution=(8, 8), num_heads=8, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=256, win_size=(8, 8), num_heads=8
          (qkv): LinearProjection(
            (to_q): Linear(in_features=256, out_features=256, bias=True)
            (to_kv): Linear(in_features=256, out_features=512, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=1024, out_features=256, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=256, input_resolution=(8, 8), num_heads=8, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=256, win_size=(8, 8), num_heads=8
          (qkv): LinearProjection(
            (to_q): Linear(in_features=256, out_features=256, bias=True)
            (to_kv): Linear(in_features=256, out_features=512, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=1024, out_features=256, bias=True)
          )
        )
      )
    )
  )
  (dowsample_3): Downsample(
    (conv): Sequential(
      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (resize_0): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (buf_0): ConvBlock_1(
    (block): Sequential(
      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): LeakyReLU(negative_slope=0.01, inplace=True)
      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (conv11): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
  )
  (resize_1): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (buf_1): ConvBlock_1(
    (block): Sequential(
      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): LeakyReLU(negative_slope=0.01, inplace=True)
      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (conv11): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
  )
  (resize_2): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (buf_2): ConvBlock_1(
    (block): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): LeakyReLU(negative_slope=0.01, inplace=True)
      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (conv11): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (resize_3): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (buf_3): ConvBlock_1(
    (block): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): LeakyReLU(negative_slope=0.01, inplace=True)
      (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (conv11): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
  )
  (resize_4): OutputProj(
    (proj): Sequential(
      (0): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (recover_0): InputProj(
    (proj): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): LeakyReLU(negative_slope=0.01, inplace=True)
    )
  )
  (conv): BasicUformerLayer(
    dim=32, input_resolution=(64, 64), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=32, input_resolution=(64, 64), num_heads=16, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=32, win_size=(8, 8), num_heads=16
          (qkv): LinearProjection(
            (to_q): Linear(in_features=32, out_features=32, bias=True)
            (to_kv): Linear(in_features=32, out_features=64, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=32, out_features=32, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=32, out_features=128, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=128, out_features=32, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=32, input_resolution=(64, 64), num_heads=16, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=32, win_size=(8, 8), num_heads=16
          (qkv): LinearProjection(
            (to_q): Linear(in_features=32, out_features=32, bias=True)
            (to_kv): Linear(in_features=32, out_features=64, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=32, out_features=32, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=32, out_features=128, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=128, out_features=32, bias=True)
          )
        )
      )
    )
  )
  (recover_1): Downsample(
    (conv): Sequential(
      (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (recover_2): Downsample(
    (conv): Sequential(
      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (recover_3): Downsample(
    (conv): Sequential(
      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (recover_4): Downsample(
    (conv): Sequential(
      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (upsample_0): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (decoderlayer_0): BasicUformerLayer(
    dim=512, input_resolution=(8, 8), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=512, input_resolution=(8, 8), num_heads=16, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=512, win_size=(8, 8), num_heads=16
          (qkv): LinearProjection(
            (to_q): Linear(in_features=512, out_features=512, bias=True)
            (to_kv): Linear(in_features=512, out_features=1024, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=512, out_features=2048, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=512, input_resolution=(8, 8), num_heads=16, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=512, win_size=(8, 8), num_heads=16
          (qkv): LinearProjection(
            (to_q): Linear(in_features=512, out_features=512, bias=True)
            (to_kv): Linear(in_features=512, out_features=1024, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=512, out_features=2048, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
      )
    )
  )
  (upsample_1): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(512, 128, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (decoderlayer_1): BasicUformerLayer(
    dim=256, input_resolution=(16, 16), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=256, input_resolution=(16, 16), num_heads=8, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=256, win_size=(8, 8), num_heads=8
          (qkv): LinearProjection(
            (to_q): Linear(in_features=256, out_features=256, bias=True)
            (to_kv): Linear(in_features=256, out_features=512, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=1024, out_features=256, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=256, input_resolution=(16, 16), num_heads=8, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=256, win_size=(8, 8), num_heads=8
          (qkv): LinearProjection(
            (to_q): Linear(in_features=256, out_features=256, bias=True)
            (to_kv): Linear(in_features=256, out_features=512, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=1024, out_features=256, bias=True)
          )
        )
      )
    )
  )
  (upsample_2): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (decoderlayer_2): BasicUformerLayer(
    dim=128, input_resolution=(32, 32), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=128, input_resolution=(32, 32), num_heads=4, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=128, win_size=(8, 8), num_heads=4
          (qkv): LinearProjection(
            (to_q): Linear(in_features=128, out_features=128, bias=True)
            (to_kv): Linear(in_features=128, out_features=256, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=128, out_features=512, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=128, input_resolution=(32, 32), num_heads=4, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=128, win_size=(8, 8), num_heads=4
          (qkv): LinearProjection(
            (to_q): Linear(in_features=128, out_features=128, bias=True)
            (to_kv): Linear(in_features=128, out_features=256, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=128, out_features=512, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
          )
        )
      )
    )
  )
  (upsample_3): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(128, 32, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (decoderlayer_3): BasicUformerLayer(
    dim=64, input_resolution=(64, 64), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=64, input_resolution=(64, 64), num_heads=2, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=64, win_size=(8, 8), num_heads=2
          (qkv): LinearProjection(
            (to_q): Linear(in_features=64, out_features=64, bias=True)
            (to_kv): Linear(in_features=64, out_features=128, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=64, out_features=256, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=256, out_features=64, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=64, input_resolution=(64, 64), num_heads=2, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=64, win_size=(8, 8), num_heads=2
          (qkv): LinearProjection(
            (to_q): Linear(in_features=64, out_features=64, bias=True)
            (to_kv): Linear(in_features=64, out_features=128, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=64, out_features=256, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=256, out_features=64, bias=True)
          )
        )
      )
    )
  )
)
[Ep 1 it 29	 PSNR SIDD: 3.3240	] ----  [best_Ep_SIDD 1 best_it_SIDD 29 Best_PSNR_SIDD 3.3240] 
[Ep 1 it 59	 PSNR SIDD: 3.4679	] ----  [best_Ep_SIDD 1 best_it_SIDD 59 Best_PSNR_SIDD 3.4679] 
[Ep 1 it 89	 PSNR SIDD: 4.3099	] ----  [best_Ep_SIDD 1 best_it_SIDD 89 Best_PSNR_SIDD 4.3099] 
[Ep 1 it 119	 PSNR SIDD: 8.3422	] ----  [best_Ep_SIDD 1 best_it_SIDD 119 Best_PSNR_SIDD 8.3422] 
Epoch: 1	Time: 16.6796	Loss: 729.3012	LearningRate 0.000013
[Ep 2 it 29	 PSNR SIDD: 8.5972	] ----  [best_Ep_SIDD 2 best_it_SIDD 29 Best_PSNR_SIDD 8.5972] 
[Ep 2 it 59	 PSNR SIDD: 9.4082	] ----  [best_Ep_SIDD 2 best_it_SIDD 59 Best_PSNR_SIDD 9.4082] 
[Ep 2 it 89	 PSNR SIDD: 9.8255	] ----  [best_Ep_SIDD 2 best_it_SIDD 89 Best_PSNR_SIDD 9.8255] 
[Ep 2 it 119	 PSNR SIDD: 9.9654	] ----  [best_Ep_SIDD 2 best_it_SIDD 119 Best_PSNR_SIDD 9.9654] 
Epoch: 2	Time: 15.4992	Loss: 140.4359	LearningRate 0.000020
[Ep 3 it 29	 PSNR SIDD: 10.0024	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 10.0024] 
[Ep 3 it 59	 PSNR SIDD: 10.2250	] ----  [best_Ep_SIDD 3 best_it_SIDD 59 Best_PSNR_SIDD 10.2250] 
[Ep 3 it 89	 PSNR SIDD: 10.3147	] ----  [best_Ep_SIDD 3 best_it_SIDD 89 Best_PSNR_SIDD 10.3147] 
[Ep 3 it 119	 PSNR SIDD: 10.4555	] ----  [best_Ep_SIDD 3 best_it_SIDD 119 Best_PSNR_SIDD 10.4555] 
Epoch: 3	Time: 15.4618	Loss: 126.7422	LearningRate 0.000020
[Ep 4 it 29	 PSNR SIDD: 10.8514	] ----  [best_Ep_SIDD 4 best_it_SIDD 29 Best_PSNR_SIDD 10.8514] 
[Ep 4 it 59	 PSNR SIDD: 11.1696	] ----  [best_Ep_SIDD 4 best_it_SIDD 59 Best_PSNR_SIDD 11.1696] 
[Ep 4 it 89	 PSNR SIDD: 11.4215	] ----  [best_Ep_SIDD 4 best_it_SIDD 89 Best_PSNR_SIDD 11.4215] 
[Ep 4 it 119	 PSNR SIDD: 11.9247	] ----  [best_Ep_SIDD 4 best_it_SIDD 119 Best_PSNR_SIDD 11.9247] 
Epoch: 4	Time: 15.3780	Loss: 107.2948	LearningRate 0.000020
[Ep 5 it 29	 PSNR SIDD: 12.5257	] ----  [best_Ep_SIDD 5 best_it_SIDD 29 Best_PSNR_SIDD 12.5257] 
[Ep 5 it 59	 PSNR SIDD: 13.0749	] ----  [best_Ep_SIDD 5 best_it_SIDD 59 Best_PSNR_SIDD 13.0749] 
[Ep 5 it 89	 PSNR SIDD: 13.5920	] ----  [best_Ep_SIDD 5 best_it_SIDD 89 Best_PSNR_SIDD 13.5920] 
[Ep 5 it 119	 PSNR SIDD: 13.9216	] ----  [best_Ep_SIDD 5 best_it_SIDD 119 Best_PSNR_SIDD 13.9216] 
Epoch: 5	Time: 14.8392	Loss: 83.2385	LearningRate 0.000020
[Ep 6 it 29	 PSNR SIDD: 14.0631	] ----  [best_Ep_SIDD 6 best_it_SIDD 29 Best_PSNR_SIDD 14.0631] 
[Ep 6 it 59	 PSNR SIDD: 13.9982	] ----  [best_Ep_SIDD 6 best_it_SIDD 29 Best_PSNR_SIDD 14.0631] 
[Ep 6 it 89	 PSNR SIDD: 14.2323	] ----  [best_Ep_SIDD 6 best_it_SIDD 89 Best_PSNR_SIDD 14.2323] 
[Ep 6 it 119	 PSNR SIDD: 13.8769	] ----  [best_Ep_SIDD 6 best_it_SIDD 89 Best_PSNR_SIDD 14.2323] 
Epoch: 6	Time: 14.2544	Loss: 55.9900	LearningRate 0.000020
[Ep 7 it 29	 PSNR SIDD: 14.3309	] ----  [best_Ep_SIDD 7 best_it_SIDD 29 Best_PSNR_SIDD 14.3309] 
[Ep 7 it 59	 PSNR SIDD: 14.2376	] ----  [best_Ep_SIDD 7 best_it_SIDD 29 Best_PSNR_SIDD 14.3309] 
[Ep 7 it 89	 PSNR SIDD: 14.4135	] ----  [best_Ep_SIDD 7 best_it_SIDD 89 Best_PSNR_SIDD 14.4135] 
[Ep 7 it 119	 PSNR SIDD: 14.3605	] ----  [best_Ep_SIDD 7 best_it_SIDD 89 Best_PSNR_SIDD 14.4135] 
Epoch: 7	Time: 14.5564	Loss: 53.2373	LearningRate 0.000020
[Ep 8 it 29	 PSNR SIDD: 14.3817	] ----  [best_Ep_SIDD 7 best_it_SIDD 89 Best_PSNR_SIDD 14.4135] 
[Ep 8 it 59	 PSNR SIDD: 14.0516	] ----  [best_Ep_SIDD 7 best_it_SIDD 89 Best_PSNR_SIDD 14.4135] 
[Ep 8 it 89	 PSNR SIDD: 13.9943	] ----  [best_Ep_SIDD 7 best_it_SIDD 89 Best_PSNR_SIDD 14.4135] 
[Ep 8 it 119	 PSNR SIDD: 14.3997	] ----  [best_Ep_SIDD 7 best_it_SIDD 89 Best_PSNR_SIDD 14.4135] 
Epoch: 8	Time: 12.8581	Loss: 54.5666	LearningRate 0.000020
[Ep 9 it 29	 PSNR SIDD: 14.5196	] ----  [best_Ep_SIDD 9 best_it_SIDD 29 Best_PSNR_SIDD 14.5196] 
[Ep 9 it 59	 PSNR SIDD: 14.3885	] ----  [best_Ep_SIDD 9 best_it_SIDD 29 Best_PSNR_SIDD 14.5196] 
[Ep 9 it 89	 PSNR SIDD: 14.3205	] ----  [best_Ep_SIDD 9 best_it_SIDD 29 Best_PSNR_SIDD 14.5196] 
[Ep 9 it 119	 PSNR SIDD: 14.1545	] ----  [best_Ep_SIDD 9 best_it_SIDD 29 Best_PSNR_SIDD 14.5196] 
Epoch: 9	Time: 13.1751	Loss: 51.5634	LearningRate 0.000020
[Ep 10 it 29	 PSNR SIDD: 14.5244	] ----  [best_Ep_SIDD 10 best_it_SIDD 29 Best_PSNR_SIDD 14.5244] 
[Ep 10 it 59	 PSNR SIDD: 14.5669	] ----  [best_Ep_SIDD 10 best_it_SIDD 59 Best_PSNR_SIDD 14.5669] 
[Ep 10 it 89	 PSNR SIDD: 14.5471	] ----  [best_Ep_SIDD 10 best_it_SIDD 59 Best_PSNR_SIDD 14.5669] 
[Ep 10 it 119	 PSNR SIDD: 14.6263	] ----  [best_Ep_SIDD 10 best_it_SIDD 119 Best_PSNR_SIDD 14.6263] 
Epoch: 10	Time: 14.7232	Loss: 51.7696	LearningRate 0.000020
[Ep 11 it 29	 PSNR SIDD: 14.5794	] ----  [best_Ep_SIDD 10 best_it_SIDD 119 Best_PSNR_SIDD 14.6263] 
[Ep 11 it 59	 PSNR SIDD: 14.6300	] ----  [best_Ep_SIDD 11 best_it_SIDD 59 Best_PSNR_SIDD 14.6300] 
[Ep 11 it 89	 PSNR SIDD: 13.9465	] ----  [best_Ep_SIDD 11 best_it_SIDD 59 Best_PSNR_SIDD 14.6300] 
[Ep 11 it 119	 PSNR SIDD: 14.6207	] ----  [best_Ep_SIDD 11 best_it_SIDD 59 Best_PSNR_SIDD 14.6300] 
Epoch: 11	Time: 13.6546	Loss: 50.1900	LearningRate 0.000020
[Ep 12 it 29	 PSNR SIDD: 14.6392	] ----  [best_Ep_SIDD 12 best_it_SIDD 29 Best_PSNR_SIDD 14.6392] 
[Ep 12 it 59	 PSNR SIDD: 14.7116	] ----  [best_Ep_SIDD 12 best_it_SIDD 59 Best_PSNR_SIDD 14.7116] 
[Ep 12 it 89	 PSNR SIDD: 14.6882	] ----  [best_Ep_SIDD 12 best_it_SIDD 59 Best_PSNR_SIDD 14.7116] 
[Ep 12 it 119	 PSNR SIDD: 14.6843	] ----  [best_Ep_SIDD 12 best_it_SIDD 59 Best_PSNR_SIDD 14.7116] 
Epoch: 12	Time: 14.5292	Loss: 50.9213	LearningRate 0.000020
[Ep 13 it 29	 PSNR SIDD: 14.7241	] ----  [best_Ep_SIDD 13 best_it_SIDD 29 Best_PSNR_SIDD 14.7241] 
[Ep 13 it 59	 PSNR SIDD: 14.6889	] ----  [best_Ep_SIDD 13 best_it_SIDD 29 Best_PSNR_SIDD 14.7241] 
[Ep 13 it 89	 PSNR SIDD: 14.1092	] ----  [best_Ep_SIDD 13 best_it_SIDD 29 Best_PSNR_SIDD 14.7241] 
[Ep 13 it 119	 PSNR SIDD: 14.7946	] ----  [best_Ep_SIDD 13 best_it_SIDD 119 Best_PSNR_SIDD 14.7946] 
Epoch: 13	Time: 14.0495	Loss: 49.5000	LearningRate 0.000020
[Ep 14 it 29	 PSNR SIDD: 14.6493	] ----  [best_Ep_SIDD 13 best_it_SIDD 119 Best_PSNR_SIDD 14.7946] 
[Ep 14 it 59	 PSNR SIDD: 14.6658	] ----  [best_Ep_SIDD 13 best_it_SIDD 119 Best_PSNR_SIDD 14.7946] 
[Ep 14 it 89	 PSNR SIDD: 14.4713	] ----  [best_Ep_SIDD 13 best_it_SIDD 119 Best_PSNR_SIDD 14.7946] 
[Ep 14 it 119	 PSNR SIDD: 14.8631	] ----  [best_Ep_SIDD 14 best_it_SIDD 119 Best_PSNR_SIDD 14.8631] 
Epoch: 14	Time: 14.4762	Loss: 49.1322	LearningRate 0.000020
[Ep 15 it 29	 PSNR SIDD: 14.6576	] ----  [best_Ep_SIDD 14 best_it_SIDD 119 Best_PSNR_SIDD 14.8631] 
[Ep 15 it 59	 PSNR SIDD: 14.7431	] ----  [best_Ep_SIDD 14 best_it_SIDD 119 Best_PSNR_SIDD 14.8631] 
[Ep 15 it 89	 PSNR SIDD: 14.9322	] ----  [best_Ep_SIDD 15 best_it_SIDD 89 Best_PSNR_SIDD 14.9322] 
[Ep 15 it 119	 PSNR SIDD: 14.7931	] ----  [best_Ep_SIDD 15 best_it_SIDD 89 Best_PSNR_SIDD 14.9322] 
Epoch: 15	Time: 14.6200	Loss: 49.9985	LearningRate 0.000020
[Ep 16 it 29	 PSNR SIDD: 14.4685	] ----  [best_Ep_SIDD 15 best_it_SIDD 89 Best_PSNR_SIDD 14.9322] 
[Ep 16 it 59	 PSNR SIDD: 14.5472	] ----  [best_Ep_SIDD 15 best_it_SIDD 89 Best_PSNR_SIDD 14.9322] 
[Ep 16 it 89	 PSNR SIDD: 14.7217	] ----  [best_Ep_SIDD 15 best_it_SIDD 89 Best_PSNR_SIDD 14.9322] 
[Ep 16 it 119	 PSNR SIDD: 14.6642	] ----  [best_Ep_SIDD 15 best_it_SIDD 89 Best_PSNR_SIDD 14.9322] 
Epoch: 16	Time: 13.8704	Loss: 48.4560	LearningRate 0.000020
[Ep 17 it 29	 PSNR SIDD: 14.8320	] ----  [best_Ep_SIDD 15 best_it_SIDD 89 Best_PSNR_SIDD 14.9322] 
[Ep 17 it 59	 PSNR SIDD: 14.8587	] ----  [best_Ep_SIDD 15 best_it_SIDD 89 Best_PSNR_SIDD 14.9322] 
[Ep 17 it 89	 PSNR SIDD: 14.8492	] ----  [best_Ep_SIDD 15 best_it_SIDD 89 Best_PSNR_SIDD 14.9322] 
[Ep 17 it 119	 PSNR SIDD: 14.4013	] ----  [best_Ep_SIDD 15 best_it_SIDD 89 Best_PSNR_SIDD 14.9322] 
Epoch: 17	Time: 13.5443	Loss: 48.6946	LearningRate 0.000020
[Ep 18 it 29	 PSNR SIDD: 14.2262	] ----  [best_Ep_SIDD 15 best_it_SIDD 89 Best_PSNR_SIDD 14.9322] 
[Ep 18 it 59	 PSNR SIDD: 14.8019	] ----  [best_Ep_SIDD 15 best_it_SIDD 89 Best_PSNR_SIDD 14.9322] 
[Ep 18 it 89	 PSNR SIDD: 14.6428	] ----  [best_Ep_SIDD 15 best_it_SIDD 89 Best_PSNR_SIDD 14.9322] 
[Ep 18 it 119	 PSNR SIDD: 14.5635	] ----  [best_Ep_SIDD 15 best_it_SIDD 89 Best_PSNR_SIDD 14.9322] 
Epoch: 18	Time: 14.3711	Loss: 48.9033	LearningRate 0.000019
[Ep 19 it 29	 PSNR SIDD: 14.0841	] ----  [best_Ep_SIDD 15 best_it_SIDD 89 Best_PSNR_SIDD 14.9322] 
[Ep 19 it 59	 PSNR SIDD: 15.0193	] ----  [best_Ep_SIDD 19 best_it_SIDD 59 Best_PSNR_SIDD 15.0193] 
[Ep 19 it 89	 PSNR SIDD: 14.4895	] ----  [best_Ep_SIDD 19 best_it_SIDD 59 Best_PSNR_SIDD 15.0193] 
[Ep 19 it 119	 PSNR SIDD: 15.0325	] ----  [best_Ep_SIDD 19 best_it_SIDD 119 Best_PSNR_SIDD 15.0325] 
Epoch: 19	Time: 14.7834	Loss: 49.1429	LearningRate 0.000019
[Ep 20 it 29	 PSNR SIDD: 14.8669	] ----  [best_Ep_SIDD 19 best_it_SIDD 119 Best_PSNR_SIDD 15.0325] 
[Ep 20 it 59	 PSNR SIDD: 14.7859	] ----  [best_Ep_SIDD 19 best_it_SIDD 119 Best_PSNR_SIDD 15.0325] 
[Ep 20 it 89	 PSNR SIDD: 14.3483	] ----  [best_Ep_SIDD 19 best_it_SIDD 119 Best_PSNR_SIDD 15.0325] 
[Ep 20 it 119	 PSNR SIDD: 14.9681	] ----  [best_Ep_SIDD 19 best_it_SIDD 119 Best_PSNR_SIDD 15.0325] 
Epoch: 20	Time: 13.1841	Loss: 47.0935	LearningRate 0.000019
[Ep 21 it 29	 PSNR SIDD: 14.9231	] ----  [best_Ep_SIDD 19 best_it_SIDD 119 Best_PSNR_SIDD 15.0325] 
[Ep 21 it 59	 PSNR SIDD: 14.3410	] ----  [best_Ep_SIDD 19 best_it_SIDD 119 Best_PSNR_SIDD 15.0325] 
[Ep 21 it 89	 PSNR SIDD: 14.5396	] ----  [best_Ep_SIDD 19 best_it_SIDD 119 Best_PSNR_SIDD 15.0325] 
[Ep 21 it 119	 PSNR SIDD: 15.0467	] ----  [best_Ep_SIDD 21 best_it_SIDD 119 Best_PSNR_SIDD 15.0467] 
Epoch: 21	Time: 14.7695	Loss: 47.4555	LearningRate 0.000019
[Ep 22 it 29	 PSNR SIDD: 13.8083	] ----  [best_Ep_SIDD 21 best_it_SIDD 119 Best_PSNR_SIDD 15.0467] 
[Ep 22 it 59	 PSNR SIDD: 14.4386	] ----  [best_Ep_SIDD 21 best_it_SIDD 119 Best_PSNR_SIDD 15.0467] 
[Ep 22 it 89	 PSNR SIDD: 14.7475	] ----  [best_Ep_SIDD 21 best_it_SIDD 119 Best_PSNR_SIDD 15.0467] 
[Ep 22 it 119	 PSNR SIDD: 14.5024	] ----  [best_Ep_SIDD 21 best_it_SIDD 119 Best_PSNR_SIDD 15.0467] 
Epoch: 22	Time: 14.3358	Loss: 46.6774	LearningRate 0.000019
[Ep 23 it 29	 PSNR SIDD: 14.2967	] ----  [best_Ep_SIDD 21 best_it_SIDD 119 Best_PSNR_SIDD 15.0467] 
[Ep 23 it 59	 PSNR SIDD: 15.1563	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 15.1563] 
[Ep 23 it 89	 PSNR SIDD: 14.7117	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 15.1563] 
[Ep 23 it 119	 PSNR SIDD: 14.3163	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 15.1563] 
Epoch: 23	Time: 14.2963	Loss: 46.8607	LearningRate 0.000019
[Ep 24 it 29	 PSNR SIDD: 14.6461	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 15.1563] 
[Ep 24 it 59	 PSNR SIDD: 14.7426	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 15.1563] 
[Ep 24 it 89	 PSNR SIDD: 14.7605	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 15.1563] 
[Ep 24 it 119	 PSNR SIDD: 14.3223	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 15.1563] 
Epoch: 24	Time: 13.4925	Loss: 46.3278	LearningRate 0.000019
[Ep 25 it 29	 PSNR SIDD: 15.0996	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 15.1563] 
[Ep 25 it 59	 PSNR SIDD: 14.9880	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 15.1563] 
[Ep 25 it 89	 PSNR SIDD: 15.0025	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 15.1563] 
[Ep 25 it 119	 PSNR SIDD: 14.5222	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 15.1563] 
Epoch: 25	Time: 13.6641	Loss: 46.6269	LearningRate 0.000019
[Ep 26 it 29	 PSNR SIDD: 14.1719	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 15.1563] 
[Ep 26 it 59	 PSNR SIDD: 14.8317	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 15.1563] 
[Ep 26 it 89	 PSNR SIDD: 15.0592	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 15.1563] 
[Ep 26 it 119	 PSNR SIDD: 14.6389	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 15.1563] 
Epoch: 26	Time: 12.4159	Loss: 45.4966	LearningRate 0.000019
[Ep 27 it 29	 PSNR SIDD: 15.1366	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 15.1563] 
[Ep 27 it 59	 PSNR SIDD: 14.5182	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 15.1563] 
[Ep 27 it 89	 PSNR SIDD: 14.4836	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 15.1563] 
[Ep 27 it 119	 PSNR SIDD: 14.9080	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 15.1563] 
Epoch: 27	Time: 13.7462	Loss: 46.0972	LearningRate 0.000019
[Ep 28 it 29	 PSNR SIDD: 14.1288	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 15.1563] 
[Ep 28 it 59	 PSNR SIDD: 14.3844	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 15.1563] 
[Ep 28 it 89	 PSNR SIDD: 14.7918	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 15.1563] 
[Ep 28 it 119	 PSNR SIDD: 14.8502	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 15.1563] 
Epoch: 28	Time: 14.1437	Loss: 45.3055	LearningRate 0.000019
[Ep 29 it 29	 PSNR SIDD: 14.6748	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 15.1563] 
[Ep 29 it 59	 PSNR SIDD: 14.0619	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 15.1563] 
[Ep 29 it 89	 PSNR SIDD: 14.7649	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 15.1563] 
[Ep 29 it 119	 PSNR SIDD: 14.2519	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 15.1563] 
Epoch: 29	Time: 13.1016	Loss: 45.6047	LearningRate 0.000018
[Ep 30 it 29	 PSNR SIDD: 14.3493	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 15.1563] 
[Ep 30 it 59	 PSNR SIDD: 14.8663	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 15.1563] 
[Ep 30 it 89	 PSNR SIDD: 14.6731	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 15.1563] 
[Ep 30 it 119	 PSNR SIDD: 14.4461	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 15.1563] 
Epoch: 30	Time: 13.4707	Loss: 45.7939	LearningRate 0.000018
[Ep 31 it 29	 PSNR SIDD: 14.5116	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 15.1563] 
[Ep 31 it 59	 PSNR SIDD: 14.2920	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 15.1563] 
[Ep 31 it 89	 PSNR SIDD: 15.0207	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 15.1563] 
[Ep 31 it 119	 PSNR SIDD: 14.6640	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 15.1563] 
Epoch: 31	Time: 14.1614	Loss: 44.5268	LearningRate 0.000018
[Ep 32 it 29	 PSNR SIDD: 14.9068	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 15.1563] 
[Ep 32 it 59	 PSNR SIDD: 14.8181	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 15.1563] 
[Ep 32 it 89	 PSNR SIDD: 14.6083	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 15.1563] 
[Ep 32 it 119	 PSNR SIDD: 14.8984	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 15.1563] 
Epoch: 32	Time: 13.9452	Loss: 45.3702	LearningRate 0.000018
[Ep 33 it 29	 PSNR SIDD: 14.2689	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 15.1563] 
[Ep 33 it 59	 PSNR SIDD: 14.4746	] ----  [best_Ep_SIDD 23 best_it_SIDD 59 Best_PSNR_SIDD 15.1563] 
[Ep 33 it 89	 PSNR SIDD: 15.2139	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 33 it 119	 PSNR SIDD: 14.6431	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
Epoch: 33	Time: 14.0865	Loss: 44.9286	LearningRate 0.000018
[Ep 34 it 29	 PSNR SIDD: 14.2926	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 34 it 59	 PSNR SIDD: 14.4126	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 34 it 89	 PSNR SIDD: 14.7297	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 34 it 119	 PSNR SIDD: 14.3262	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
Epoch: 34	Time: 14.2523	Loss: 45.2066	LearningRate 0.000018
[Ep 35 it 29	 PSNR SIDD: 14.7633	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 35 it 59	 PSNR SIDD: 14.5601	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 35 it 89	 PSNR SIDD: 14.2531	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 35 it 119	 PSNR SIDD: 14.7750	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
Epoch: 35	Time: 13.4016	Loss: 43.6846	LearningRate 0.000018
[Ep 36 it 29	 PSNR SIDD: 14.6607	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 36 it 59	 PSNR SIDD: 14.7570	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 36 it 89	 PSNR SIDD: 14.5805	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 36 it 119	 PSNR SIDD: 15.0435	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
Epoch: 36	Time: 13.8306	Loss: 45.5627	LearningRate 0.000018
[Ep 37 it 29	 PSNR SIDD: 13.9235	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 37 it 59	 PSNR SIDD: 14.3926	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 37 it 89	 PSNR SIDD: 14.7219	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 37 it 119	 PSNR SIDD: 14.2445	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
Epoch: 37	Time: 13.1024	Loss: 46.3395	LearningRate 0.000017
[Ep 38 it 29	 PSNR SIDD: 14.3824	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 38 it 59	 PSNR SIDD: 14.9241	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 38 it 89	 PSNR SIDD: 14.8568	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 38 it 119	 PSNR SIDD: 14.4348	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
Epoch: 38	Time: 13.6196	Loss: 43.5882	LearningRate 0.000017
[Ep 39 it 29	 PSNR SIDD: 15.1884	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 39 it 59	 PSNR SIDD: 14.3607	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 39 it 89	 PSNR SIDD: 14.0432	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 39 it 119	 PSNR SIDD: 14.5206	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
Epoch: 39	Time: 14.1535	Loss: 44.5733	LearningRate 0.000017
[Ep 40 it 29	 PSNR SIDD: 14.9218	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 40 it 59	 PSNR SIDD: 15.0507	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 40 it 89	 PSNR SIDD: 14.9249	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 40 it 119	 PSNR SIDD: 14.5467	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
Epoch: 40	Time: 14.3121	Loss: 44.1947	LearningRate 0.000017
[Ep 41 it 29	 PSNR SIDD: 14.4997	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 41 it 59	 PSNR SIDD: 14.6727	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 41 it 89	 PSNR SIDD: 14.6936	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 41 it 119	 PSNR SIDD: 13.7874	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
Epoch: 41	Time: 14.1781	Loss: 43.7086	LearningRate 0.000017
[Ep 42 it 29	 PSNR SIDD: 14.8268	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 42 it 59	 PSNR SIDD: 15.0909	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 42 it 89	 PSNR SIDD: 14.6887	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 42 it 119	 PSNR SIDD: 14.6172	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
Epoch: 42	Time: 13.8253	Loss: 43.8659	LearningRate 0.000017
[Ep 43 it 29	 PSNR SIDD: 14.3582	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 43 it 59	 PSNR SIDD: 14.3529	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 43 it 89	 PSNR SIDD: 15.1157	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 43 it 119	 PSNR SIDD: 15.1499	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
Epoch: 43	Time: 13.2046	Loss: 43.7895	LearningRate 0.000017
[Ep 44 it 29	 PSNR SIDD: 15.0339	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 44 it 59	 PSNR SIDD: 13.9786	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 44 it 89	 PSNR SIDD: 13.5038	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 44 it 119	 PSNR SIDD: 14.4981	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
Epoch: 44	Time: 13.5829	Loss: 42.4315	LearningRate 0.000016
[Ep 45 it 29	 PSNR SIDD: 14.9843	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 45 it 59	 PSNR SIDD: 14.3655	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 45 it 89	 PSNR SIDD: 14.1579	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 45 it 119	 PSNR SIDD: 15.1900	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
Epoch: 45	Time: 14.0021	Loss: 42.8629	LearningRate 0.000016
[Ep 46 it 29	 PSNR SIDD: 14.5535	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 46 it 59	 PSNR SIDD: 14.8187	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 46 it 89	 PSNR SIDD: 14.5740	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 46 it 119	 PSNR SIDD: 14.9955	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
Epoch: 46	Time: 13.9260	Loss: 44.1614	LearningRate 0.000016
[Ep 47 it 29	 PSNR SIDD: 14.4430	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 47 it 59	 PSNR SIDD: 14.4020	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 47 it 89	 PSNR SIDD: 14.8976	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 47 it 119	 PSNR SIDD: 15.0069	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
Epoch: 47	Time: 13.0011	Loss: 42.5685	LearningRate 0.000016
[Ep 48 it 29	 PSNR SIDD: 14.1733	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 48 it 59	 PSNR SIDD: 14.7648	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 48 it 89	 PSNR SIDD: 14.6072	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 48 it 119	 PSNR SIDD: 14.0673	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
Epoch: 48	Time: 13.6425	Loss: 42.2808	LearningRate 0.000016
[Ep 49 it 29	 PSNR SIDD: 15.0144	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 49 it 59	 PSNR SIDD: 15.0332	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 49 it 89	 PSNR SIDD: 13.5542	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 49 it 119	 PSNR SIDD: 14.4991	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
Epoch: 49	Time: 13.9969	Loss: 42.6381	LearningRate 0.000016
[Ep 50 it 29	 PSNR SIDD: 14.8599	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 50 it 59	 PSNR SIDD: 14.3573	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 50 it 89	 PSNR SIDD: 14.7280	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 50 it 119	 PSNR SIDD: 14.7787	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
Epoch: 50	Time: 14.1071	Loss: 42.3946	LearningRate 0.000015
[Ep 51 it 29	 PSNR SIDD: 14.9446	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 51 it 59	 PSNR SIDD: 14.7325	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 51 it 89	 PSNR SIDD: 14.3619	] ----  [best_Ep_SIDD 33 best_it_SIDD 89 Best_PSNR_SIDD 15.2139] 
[Ep 51 it 119	 PSNR SIDD: 15.4592	] ----  [best_Ep_SIDD 51 best_it_SIDD 119 Best_PSNR_SIDD 15.4592] 
Epoch: 51	Time: 15.1154	Loss: 42.6380	LearningRate 0.000015
[Ep 52 it 29	 PSNR SIDD: 14.6111	] ----  [best_Ep_SIDD 51 best_it_SIDD 119 Best_PSNR_SIDD 15.4592] 
[Ep 52 it 59	 PSNR SIDD: 14.6866	] ----  [best_Ep_SIDD 51 best_it_SIDD 119 Best_PSNR_SIDD 15.4592] 
[Ep 52 it 89	 PSNR SIDD: 15.1680	] ----  [best_Ep_SIDD 51 best_it_SIDD 119 Best_PSNR_SIDD 15.4592] 
[Ep 52 it 119	 PSNR SIDD: 14.2951	] ----  [best_Ep_SIDD 51 best_it_SIDD 119 Best_PSNR_SIDD 15.4592] 
Epoch: 52	Time: 12.9315	Loss: 42.0429	LearningRate 0.000015
[Ep 53 it 29	 PSNR SIDD: 14.7585	] ----  [best_Ep_SIDD 51 best_it_SIDD 119 Best_PSNR_SIDD 15.4592] 
[Ep 53 it 59	 PSNR SIDD: 15.0090	] ----  [best_Ep_SIDD 51 best_it_SIDD 119 Best_PSNR_SIDD 15.4592] 
[Ep 53 it 89	 PSNR SIDD: 15.2489	] ----  [best_Ep_SIDD 51 best_it_SIDD 119 Best_PSNR_SIDD 15.4592] 
[Ep 53 it 119	 PSNR SIDD: 14.4367	] ----  [best_Ep_SIDD 51 best_it_SIDD 119 Best_PSNR_SIDD 15.4592] 
Epoch: 53	Time: 13.7243	Loss: 42.2197	LearningRate 0.000015
[Ep 54 it 29	 PSNR SIDD: 15.3275	] ----  [best_Ep_SIDD 51 best_it_SIDD 119 Best_PSNR_SIDD 15.4592] 
[Ep 54 it 59	 PSNR SIDD: 14.8751	] ----  [best_Ep_SIDD 51 best_it_SIDD 119 Best_PSNR_SIDD 15.4592] 
