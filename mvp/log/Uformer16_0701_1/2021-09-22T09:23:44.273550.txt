Namespace(arch='Uformer', att_se=False, batch_size=32, checkpoint=50, dataset='SIDD', embed_dim=16, env='16_0701_1', eval_workers=8, global_skip=False, gpu='0', local_skip=False, lr_initial=0.0002, mode='denoising', nepoch=250, norm_layer='nn.LayerNorm', optimizer='adamw', pretrain_weights='./log/Uformer32/models/model_best.pth', resume=False, save_dir='/home/ma-user/work/deNoTr/log', save_images=False, token_mlp='leff', token_projection='linear', train_dir='../datasets/denoising/sidd/train', train_ps=128, train_workers=16, val_dir='../datasets/denoising/sidd/train', vit_depth=12, vit_dim=256, vit_mlp_dim=512, vit_nheads=8, vit_patch_size=16, vit_share=False, warmup=True, warmup_epochs=3, weight_decay=0.02, win_size=8)
Uformer(
  embed_dim=16, token_projection=linear, token_mlp=leff,win_size=8
  (pos_drop): Dropout(p=0.0, inplace=False)
  (input_proj): InputProj(
    (proj): Sequential(
      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): LeakyReLU(negative_slope=0.01, inplace=True)
    )
  )
  (output_proj): OutputProj(
    (proj): Sequential(
      (0): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (encoderlayer_0): BasicUformerLayer(
    dim=16, input_resolution=(128, 128), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=16, input_resolution=(128, 128), num_heads=1, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=16, win_size=(8, 8), num_heads=1
          (qkv): LinearProjection(
            (to_q): Linear(in_features=16, out_features=16, bias=True)
            (to_kv): Linear(in_features=16, out_features=32, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=16, out_features=16, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=64, out_features=16, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=16, input_resolution=(128, 128), num_heads=1, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=16, win_size=(8, 8), num_heads=1
          (qkv): LinearProjection(
            (to_q): Linear(in_features=16, out_features=16, bias=True)
            (to_kv): Linear(in_features=16, out_features=32, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=16, out_features=16, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=64, out_features=16, bias=True)
          )
        )
      )
    )
  )
  (dowsample_0): Downsample(
    (conv): Sequential(
      (0): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (encoderlayer_1): BasicUformerLayer(
    dim=32, input_resolution=(64, 64), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=32, input_resolution=(64, 64), num_heads=2, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=32, win_size=(8, 8), num_heads=2
          (qkv): LinearProjection(
            (to_q): Linear(in_features=32, out_features=32, bias=True)
            (to_kv): Linear(in_features=32, out_features=64, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=32, out_features=32, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=32, out_features=128, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=128, out_features=32, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=32, input_resolution=(64, 64), num_heads=2, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=32, win_size=(8, 8), num_heads=2
          (qkv): LinearProjection(
            (to_q): Linear(in_features=32, out_features=32, bias=True)
            (to_kv): Linear(in_features=32, out_features=64, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=32, out_features=32, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=32, out_features=128, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=128, out_features=32, bias=True)
          )
        )
      )
    )
  )
  (dowsample_1): Downsample(
    (conv): Sequential(
      (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (encoderlayer_2): BasicUformerLayer(
    dim=64, input_resolution=(32, 32), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=64, input_resolution=(32, 32), num_heads=4, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=64, win_size=(8, 8), num_heads=4
          (qkv): LinearProjection(
            (to_q): Linear(in_features=64, out_features=64, bias=True)
            (to_kv): Linear(in_features=64, out_features=128, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=64, out_features=256, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=256, out_features=64, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=64, input_resolution=(32, 32), num_heads=4, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=64, win_size=(8, 8), num_heads=4
          (qkv): LinearProjection(
            (to_q): Linear(in_features=64, out_features=64, bias=True)
            (to_kv): Linear(in_features=64, out_features=128, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=64, out_features=256, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=256, out_features=64, bias=True)
          )
        )
      )
    )
  )
  (dowsample_2): Downsample(
    (conv): Sequential(
      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (encoderlayer_3): BasicUformerLayer(
    dim=128, input_resolution=(16, 16), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=128, input_resolution=(16, 16), num_heads=8, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=128, win_size=(8, 8), num_heads=8
          (qkv): LinearProjection(
            (to_q): Linear(in_features=128, out_features=128, bias=True)
            (to_kv): Linear(in_features=128, out_features=256, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=128, out_features=512, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=128, input_resolution=(16, 16), num_heads=8, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=128, win_size=(8, 8), num_heads=8
          (qkv): LinearProjection(
            (to_q): Linear(in_features=128, out_features=128, bias=True)
            (to_kv): Linear(in_features=128, out_features=256, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=128, out_features=512, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
          )
        )
      )
    )
  )
  (dowsample_3): Downsample(
    (conv): Sequential(
      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (conv): BasicUformerLayer(
    dim=256, input_resolution=(8, 8), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=256, input_resolution=(8, 8), num_heads=16, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=256, win_size=(8, 8), num_heads=16
          (qkv): LinearProjection(
            (to_q): Linear(in_features=256, out_features=256, bias=True)
            (to_kv): Linear(in_features=256, out_features=512, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=1024, out_features=256, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=256, input_resolution=(8, 8), num_heads=16, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=256, win_size=(8, 8), num_heads=16
          (qkv): LinearProjection(
            (to_q): Linear(in_features=256, out_features=256, bias=True)
            (to_kv): Linear(in_features=256, out_features=512, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=1024, out_features=256, bias=True)
          )
        )
      )
    )
  )
  (upsample_0): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (decoderlayer_0): BasicUformerLayer(
    dim=256, input_resolution=(16, 16), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=256, input_resolution=(16, 16), num_heads=16, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=256, win_size=(8, 8), num_heads=16
          (qkv): LinearProjection(
            (to_q): Linear(in_features=256, out_features=256, bias=True)
            (to_kv): Linear(in_features=256, out_features=512, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=1024, out_features=256, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=256, input_resolution=(16, 16), num_heads=16, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=256, win_size=(8, 8), num_heads=16
          (qkv): LinearProjection(
            (to_q): Linear(in_features=256, out_features=256, bias=True)
            (to_kv): Linear(in_features=256, out_features=512, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=1024, out_features=256, bias=True)
          )
        )
      )
    )
  )
  (upsample_1): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (decoderlayer_1): BasicUformerLayer(
    dim=128, input_resolution=(32, 32), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=128, input_resolution=(32, 32), num_heads=8, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=128, win_size=(8, 8), num_heads=8
          (qkv): LinearProjection(
            (to_q): Linear(in_features=128, out_features=128, bias=True)
            (to_kv): Linear(in_features=128, out_features=256, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=128, out_features=512, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=128, input_resolution=(32, 32), num_heads=8, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=128, win_size=(8, 8), num_heads=8
          (qkv): LinearProjection(
            (to_q): Linear(in_features=128, out_features=128, bias=True)
            (to_kv): Linear(in_features=128, out_features=256, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=128, out_features=512, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
          )
        )
      )
    )
  )
  (upsample_2): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(128, 32, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (decoderlayer_2): BasicUformerLayer(
    dim=64, input_resolution=(64, 64), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=64, input_resolution=(64, 64), num_heads=4, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=64, win_size=(8, 8), num_heads=4
          (qkv): LinearProjection(
            (to_q): Linear(in_features=64, out_features=64, bias=True)
            (to_kv): Linear(in_features=64, out_features=128, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=64, out_features=256, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=256, out_features=64, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=64, input_resolution=(64, 64), num_heads=4, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=64, win_size=(8, 8), num_heads=4
          (qkv): LinearProjection(
            (to_q): Linear(in_features=64, out_features=64, bias=True)
            (to_kv): Linear(in_features=64, out_features=128, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=64, out_features=256, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=256, out_features=64, bias=True)
          )
        )
      )
    )
  )
  (upsample_3): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(64, 16, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (decoderlayer_3): BasicUformerLayer(
    dim=32, input_resolution=(128, 128), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=32, input_resolution=(128, 128), num_heads=2, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=32, win_size=(8, 8), num_heads=2
          (qkv): LinearProjection(
            (to_q): Linear(in_features=32, out_features=32, bias=True)
            (to_kv): Linear(in_features=32, out_features=64, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=32, out_features=32, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=32, out_features=128, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=128, out_features=32, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=32, input_resolution=(128, 128), num_heads=2, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=32, win_size=(8, 8), num_heads=2
          (qkv): LinearProjection(
            (to_q): Linear(in_features=32, out_features=32, bias=True)
            (to_kv): Linear(in_features=32, out_features=64, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=32, out_features=32, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=32, out_features=128, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=128, out_features=32, bias=True)
          )
        )
      )
    )
  )
)
[Ep 1 it 9	 PSNR SIDD: 22.3116	] ----  [best_Ep_SIDD 1 best_it_SIDD 9 Best_PSNR_SIDD 22.3116] 
[Ep 1 it 19	 PSNR SIDD: 23.2363	] ----  [best_Ep_SIDD 1 best_it_SIDD 19 Best_PSNR_SIDD 23.2363] 
[Ep 1 it 29	 PSNR SIDD: 23.6185	] ----  [best_Ep_SIDD 1 best_it_SIDD 29 Best_PSNR_SIDD 23.6185] 
[Ep 1 it 39	 PSNR SIDD: 23.7519	] ----  [best_Ep_SIDD 1 best_it_SIDD 39 Best_PSNR_SIDD 23.7519] 
Epoch: 1	Time: 107.9927	Loss: 2.5177	LearningRate 0.000133
[Ep 2 it 9	 PSNR SIDD: 23.8799	] ----  [best_Ep_SIDD 2 best_it_SIDD 9 Best_PSNR_SIDD 23.8799] 
[Ep 2 it 19	 PSNR SIDD: 24.0377	] ----  [best_Ep_SIDD 2 best_it_SIDD 19 Best_PSNR_SIDD 24.0377] 
[Ep 2 it 29	 PSNR SIDD: 24.2049	] ----  [best_Ep_SIDD 2 best_it_SIDD 29 Best_PSNR_SIDD 24.2049] 
[Ep 2 it 39	 PSNR SIDD: 24.3452	] ----  [best_Ep_SIDD 2 best_it_SIDD 39 Best_PSNR_SIDD 24.3452] 
Epoch: 2	Time: 105.1764	Loss: 2.2409	LearningRate 0.000200
[Ep 3 it 9	 PSNR SIDD: 24.6277	] ----  [best_Ep_SIDD 3 best_it_SIDD 9 Best_PSNR_SIDD 24.6277] 
[Ep 3 it 19	 PSNR SIDD: 24.8870	] ----  [best_Ep_SIDD 3 best_it_SIDD 19 Best_PSNR_SIDD 24.8870] 
[Ep 3 it 29	 PSNR SIDD: 25.1184	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 25.1184] 
[Ep 3 it 39	 PSNR SIDD: 25.8421	] ----  [best_Ep_SIDD 3 best_it_SIDD 39 Best_PSNR_SIDD 25.8421] 
Epoch: 3	Time: 106.4042	Loss: 2.0261	LearningRate 0.000200
[Ep 4 it 9	 PSNR SIDD: 26.5147	] ----  [best_Ep_SIDD 4 best_it_SIDD 9 Best_PSNR_SIDD 26.5147] 
[Ep 4 it 19	 PSNR SIDD: 26.8755	] ----  [best_Ep_SIDD 4 best_it_SIDD 19 Best_PSNR_SIDD 26.8755] 
[Ep 4 it 29	 PSNR SIDD: 27.7658	] ----  [best_Ep_SIDD 4 best_it_SIDD 29 Best_PSNR_SIDD 27.7658] 
[Ep 4 it 39	 PSNR SIDD: 28.3950	] ----  [best_Ep_SIDD 4 best_it_SIDD 39 Best_PSNR_SIDD 28.3950] 
Epoch: 4	Time: 105.5051	Loss: 1.5927	LearningRate 0.000200
[Ep 5 it 9	 PSNR SIDD: 28.8261	] ----  [best_Ep_SIDD 5 best_it_SIDD 9 Best_PSNR_SIDD 28.8261] 
[Ep 5 it 19	 PSNR SIDD: 29.1458	] ----  [best_Ep_SIDD 5 best_it_SIDD 19 Best_PSNR_SIDD 29.1458] 
[Ep 5 it 29	 PSNR SIDD: 29.8909	] ----  [best_Ep_SIDD 5 best_it_SIDD 29 Best_PSNR_SIDD 29.8909] 
[Ep 5 it 39	 PSNR SIDD: 30.2112	] ----  [best_Ep_SIDD 5 best_it_SIDD 39 Best_PSNR_SIDD 30.2112] 
Epoch: 5	Time: 106.5164	Loss: 1.2276	LearningRate 0.000200
[Ep 6 it 9	 PSNR SIDD: 30.6190	] ----  [best_Ep_SIDD 6 best_it_SIDD 9 Best_PSNR_SIDD 30.6190] 
[Ep 6 it 19	 PSNR SIDD: 30.8482	] ----  [best_Ep_SIDD 6 best_it_SIDD 19 Best_PSNR_SIDD 30.8482] 
[Ep 6 it 29	 PSNR SIDD: 30.6111	] ----  [best_Ep_SIDD 6 best_it_SIDD 19 Best_PSNR_SIDD 30.8482] 
[Ep 6 it 39	 PSNR SIDD: 31.2136	] ----  [best_Ep_SIDD 6 best_it_SIDD 39 Best_PSNR_SIDD 31.2136] 
Epoch: 6	Time: 105.2814	Loss: 0.8702	LearningRate 0.000200
[Ep 7 it 9	 PSNR SIDD: 31.3357	] ----  [best_Ep_SIDD 7 best_it_SIDD 9 Best_PSNR_SIDD 31.3357] 
[Ep 7 it 19	 PSNR SIDD: 31.0630	] ----  [best_Ep_SIDD 7 best_it_SIDD 9 Best_PSNR_SIDD 31.3357] 
[Ep 7 it 29	 PSNR SIDD: 31.5197	] ----  [best_Ep_SIDD 7 best_it_SIDD 29 Best_PSNR_SIDD 31.5197] 
[Ep 7 it 39	 PSNR SIDD: 31.8699	] ----  [best_Ep_SIDD 7 best_it_SIDD 39 Best_PSNR_SIDD 31.8699] 
Epoch: 7	Time: 106.4304	Loss: 0.7812	LearningRate 0.000200
[Ep 8 it 9	 PSNR SIDD: 32.0499	] ----  [best_Ep_SIDD 8 best_it_SIDD 9 Best_PSNR_SIDD 32.0499] 
[Ep 8 it 19	 PSNR SIDD: 31.8678	] ----  [best_Ep_SIDD 8 best_it_SIDD 9 Best_PSNR_SIDD 32.0499] 
[Ep 8 it 29	 PSNR SIDD: 32.2170	] ----  [best_Ep_SIDD 8 best_it_SIDD 29 Best_PSNR_SIDD 32.2170] 
[Ep 8 it 39	 PSNR SIDD: 32.4167	] ----  [best_Ep_SIDD 8 best_it_SIDD 39 Best_PSNR_SIDD 32.4167] 
Epoch: 8	Time: 105.9185	Loss: 0.7154	LearningRate 0.000200
[Ep 9 it 9	 PSNR SIDD: 32.2050	] ----  [best_Ep_SIDD 8 best_it_SIDD 39 Best_PSNR_SIDD 32.4167] 
[Ep 9 it 19	 PSNR SIDD: 32.5487	] ----  [best_Ep_SIDD 9 best_it_SIDD 19 Best_PSNR_SIDD 32.5487] 
[Ep 9 it 29	 PSNR SIDD: 32.8268	] ----  [best_Ep_SIDD 9 best_it_SIDD 29 Best_PSNR_SIDD 32.8268] 
[Ep 9 it 39	 PSNR SIDD: 32.7834	] ----  [best_Ep_SIDD 9 best_it_SIDD 29 Best_PSNR_SIDD 32.8268] 
Epoch: 9	Time: 105.8040	Loss: 0.6743	LearningRate 0.000200
[Ep 10 it 9	 PSNR SIDD: 32.8410	] ----  [best_Ep_SIDD 10 best_it_SIDD 9 Best_PSNR_SIDD 32.8410] 
[Ep 10 it 19	 PSNR SIDD: 32.7567	] ----  [best_Ep_SIDD 10 best_it_SIDD 9 Best_PSNR_SIDD 32.8410] 
[Ep 10 it 29	 PSNR SIDD: 33.2804	] ----  [best_Ep_SIDD 10 best_it_SIDD 29 Best_PSNR_SIDD 33.2804] 
[Ep 10 it 39	 PSNR SIDD: 33.2842	] ----  [best_Ep_SIDD 10 best_it_SIDD 39 Best_PSNR_SIDD 33.2842] 
Epoch: 10	Time: 106.3630	Loss: 0.6448	LearningRate 0.000200
[Ep 11 it 9	 PSNR SIDD: 33.3126	] ----  [best_Ep_SIDD 11 best_it_SIDD 9 Best_PSNR_SIDD 33.3126] 
[Ep 11 it 19	 PSNR SIDD: 33.1638	] ----  [best_Ep_SIDD 11 best_it_SIDD 9 Best_PSNR_SIDD 33.3126] 
[Ep 11 it 29	 PSNR SIDD: 33.5470	] ----  [best_Ep_SIDD 11 best_it_SIDD 29 Best_PSNR_SIDD 33.5470] 
[Ep 11 it 39	 PSNR SIDD: 32.9022	] ----  [best_Ep_SIDD 11 best_it_SIDD 29 Best_PSNR_SIDD 33.5470] 
Epoch: 11	Time: 105.9766	Loss: 0.6322	LearningRate 0.000199
[Ep 12 it 9	 PSNR SIDD: 33.4751	] ----  [best_Ep_SIDD 11 best_it_SIDD 29 Best_PSNR_SIDD 33.5470] 
[Ep 12 it 19	 PSNR SIDD: 33.7397	] ----  [best_Ep_SIDD 12 best_it_SIDD 19 Best_PSNR_SIDD 33.7397] 
[Ep 12 it 29	 PSNR SIDD: 33.7903	] ----  [best_Ep_SIDD 12 best_it_SIDD 29 Best_PSNR_SIDD 33.7903] 
[Ep 12 it 39	 PSNR SIDD: 33.7725	] ----  [best_Ep_SIDD 12 best_it_SIDD 29 Best_PSNR_SIDD 33.7903] 
Epoch: 12	Time: 105.5572	Loss: 0.6162	LearningRate 0.000199
[Ep 13 it 9	 PSNR SIDD: 33.8505	] ----  [best_Ep_SIDD 13 best_it_SIDD 9 Best_PSNR_SIDD 33.8505] 
[Ep 13 it 19	 PSNR SIDD: 33.6572	] ----  [best_Ep_SIDD 13 best_it_SIDD 9 Best_PSNR_SIDD 33.8505] 
[Ep 13 it 29	 PSNR SIDD: 33.8750	] ----  [best_Ep_SIDD 13 best_it_SIDD 29 Best_PSNR_SIDD 33.8750] 
[Ep 13 it 39	 PSNR SIDD: 34.1009	] ----  [best_Ep_SIDD 13 best_it_SIDD 39 Best_PSNR_SIDD 34.1009] 
Epoch: 13	Time: 106.4608	Loss: 0.5978	LearningRate 0.000199
[Ep 14 it 9	 PSNR SIDD: 33.9399	] ----  [best_Ep_SIDD 13 best_it_SIDD 39 Best_PSNR_SIDD 34.1009] 
[Ep 14 it 19	 PSNR SIDD: 33.8345	] ----  [best_Ep_SIDD 13 best_it_SIDD 39 Best_PSNR_SIDD 34.1009] 
[Ep 14 it 29	 PSNR SIDD: 34.1341	] ----  [best_Ep_SIDD 14 best_it_SIDD 29 Best_PSNR_SIDD 34.1341] 
[Ep 14 it 39	 PSNR SIDD: 34.3140	] ----  [best_Ep_SIDD 14 best_it_SIDD 39 Best_PSNR_SIDD 34.3140] 
Epoch: 14	Time: 106.4119	Loss: 0.5782	LearningRate 0.000199
[Ep 15 it 9	 PSNR SIDD: 34.3811	] ----  [best_Ep_SIDD 15 best_it_SIDD 9 Best_PSNR_SIDD 34.3811] 
[Ep 15 it 19	 PSNR SIDD: 34.4578	] ----  [best_Ep_SIDD 15 best_it_SIDD 19 Best_PSNR_SIDD 34.4578] 
[Ep 15 it 29	 PSNR SIDD: 34.5006	] ----  [best_Ep_SIDD 15 best_it_SIDD 29 Best_PSNR_SIDD 34.5006] 
[Ep 15 it 39	 PSNR SIDD: 34.5395	] ----  [best_Ep_SIDD 15 best_it_SIDD 39 Best_PSNR_SIDD 34.5395] 
Epoch: 15	Time: 106.6403	Loss: 0.5606	LearningRate 0.000199
[Ep 16 it 9	 PSNR SIDD: 34.5750	] ----  [best_Ep_SIDD 16 best_it_SIDD 9 Best_PSNR_SIDD 34.5750] 
[Ep 16 it 19	 PSNR SIDD: 34.3387	] ----  [best_Ep_SIDD 16 best_it_SIDD 9 Best_PSNR_SIDD 34.5750] 
[Ep 16 it 29	 PSNR SIDD: 34.5771	] ----  [best_Ep_SIDD 16 best_it_SIDD 29 Best_PSNR_SIDD 34.5771] 
[Ep 16 it 39	 PSNR SIDD: 34.6542	] ----  [best_Ep_SIDD 16 best_it_SIDD 39 Best_PSNR_SIDD 34.6542] 
Epoch: 16	Time: 106.3255	Loss: 0.5540	LearningRate 0.000198
[Ep 17 it 9	 PSNR SIDD: 34.7640	] ----  [best_Ep_SIDD 17 best_it_SIDD 9 Best_PSNR_SIDD 34.7640] 
[Ep 17 it 19	 PSNR SIDD: 34.6539	] ----  [best_Ep_SIDD 17 best_it_SIDD 9 Best_PSNR_SIDD 34.7640] 
[Ep 17 it 29	 PSNR SIDD: 34.4864	] ----  [best_Ep_SIDD 17 best_it_SIDD 9 Best_PSNR_SIDD 34.7640] 
[Ep 17 it 39	 PSNR SIDD: 34.8025	] ----  [best_Ep_SIDD 17 best_it_SIDD 39 Best_PSNR_SIDD 34.8025] 
Epoch: 17	Time: 106.5335	Loss: 0.5457	LearningRate 0.000198
[Ep 18 it 9	 PSNR SIDD: 34.7537	] ----  [best_Ep_SIDD 17 best_it_SIDD 39 Best_PSNR_SIDD 34.8025] 
[Ep 18 it 19	 PSNR SIDD: 34.7805	] ----  [best_Ep_SIDD 17 best_it_SIDD 39 Best_PSNR_SIDD 34.8025] 
[Ep 18 it 29	 PSNR SIDD: 34.9178	] ----  [best_Ep_SIDD 18 best_it_SIDD 29 Best_PSNR_SIDD 34.9178] 
[Ep 18 it 39	 PSNR SIDD: 34.9067	] ----  [best_Ep_SIDD 18 best_it_SIDD 29 Best_PSNR_SIDD 34.9178] 
Epoch: 18	Time: 106.1824	Loss: 0.5387	LearningRate 0.000198
[Ep 19 it 9	 PSNR SIDD: 34.9531	] ----  [best_Ep_SIDD 19 best_it_SIDD 9 Best_PSNR_SIDD 34.9531] 
[Ep 19 it 19	 PSNR SIDD: 34.8181	] ----  [best_Ep_SIDD 19 best_it_SIDD 9 Best_PSNR_SIDD 34.9531] 
[Ep 19 it 29	 PSNR SIDD: 34.8775	] ----  [best_Ep_SIDD 19 best_it_SIDD 9 Best_PSNR_SIDD 34.9531] 
[Ep 19 it 39	 PSNR SIDD: 35.1157	] ----  [best_Ep_SIDD 19 best_it_SIDD 39 Best_PSNR_SIDD 35.1157] 
Epoch: 19	Time: 106.5570	Loss: 0.5313	LearningRate 0.000198
[Ep 20 it 9	 PSNR SIDD: 34.6487	] ----  [best_Ep_SIDD 19 best_it_SIDD 39 Best_PSNR_SIDD 35.1157] 
[Ep 20 it 19	 PSNR SIDD: 35.0321	] ----  [best_Ep_SIDD 19 best_it_SIDD 39 Best_PSNR_SIDD 35.1157] 
[Ep 20 it 29	 PSNR SIDD: 34.7857	] ----  [best_Ep_SIDD 19 best_it_SIDD 39 Best_PSNR_SIDD 35.1157] 
[Ep 20 it 39	 PSNR SIDD: 34.8360	] ----  [best_Ep_SIDD 19 best_it_SIDD 39 Best_PSNR_SIDD 35.1157] 
Epoch: 20	Time: 106.2434	Loss: 0.5143	LearningRate 0.000197
[Ep 21 it 9	 PSNR SIDD: 35.2006	] ----  [best_Ep_SIDD 21 best_it_SIDD 9 Best_PSNR_SIDD 35.2006] 
[Ep 21 it 19	 PSNR SIDD: 34.6162	] ----  [best_Ep_SIDD 21 best_it_SIDD 9 Best_PSNR_SIDD 35.2006] 
[Ep 21 it 29	 PSNR SIDD: 35.2736	] ----  [best_Ep_SIDD 21 best_it_SIDD 29 Best_PSNR_SIDD 35.2736] 
[Ep 21 it 39	 PSNR SIDD: 34.8873	] ----  [best_Ep_SIDD 21 best_it_SIDD 29 Best_PSNR_SIDD 35.2736] 
Epoch: 21	Time: 106.6084	Loss: 0.5225	LearningRate 0.000197
[Ep 22 it 9	 PSNR SIDD: 34.9938	] ----  [best_Ep_SIDD 21 best_it_SIDD 29 Best_PSNR_SIDD 35.2736] 
[Ep 22 it 19	 PSNR SIDD: 35.1592	] ----  [best_Ep_SIDD 21 best_it_SIDD 29 Best_PSNR_SIDD 35.2736] 
[Ep 22 it 29	 PSNR SIDD: 35.2796	] ----  [best_Ep_SIDD 22 best_it_SIDD 29 Best_PSNR_SIDD 35.2796] 
[Ep 22 it 39	 PSNR SIDD: 35.2014	] ----  [best_Ep_SIDD 22 best_it_SIDD 29 Best_PSNR_SIDD 35.2796] 
Epoch: 22	Time: 107.0360	Loss: 0.5106	LearningRate 0.000197
