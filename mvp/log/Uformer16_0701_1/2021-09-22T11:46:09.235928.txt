Namespace(arch='Uformer', att_se=False, batch_size=32, checkpoint=50, dataset='SIDD', embed_dim=16, env='16_0701_1', eval_workers=8, global_skip=False, gpu='0', local_skip=False, lr_initial=0.0002, mode='denoising', nepoch=250, norm_layer='nn.LayerNorm', optimizer='adamw', pretrain_weights='./log/Uformer32/models/model_best.pth', resume=False, save_dir='/home/ma-user/work/deNoTr/log', save_images=False, token_mlp='leff', token_projection='linear', train_dir='../datasets/delowlight/sidd/train', train_ps=128, train_workers=16, val_dir='../datasets/delowlight/lol/valid', vit_depth=12, vit_dim=256, vit_mlp_dim=512, vit_nheads=8, vit_patch_size=16, vit_share=False, warmup=True, warmup_epochs=3, weight_decay=0.02, win_size=8)
Uformer(
  embed_dim=16, token_projection=linear, token_mlp=leff,win_size=8
  (pos_drop): Dropout(p=0.0, inplace=False)
  (input_proj): InputProj(
    (proj): Sequential(
      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): LeakyReLU(negative_slope=0.01, inplace=True)
    )
  )
  (output_proj): OutputProj(
    (proj): Sequential(
      (0): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (encoderlayer_0): BasicUformerLayer(
    dim=16, input_resolution=(128, 128), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=16, input_resolution=(128, 128), num_heads=1, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=16, win_size=(8, 8), num_heads=1
          (qkv): LinearProjection(
            (to_q): Linear(in_features=16, out_features=16, bias=True)
            (to_kv): Linear(in_features=16, out_features=32, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=16, out_features=16, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=64, out_features=16, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=16, input_resolution=(128, 128), num_heads=1, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=16, win_size=(8, 8), num_heads=1
          (qkv): LinearProjection(
            (to_q): Linear(in_features=16, out_features=16, bias=True)
            (to_kv): Linear(in_features=16, out_features=32, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=16, out_features=16, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=64, out_features=16, bias=True)
          )
        )
      )
    )
  )
  (dowsample_0): Downsample(
    (conv): Sequential(
      (0): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (encoderlayer_1): BasicUformerLayer(
    dim=32, input_resolution=(64, 64), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=32, input_resolution=(64, 64), num_heads=2, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=32, win_size=(8, 8), num_heads=2
          (qkv): LinearProjection(
            (to_q): Linear(in_features=32, out_features=32, bias=True)
            (to_kv): Linear(in_features=32, out_features=64, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=32, out_features=32, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=32, out_features=128, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=128, out_features=32, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=32, input_resolution=(64, 64), num_heads=2, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=32, win_size=(8, 8), num_heads=2
          (qkv): LinearProjection(
            (to_q): Linear(in_features=32, out_features=32, bias=True)
            (to_kv): Linear(in_features=32, out_features=64, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=32, out_features=32, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=32, out_features=128, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=128, out_features=32, bias=True)
          )
        )
      )
    )
  )
  (dowsample_1): Downsample(
    (conv): Sequential(
      (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (encoderlayer_2): BasicUformerLayer(
    dim=64, input_resolution=(32, 32), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=64, input_resolution=(32, 32), num_heads=4, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=64, win_size=(8, 8), num_heads=4
          (qkv): LinearProjection(
            (to_q): Linear(in_features=64, out_features=64, bias=True)
            (to_kv): Linear(in_features=64, out_features=128, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=64, out_features=256, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=256, out_features=64, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=64, input_resolution=(32, 32), num_heads=4, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=64, win_size=(8, 8), num_heads=4
          (qkv): LinearProjection(
            (to_q): Linear(in_features=64, out_features=64, bias=True)
            (to_kv): Linear(in_features=64, out_features=128, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=64, out_features=256, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=256, out_features=64, bias=True)
          )
        )
      )
    )
  )
  (dowsample_2): Downsample(
    (conv): Sequential(
      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (encoderlayer_3): BasicUformerLayer(
    dim=128, input_resolution=(16, 16), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=128, input_resolution=(16, 16), num_heads=8, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=128, win_size=(8, 8), num_heads=8
          (qkv): LinearProjection(
            (to_q): Linear(in_features=128, out_features=128, bias=True)
            (to_kv): Linear(in_features=128, out_features=256, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=128, out_features=512, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=128, input_resolution=(16, 16), num_heads=8, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=128, win_size=(8, 8), num_heads=8
          (qkv): LinearProjection(
            (to_q): Linear(in_features=128, out_features=128, bias=True)
            (to_kv): Linear(in_features=128, out_features=256, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=128, out_features=512, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
          )
        )
      )
    )
  )
  (dowsample_3): Downsample(
    (conv): Sequential(
      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (conv): BasicUformerLayer(
    dim=256, input_resolution=(8, 8), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=256, input_resolution=(8, 8), num_heads=16, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=256, win_size=(8, 8), num_heads=16
          (qkv): LinearProjection(
            (to_q): Linear(in_features=256, out_features=256, bias=True)
            (to_kv): Linear(in_features=256, out_features=512, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=1024, out_features=256, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=256, input_resolution=(8, 8), num_heads=16, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=256, win_size=(8, 8), num_heads=16
          (qkv): LinearProjection(
            (to_q): Linear(in_features=256, out_features=256, bias=True)
            (to_kv): Linear(in_features=256, out_features=512, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=1024, out_features=256, bias=True)
          )
        )
      )
    )
  )
  (upsample_0): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (decoderlayer_0): BasicUformerLayer(
    dim=256, input_resolution=(16, 16), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=256, input_resolution=(16, 16), num_heads=16, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=256, win_size=(8, 8), num_heads=16
          (qkv): LinearProjection(
            (to_q): Linear(in_features=256, out_features=256, bias=True)
            (to_kv): Linear(in_features=256, out_features=512, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=1024, out_features=256, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=256, input_resolution=(16, 16), num_heads=16, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=256, win_size=(8, 8), num_heads=16
          (qkv): LinearProjection(
            (to_q): Linear(in_features=256, out_features=256, bias=True)
            (to_kv): Linear(in_features=256, out_features=512, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=1024, out_features=256, bias=True)
          )
        )
      )
    )
  )
  (upsample_1): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (decoderlayer_1): BasicUformerLayer(
    dim=128, input_resolution=(32, 32), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=128, input_resolution=(32, 32), num_heads=8, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=128, win_size=(8, 8), num_heads=8
          (qkv): LinearProjection(
            (to_q): Linear(in_features=128, out_features=128, bias=True)
            (to_kv): Linear(in_features=128, out_features=256, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=128, out_features=512, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=128, input_resolution=(32, 32), num_heads=8, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=128, win_size=(8, 8), num_heads=8
          (qkv): LinearProjection(
            (to_q): Linear(in_features=128, out_features=128, bias=True)
            (to_kv): Linear(in_features=128, out_features=256, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=128, out_features=512, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
          )
        )
      )
    )
  )
  (upsample_2): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(128, 32, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (decoderlayer_2): BasicUformerLayer(
    dim=64, input_resolution=(64, 64), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=64, input_resolution=(64, 64), num_heads=4, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=64, win_size=(8, 8), num_heads=4
          (qkv): LinearProjection(
            (to_q): Linear(in_features=64, out_features=64, bias=True)
            (to_kv): Linear(in_features=64, out_features=128, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=64, out_features=256, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=256, out_features=64, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=64, input_resolution=(64, 64), num_heads=4, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=64, win_size=(8, 8), num_heads=4
          (qkv): LinearProjection(
            (to_q): Linear(in_features=64, out_features=64, bias=True)
            (to_kv): Linear(in_features=64, out_features=128, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=64, out_features=256, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=256, out_features=64, bias=True)
          )
        )
      )
    )
  )
  (upsample_3): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(64, 16, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (decoderlayer_3): BasicUformerLayer(
    dim=32, input_resolution=(128, 128), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=32, input_resolution=(128, 128), num_heads=2, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=32, win_size=(8, 8), num_heads=2
          (qkv): LinearProjection(
            (to_q): Linear(in_features=32, out_features=32, bias=True)
            (to_kv): Linear(in_features=32, out_features=64, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=32, out_features=32, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=32, out_features=128, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=128, out_features=32, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=32, input_resolution=(128, 128), num_heads=2, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=32, win_size=(8, 8), num_heads=2
          (qkv): LinearProjection(
            (to_q): Linear(in_features=32, out_features=32, bias=True)
            (to_kv): Linear(in_features=32, out_features=64, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=32, out_features=32, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=32, out_features=128, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=128, out_features=32, bias=True)
          )
        )
      )
    )
  )
)
[Ep 1 it 9	 PSNR SIDD: 11.3147	] ----  [best_Ep_SIDD 1 best_it_SIDD 9 Best_PSNR_SIDD 11.3147] 
[Ep 1 it 19	 PSNR SIDD: 12.6026	] ----  [best_Ep_SIDD 1 best_it_SIDD 19 Best_PSNR_SIDD 12.6026] 
[Ep 1 it 29	 PSNR SIDD: 12.4861	] ----  [best_Ep_SIDD 1 best_it_SIDD 19 Best_PSNR_SIDD 12.6026] 
[Ep 1 it 39	 PSNR SIDD: 12.3224	] ----  [best_Ep_SIDD 1 best_it_SIDD 19 Best_PSNR_SIDD 12.6026] 
Epoch: 1	Time: 29.7083	Loss: 3.9038	LearningRate 0.000133
[Ep 2 it 9	 PSNR SIDD: 12.1261	] ----  [best_Ep_SIDD 1 best_it_SIDD 19 Best_PSNR_SIDD 12.6026] 
[Ep 2 it 19	 PSNR SIDD: 12.0310	] ----  [best_Ep_SIDD 1 best_it_SIDD 19 Best_PSNR_SIDD 12.6026] 
[Ep 2 it 29	 PSNR SIDD: 12.3009	] ----  [best_Ep_SIDD 1 best_it_SIDD 19 Best_PSNR_SIDD 12.6026] 
[Ep 2 it 39	 PSNR SIDD: 13.1048	] ----  [best_Ep_SIDD 2 best_it_SIDD 39 Best_PSNR_SIDD 13.1048] 
Epoch: 2	Time: 26.7191	Loss: 3.0630	LearningRate 0.000200
[Ep 3 it 9	 PSNR SIDD: 12.7779	] ----  [best_Ep_SIDD 2 best_it_SIDD 39 Best_PSNR_SIDD 13.1048] 
[Ep 3 it 19	 PSNR SIDD: 13.2412	] ----  [best_Ep_SIDD 3 best_it_SIDD 19 Best_PSNR_SIDD 13.2412] 
[Ep 3 it 29	 PSNR SIDD: 13.5555	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 3 it 39	 PSNR SIDD: 13.1609	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 3	Time: 27.4496	Loss: 2.6944	LearningRate 0.000200
[Ep 4 it 9	 PSNR SIDD: 13.0602	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 4 it 19	 PSNR SIDD: 12.7520	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 4 it 29	 PSNR SIDD: 13.3520	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 4 it 39	 PSNR SIDD: 13.1311	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 4	Time: 26.1681	Loss: 2.5181	LearningRate 0.000200
[Ep 5 it 9	 PSNR SIDD: 12.9240	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 5 it 19	 PSNR SIDD: 12.0420	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 5 it 29	 PSNR SIDD: 13.5471	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 5 it 39	 PSNR SIDD: 13.2337	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 5	Time: 26.6704	Loss: 2.5403	LearningRate 0.000200
[Ep 6 it 9	 PSNR SIDD: 12.7752	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 6 it 19	 PSNR SIDD: 12.2126	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 6 it 29	 PSNR SIDD: 12.7529	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 6 it 39	 PSNR SIDD: 13.0539	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 6	Time: 26.8183	Loss: 2.2017	LearningRate 0.000200
[Ep 7 it 9	 PSNR SIDD: 12.6674	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 7 it 19	 PSNR SIDD: 12.4559	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 7 it 29	 PSNR SIDD: 12.9388	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 7 it 39	 PSNR SIDD: 12.2321	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 7	Time: 27.1368	Loss: 2.1647	LearningRate 0.000200
[Ep 8 it 9	 PSNR SIDD: 13.0471	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 8 it 19	 PSNR SIDD: 12.5803	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 8 it 29	 PSNR SIDD: 12.8675	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 8 it 39	 PSNR SIDD: 13.0239	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 8	Time: 26.6857	Loss: 2.1320	LearningRate 0.000200
[Ep 9 it 9	 PSNR SIDD: 12.2631	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 9 it 19	 PSNR SIDD: 12.5619	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 9 it 29	 PSNR SIDD: 13.1713	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 9 it 39	 PSNR SIDD: 12.3585	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 9	Time: 27.2171	Loss: 2.1371	LearningRate 0.000200
[Ep 10 it 9	 PSNR SIDD: 12.7387	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 10 it 19	 PSNR SIDD: 12.8843	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 10 it 29	 PSNR SIDD: 12.6076	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 10 it 39	 PSNR SIDD: 12.5873	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 10	Time: 26.2957	Loss: 2.1554	LearningRate 0.000200
[Ep 11 it 9	 PSNR SIDD: 12.9993	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 11 it 19	 PSNR SIDD: 12.9959	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 11 it 29	 PSNR SIDD: 12.7830	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 11 it 39	 PSNR SIDD: 12.6100	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 11	Time: 26.4338	Loss: 2.1070	LearningRate 0.000199
[Ep 12 it 9	 PSNR SIDD: 13.1276	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 12 it 19	 PSNR SIDD: 12.5094	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 12 it 29	 PSNR SIDD: 12.8970	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 12 it 39	 PSNR SIDD: 13.0855	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 12	Time: 26.2353	Loss: 2.0882	LearningRate 0.000199
[Ep 13 it 9	 PSNR SIDD: 12.2838	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 13 it 19	 PSNR SIDD: 12.6997	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 13 it 29	 PSNR SIDD: 12.9824	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 13 it 39	 PSNR SIDD: 12.5389	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 13	Time: 26.2993	Loss: 2.0907	LearningRate 0.000199
[Ep 14 it 9	 PSNR SIDD: 12.5066	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 14 it 19	 PSNR SIDD: 13.2116	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 14 it 29	 PSNR SIDD: 12.7955	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 14 it 39	 PSNR SIDD: 13.0269	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 14	Time: 27.3822	Loss: 2.0104	LearningRate 0.000199
[Ep 15 it 9	 PSNR SIDD: 12.8365	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 15 it 19	 PSNR SIDD: 12.5392	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 15 it 29	 PSNR SIDD: 13.2416	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 15 it 39	 PSNR SIDD: 12.4863	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 15	Time: 27.1500	Loss: 1.9996	LearningRate 0.000199
[Ep 16 it 9	 PSNR SIDD: 12.9375	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 16 it 19	 PSNR SIDD: 13.0071	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 16 it 29	 PSNR SIDD: 12.1929	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 16 it 39	 PSNR SIDD: 13.2050	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 16	Time: 27.1030	Loss: 1.9870	LearningRate 0.000198
[Ep 17 it 9	 PSNR SIDD: 12.8480	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 17 it 19	 PSNR SIDD: 13.0581	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 17 it 29	 PSNR SIDD: 12.9565	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 17 it 39	 PSNR SIDD: 13.1783	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 17	Time: 26.3225	Loss: 1.9665	LearningRate 0.000198
[Ep 18 it 9	 PSNR SIDD: 12.5781	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 18 it 19	 PSNR SIDD: 12.6702	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 18 it 29	 PSNR SIDD: 13.1148	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 18 it 39	 PSNR SIDD: 12.8965	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 18	Time: 27.2447	Loss: 1.9620	LearningRate 0.000198
[Ep 19 it 9	 PSNR SIDD: 12.9800	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 19 it 19	 PSNR SIDD: 13.3554	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 19 it 29	 PSNR SIDD: 12.3718	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 19 it 39	 PSNR SIDD: 13.0827	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 19	Time: 26.6862	Loss: 1.9345	LearningRate 0.000198
[Ep 20 it 9	 PSNR SIDD: 12.8176	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 20 it 19	 PSNR SIDD: 12.6561	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 20 it 29	 PSNR SIDD: 13.2060	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 20 it 39	 PSNR SIDD: 13.2921	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 20	Time: 26.2050	Loss: 2.0055	LearningRate 0.000197
[Ep 21 it 9	 PSNR SIDD: 12.7582	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 21 it 19	 PSNR SIDD: 13.0201	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 21 it 29	 PSNR SIDD: 12.8510	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 21 it 39	 PSNR SIDD: 13.2417	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 21	Time: 26.2632	Loss: 1.8940	LearningRate 0.000197
[Ep 22 it 9	 PSNR SIDD: 12.7466	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 22 it 19	 PSNR SIDD: 12.6813	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 22 it 29	 PSNR SIDD: 13.1719	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 22 it 39	 PSNR SIDD: 12.7836	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 22	Time: 26.1484	Loss: 1.9312	LearningRate 0.000197
[Ep 23 it 9	 PSNR SIDD: 12.9102	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 23 it 19	 PSNR SIDD: 12.6839	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 23 it 29	 PSNR SIDD: 13.2212	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 23 it 39	 PSNR SIDD: 12.4415	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 23	Time: 26.6864	Loss: 1.8574	LearningRate 0.000196
[Ep 24 it 9	 PSNR SIDD: 13.1864	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 24 it 19	 PSNR SIDD: 12.8084	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 24 it 29	 PSNR SIDD: 13.0114	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 24 it 39	 PSNR SIDD: 12.8402	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 24	Time: 27.5427	Loss: 1.8823	LearningRate 0.000196
[Ep 25 it 9	 PSNR SIDD: 13.4698	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 25 it 19	 PSNR SIDD: 12.1174	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 25 it 29	 PSNR SIDD: 12.9496	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 25 it 39	 PSNR SIDD: 13.0530	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 25	Time: 26.4133	Loss: 1.9681	LearningRate 0.000196
[Ep 26 it 9	 PSNR SIDD: 12.6568	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 26 it 19	 PSNR SIDD: 13.0063	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 26 it 29	 PSNR SIDD: 12.8641	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 26 it 39	 PSNR SIDD: 12.9680	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 26	Time: 26.5373	Loss: 1.8266	LearningRate 0.000195
[Ep 27 it 9	 PSNR SIDD: 12.5423	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 27 it 19	 PSNR SIDD: 13.2992	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 27 it 29	 PSNR SIDD: 13.2358	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 27 it 39	 PSNR SIDD: 12.3988	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 27	Time: 27.2204	Loss: 1.9226	LearningRate 0.000195
[Ep 28 it 9	 PSNR SIDD: 13.0685	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 28 it 19	 PSNR SIDD: 12.8735	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 28 it 29	 PSNR SIDD: 13.1090	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 28 it 39	 PSNR SIDD: 12.8393	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 28	Time: 26.6082	Loss: 1.8969	LearningRate 0.000195
[Ep 29 it 9	 PSNR SIDD: 12.9430	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 29 it 19	 PSNR SIDD: 12.7379	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 29 it 29	 PSNR SIDD: 13.0739	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 29 it 39	 PSNR SIDD: 12.6805	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 29	Time: 26.3006	Loss: 1.8397	LearningRate 0.000194
[Ep 30 it 9	 PSNR SIDD: 13.1470	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 30 it 19	 PSNR SIDD: 12.6399	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 30 it 29	 PSNR SIDD: 12.6388	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 30 it 39	 PSNR SIDD: 12.8475	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 30	Time: 27.5448	Loss: 1.8385	LearningRate 0.000194
[Ep 31 it 9	 PSNR SIDD: 12.5079	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 31 it 19	 PSNR SIDD: 13.1661	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 31 it 29	 PSNR SIDD: 12.7833	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 31 it 39	 PSNR SIDD: 12.7542	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 31	Time: 26.4602	Loss: 1.8265	LearningRate 0.000193
[Ep 32 it 9	 PSNR SIDD: 12.7511	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 32 it 19	 PSNR SIDD: 13.1307	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 32 it 29	 PSNR SIDD: 12.6086	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 32 it 39	 PSNR SIDD: 12.8899	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 32	Time: 26.2389	Loss: 1.9159	LearningRate 0.000193
[Ep 33 it 9	 PSNR SIDD: 12.8648	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 33 it 19	 PSNR SIDD: 12.9031	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 33 it 29	 PSNR SIDD: 13.1097	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 33 it 39	 PSNR SIDD: 12.9689	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 33	Time: 26.6904	Loss: 1.8467	LearningRate 0.000192
[Ep 34 it 9	 PSNR SIDD: 13.0006	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 34 it 19	 PSNR SIDD: 12.7202	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 34 it 29	 PSNR SIDD: 13.1213	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 34 it 39	 PSNR SIDD: 12.5949	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 34	Time: 27.1587	Loss: 1.8847	LearningRate 0.000192
[Ep 35 it 9	 PSNR SIDD: 12.9469	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 35 it 19	 PSNR SIDD: 12.5091	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 35 it 29	 PSNR SIDD: 12.8993	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 35 it 39	 PSNR SIDD: 12.9419	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 35	Time: 26.6509	Loss: 1.8446	LearningRate 0.000191
[Ep 36 it 9	 PSNR SIDD: 12.9350	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 36 it 19	 PSNR SIDD: 13.0954	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 36 it 29	 PSNR SIDD: 12.9769	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 36 it 39	 PSNR SIDD: 12.6574	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 36	Time: 27.3802	Loss: 1.8659	LearningRate 0.000191
[Ep 37 it 9	 PSNR SIDD: 12.9491	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 37 it 19	 PSNR SIDD: 12.8795	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 37 it 29	 PSNR SIDD: 12.6192	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 37 it 39	 PSNR SIDD: 12.9168	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 37	Time: 27.0724	Loss: 1.8118	LearningRate 0.000190
[Ep 38 it 9	 PSNR SIDD: 12.7974	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 38 it 19	 PSNR SIDD: 13.1503	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 38 it 29	 PSNR SIDD: 12.7531	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 38 it 39	 PSNR SIDD: 12.5395	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 38	Time: 26.9198	Loss: 1.8489	LearningRate 0.000190
[Ep 39 it 9	 PSNR SIDD: 12.9103	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 39 it 19	 PSNR SIDD: 12.5055	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 39 it 29	 PSNR SIDD: 12.9961	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 39 it 39	 PSNR SIDD: 12.6865	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 39	Time: 26.9374	Loss: 1.8271	LearningRate 0.000189
[Ep 40 it 9	 PSNR SIDD: 13.0093	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 40 it 19	 PSNR SIDD: 12.7021	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 40 it 29	 PSNR SIDD: 13.3141	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 40 it 39	 PSNR SIDD: 12.1321	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 40	Time: 26.9104	Loss: 1.8544	LearningRate 0.000189
[Ep 41 it 9	 PSNR SIDD: 13.0946	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 41 it 19	 PSNR SIDD: 12.4443	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 41 it 29	 PSNR SIDD: 12.5383	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 41 it 39	 PSNR SIDD: 12.7875	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 41	Time: 26.8567	Loss: 1.8602	LearningRate 0.000188
[Ep 42 it 9	 PSNR SIDD: 12.6657	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 42 it 19	 PSNR SIDD: 12.8412	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 42 it 29	 PSNR SIDD: 12.6395	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 42 it 39	 PSNR SIDD: 12.9016	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 42	Time: 26.1382	Loss: 1.7667	LearningRate 0.000187
[Ep 43 it 9	 PSNR SIDD: 12.4044	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 43 it 19	 PSNR SIDD: 13.3228	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 43 it 29	 PSNR SIDD: 12.4975	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 43 it 39	 PSNR SIDD: 12.6722	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 43	Time: 26.8136	Loss: 1.8488	LearningRate 0.000187
[Ep 44 it 9	 PSNR SIDD: 12.8305	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 44 it 19	 PSNR SIDD: 12.6501	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 44 it 29	 PSNR SIDD: 12.9717	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 44 it 39	 PSNR SIDD: 12.8130	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 44	Time: 27.0317	Loss: 1.7938	LearningRate 0.000186
[Ep 45 it 9	 PSNR SIDD: 12.5975	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 45 it 19	 PSNR SIDD: 12.6970	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 45 it 29	 PSNR SIDD: 12.7546	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 45 it 39	 PSNR SIDD: 12.8644	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 45	Time: 26.6461	Loss: 1.7735	LearningRate 0.000186
[Ep 46 it 9	 PSNR SIDD: 12.7851	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 46 it 19	 PSNR SIDD: 12.4989	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 46 it 29	 PSNR SIDD: 12.6637	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 46 it 39	 PSNR SIDD: 12.7757	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 46	Time: 26.3214	Loss: 1.7965	LearningRate 0.000185
[Ep 47 it 9	 PSNR SIDD: 12.4021	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 47 it 19	 PSNR SIDD: 13.0238	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 47 it 29	 PSNR SIDD: 12.6839	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 47 it 39	 PSNR SIDD: 13.0718	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 47	Time: 26.6626	Loss: 1.8025	LearningRate 0.000184
[Ep 48 it 9	 PSNR SIDD: 12.6855	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 48 it 19	 PSNR SIDD: 12.3673	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 48 it 29	 PSNR SIDD: 13.3242	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 48 it 39	 PSNR SIDD: 12.4795	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 48	Time: 27.2257	Loss: 1.8163	LearningRate 0.000183
[Ep 49 it 9	 PSNR SIDD: 13.0308	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 49 it 19	 PSNR SIDD: 12.8492	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 49 it 29	 PSNR SIDD: 12.7131	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 49 it 39	 PSNR SIDD: 12.7171	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 49	Time: 26.6068	Loss: 1.8412	LearningRate 0.000183
[Ep 50 it 9	 PSNR SIDD: 12.8905	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 50 it 19	 PSNR SIDD: 12.8768	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 50 it 29	 PSNR SIDD: 12.7836	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
[Ep 50 it 39	 PSNR SIDD: 12.6058	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 13.5555] 
Epoch: 50	Time: 26.5473	Loss: 1.8010	LearningRate 0.000182
