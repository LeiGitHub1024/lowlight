Namespace(arch='Uformer', att_se=False, batch_size=32, checkpoint=50, dataset='SIDD', embed_dim=32, env='32_0701_1', eval_workers=8, global_skip=False, gpu='0', local_skip=False, lr_initial=0.0002, mode='denoising', nepoch=100, norm_layer='nn.LayerNorm', optimizer='adamw', pretrain_weights='./log/Uformer32/models/model_best.pth', resume=False, save_dir='/home/ma-user/work/deNoTr/log', save_images=False, token_mlp='leff', token_projection='linear', train_dir='../datasets/delowlight/lol/train', train_ps=128, train_workers=16, val_dir='../datasets/delowlight/lol/valid', vit_depth=12, vit_dim=256, vit_mlp_dim=512, vit_nheads=8, vit_patch_size=16, vit_share=False, warmup=True, warmup_epochs=3, weight_decay=0.02, win_size=8)
Uformer(
  embed_dim=32, token_projection=linear, token_mlp=leff,win_size=8
  (pos_drop): Dropout(p=0.0, inplace=False)
  (input_proj): InputProj(
    (proj): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): LeakyReLU(negative_slope=0.01, inplace=True)
    )
  )
  (output_proj): OutputProj(
    (proj): Sequential(
      (0): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (encoderlayer_0): BasicUformerLayer(
    dim=32, input_resolution=(128, 128), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=32, input_resolution=(128, 128), num_heads=1, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=32, win_size=(8, 8), num_heads=1
          (qkv): LinearProjection(
            (to_q): Linear(in_features=32, out_features=32, bias=True)
            (to_kv): Linear(in_features=32, out_features=64, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=32, out_features=32, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=32, out_features=128, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=128, out_features=32, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=32, input_resolution=(128, 128), num_heads=1, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=32, win_size=(8, 8), num_heads=1
          (qkv): LinearProjection(
            (to_q): Linear(in_features=32, out_features=32, bias=True)
            (to_kv): Linear(in_features=32, out_features=64, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=32, out_features=32, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=32, out_features=128, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=128, out_features=32, bias=True)
          )
        )
      )
    )
  )
  (dowsample_0): Downsample(
    (conv): Sequential(
      (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (encoderlayer_1): BasicUformerLayer(
    dim=64, input_resolution=(64, 64), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=64, input_resolution=(64, 64), num_heads=2, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=64, win_size=(8, 8), num_heads=2
          (qkv): LinearProjection(
            (to_q): Linear(in_features=64, out_features=64, bias=True)
            (to_kv): Linear(in_features=64, out_features=128, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=64, out_features=256, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=256, out_features=64, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=64, input_resolution=(64, 64), num_heads=2, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=64, win_size=(8, 8), num_heads=2
          (qkv): LinearProjection(
            (to_q): Linear(in_features=64, out_features=64, bias=True)
            (to_kv): Linear(in_features=64, out_features=128, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=64, out_features=256, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=256, out_features=64, bias=True)
          )
        )
      )
    )
  )
  (dowsample_1): Downsample(
    (conv): Sequential(
      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (encoderlayer_2): BasicUformerLayer(
    dim=128, input_resolution=(32, 32), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=128, input_resolution=(32, 32), num_heads=4, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=128, win_size=(8, 8), num_heads=4
          (qkv): LinearProjection(
            (to_q): Linear(in_features=128, out_features=128, bias=True)
            (to_kv): Linear(in_features=128, out_features=256, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=128, out_features=512, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=128, input_resolution=(32, 32), num_heads=4, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=128, win_size=(8, 8), num_heads=4
          (qkv): LinearProjection(
            (to_q): Linear(in_features=128, out_features=128, bias=True)
            (to_kv): Linear(in_features=128, out_features=256, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=128, out_features=512, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
          )
        )
      )
    )
  )
  (dowsample_2): Downsample(
    (conv): Sequential(
      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (encoderlayer_3): BasicUformerLayer(
    dim=256, input_resolution=(16, 16), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=256, input_resolution=(16, 16), num_heads=8, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=256, win_size=(8, 8), num_heads=8
          (qkv): LinearProjection(
            (to_q): Linear(in_features=256, out_features=256, bias=True)
            (to_kv): Linear(in_features=256, out_features=512, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=1024, out_features=256, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=256, input_resolution=(16, 16), num_heads=8, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=256, win_size=(8, 8), num_heads=8
          (qkv): LinearProjection(
            (to_q): Linear(in_features=256, out_features=256, bias=True)
            (to_kv): Linear(in_features=256, out_features=512, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=1024, out_features=256, bias=True)
          )
        )
      )
    )
  )
  (dowsample_3): Downsample(
    (conv): Sequential(
      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (conv): BasicUformerLayer(
    dim=512, input_resolution=(8, 8), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=512, input_resolution=(8, 8), num_heads=16, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=512, win_size=(8, 8), num_heads=16
          (qkv): LinearProjection(
            (to_q): Linear(in_features=512, out_features=512, bias=True)
            (to_kv): Linear(in_features=512, out_features=1024, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=512, out_features=2048, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=512, input_resolution=(8, 8), num_heads=16, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=512, win_size=(8, 8), num_heads=16
          (qkv): LinearProjection(
            (to_q): Linear(in_features=512, out_features=512, bias=True)
            (to_kv): Linear(in_features=512, out_features=1024, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=512, out_features=2048, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
      )
    )
  )
  (upsample_0): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (decoderlayer_0): BasicUformerLayer(
    dim=512, input_resolution=(16, 16), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=512, input_resolution=(16, 16), num_heads=16, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=512, win_size=(8, 8), num_heads=16
          (qkv): LinearProjection(
            (to_q): Linear(in_features=512, out_features=512, bias=True)
            (to_kv): Linear(in_features=512, out_features=1024, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=512, out_features=2048, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=512, input_resolution=(16, 16), num_heads=16, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=512, win_size=(8, 8), num_heads=16
          (qkv): LinearProjection(
            (to_q): Linear(in_features=512, out_features=512, bias=True)
            (to_kv): Linear(in_features=512, out_features=1024, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=512, out_features=2048, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
      )
    )
  )
  (upsample_1): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(512, 128, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (decoderlayer_1): BasicUformerLayer(
    dim=256, input_resolution=(32, 32), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=256, input_resolution=(32, 32), num_heads=8, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=256, win_size=(8, 8), num_heads=8
          (qkv): LinearProjection(
            (to_q): Linear(in_features=256, out_features=256, bias=True)
            (to_kv): Linear(in_features=256, out_features=512, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=1024, out_features=256, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=256, input_resolution=(32, 32), num_heads=8, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=256, win_size=(8, 8), num_heads=8
          (qkv): LinearProjection(
            (to_q): Linear(in_features=256, out_features=256, bias=True)
            (to_kv): Linear(in_features=256, out_features=512, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=1024, out_features=256, bias=True)
          )
        )
      )
    )
  )
  (upsample_2): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (decoderlayer_2): BasicUformerLayer(
    dim=128, input_resolution=(64, 64), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=128, input_resolution=(64, 64), num_heads=4, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=128, win_size=(8, 8), num_heads=4
          (qkv): LinearProjection(
            (to_q): Linear(in_features=128, out_features=128, bias=True)
            (to_kv): Linear(in_features=128, out_features=256, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=128, out_features=512, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=128, input_resolution=(64, 64), num_heads=4, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=128, win_size=(8, 8), num_heads=4
          (qkv): LinearProjection(
            (to_q): Linear(in_features=128, out_features=128, bias=True)
            (to_kv): Linear(in_features=128, out_features=256, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=128, out_features=512, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
          )
        )
      )
    )
  )
  (upsample_3): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(128, 32, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (decoderlayer_3): BasicUformerLayer(
    dim=64, input_resolution=(128, 128), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=64, input_resolution=(128, 128), num_heads=2, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=64, win_size=(8, 8), num_heads=2
          (qkv): LinearProjection(
            (to_q): Linear(in_features=64, out_features=64, bias=True)
            (to_kv): Linear(in_features=64, out_features=128, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=64, out_features=256, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=256, out_features=64, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=64, input_resolution=(128, 128), num_heads=2, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=64, win_size=(8, 8), num_heads=2
          (qkv): LinearProjection(
            (to_q): Linear(in_features=64, out_features=64, bias=True)
            (to_kv): Linear(in_features=64, out_features=128, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=64, out_features=256, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=256, out_features=64, bias=True)
          )
        )
      )
    )
  )
)
[Ep 1 it 14	 PSNR SIDD: 12.6020	] ----  [best_Ep_SIDD 1 best_it_SIDD 14 Best_PSNR_SIDD 12.6020] 
[Ep 1 it 29	 PSNR SIDD: 14.3276	] ----  [best_Ep_SIDD 1 best_it_SIDD 29 Best_PSNR_SIDD 14.3276] 
[Ep 1 it 44	 PSNR SIDD: 13.3526	] ----  [best_Ep_SIDD 1 best_it_SIDD 29 Best_PSNR_SIDD 14.3276] 
[Ep 1 it 59	 PSNR SIDD: 13.6058	] ----  [best_Ep_SIDD 1 best_it_SIDD 29 Best_PSNR_SIDD 14.3276] 
Epoch: 1	Time: 53.4463	Loss: 170.3181	LearningRate 0.000133
[Ep 2 it 14	 PSNR SIDD: 14.9918	] ----  [best_Ep_SIDD 2 best_it_SIDD 14 Best_PSNR_SIDD 14.9918] 
[Ep 2 it 29	 PSNR SIDD: 15.0823	] ----  [best_Ep_SIDD 2 best_it_SIDD 29 Best_PSNR_SIDD 15.0823] 
[Ep 2 it 44	 PSNR SIDD: 14.9218	] ----  [best_Ep_SIDD 2 best_it_SIDD 29 Best_PSNR_SIDD 15.0823] 
[Ep 2 it 59	 PSNR SIDD: 14.4698	] ----  [best_Ep_SIDD 2 best_it_SIDD 29 Best_PSNR_SIDD 15.0823] 
Epoch: 2	Time: 51.1809	Loss: 164.7902	LearningRate 0.000200
[Ep 3 it 14	 PSNR SIDD: 15.4597	] ----  [best_Ep_SIDD 3 best_it_SIDD 14 Best_PSNR_SIDD 15.4597] 
[Ep 3 it 29	 PSNR SIDD: 15.5322	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 15.5322] 
[Ep 3 it 44	 PSNR SIDD: 14.5204	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 15.5322] 
[Ep 3 it 59	 PSNR SIDD: 8.8100	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 15.5322] 
Epoch: 3	Time: 51.4107	Loss: 173.1959	LearningRate 0.000200
[Ep 4 it 14	 PSNR SIDD: 9.5509	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 15.5322] 
[Ep 4 it 29	 PSNR SIDD: 10.5443	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 15.5322] 
[Ep 4 it 44	 PSNR SIDD: 10.4835	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 15.5322] 
[Ep 4 it 59	 PSNR SIDD: 9.8610	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 15.5322] 
Epoch: 4	Time: 49.8342	Loss: 194.4055	LearningRate 0.000200
[Ep 5 it 14	 PSNR SIDD: 11.1632	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 15.5322] 
[Ep 5 it 29	 PSNR SIDD: 12.0464	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 15.5322] 
[Ep 5 it 44	 PSNR SIDD: 11.2757	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 15.5322] 
[Ep 5 it 59	 PSNR SIDD: 12.3009	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 15.5322] 
Epoch: 5	Time: 49.9172	Loss: 182.9754	LearningRate 0.000200
[Ep 6 it 14	 PSNR SIDD: 11.3814	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 15.5322] 
[Ep 6 it 29	 PSNR SIDD: 11.4419	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 15.5322] 
[Ep 6 it 44	 PSNR SIDD: 12.0771	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 15.5322] 
[Ep 6 it 59	 PSNR SIDD: 11.4786	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 15.5322] 
Epoch: 6	Time: 51.0086	Loss: 179.1528	LearningRate 0.000199
[Ep 7 it 14	 PSNR SIDD: 13.3275	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 15.5322] 
[Ep 7 it 29	 PSNR SIDD: 15.3373	] ----  [best_Ep_SIDD 3 best_it_SIDD 29 Best_PSNR_SIDD 15.5322] 
[Ep 7 it 44	 PSNR SIDD: 15.9426	] ----  [best_Ep_SIDD 7 best_it_SIDD 44 Best_PSNR_SIDD 15.9426] 
[Ep 7 it 59	 PSNR SIDD: 16.2009	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
Epoch: 7	Time: 51.7191	Loss: 165.7785	LearningRate 0.000199
[Ep 8 it 14	 PSNR SIDD: 15.6654	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 8 it 29	 PSNR SIDD: 16.0637	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 8 it 44	 PSNR SIDD: 12.0670	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 8 it 59	 PSNR SIDD: 11.4839	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
Epoch: 8	Time: 50.8357	Loss: 167.3263	LearningRate 0.000198
[Ep 9 it 14	 PSNR SIDD: 8.3657	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 9 it 29	 PSNR SIDD: 8.5661	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 9 it 44	 PSNR SIDD: 6.5804	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 9 it 59	 PSNR SIDD: 5.3606	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
Epoch: 9	Time: 50.4745	Loss: 173.5412	LearningRate 0.000198
[Ep 10 it 14	 PSNR SIDD: 5.6754	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 10 it 29	 PSNR SIDD: 5.8430	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 10 it 44	 PSNR SIDD: 5.2083	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 10 it 59	 PSNR SIDD: 4.8823	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
Epoch: 10	Time: 51.7602	Loss: 187.5877	LearningRate 0.000197
[Ep 11 it 14	 PSNR SIDD: 4.9825	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 11 it 29	 PSNR SIDD: 5.0652	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 11 it 44	 PSNR SIDD: 5.1204	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 11 it 59	 PSNR SIDD: 5.1997	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
Epoch: 11	Time: 50.3811	Loss: 192.4890	LearningRate 0.000196
[Ep 12 it 14	 PSNR SIDD: 5.2483	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 12 it 29	 PSNR SIDD: 5.2634	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 12 it 44	 PSNR SIDD: 5.2727	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 12 it 59	 PSNR SIDD: 5.2837	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
Epoch: 12	Time: 50.8866	Loss: 190.9645	LearningRate 0.000195
[Ep 13 it 14	 PSNR SIDD: 5.3052	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 13 it 29	 PSNR SIDD: 5.4157	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 13 it 44	 PSNR SIDD: 5.5340	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 13 it 59	 PSNR SIDD: 5.5518	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
Epoch: 13	Time: 50.0432	Loss: 189.8379	LearningRate 0.000194
[Ep 14 it 14	 PSNR SIDD: 5.5584	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 14 it 29	 PSNR SIDD: 5.5593	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 14 it 44	 PSNR SIDD: 5.5649	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 14 it 59	 PSNR SIDD: 5.5733	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
Epoch: 14	Time: 50.2573	Loss: 188.6972	LearningRate 0.000193
[Ep 15 it 14	 PSNR SIDD: 5.5851	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 15 it 29	 PSNR SIDD: 5.6244	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 15 it 44	 PSNR SIDD: 5.8025	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 15 it 59	 PSNR SIDD: 5.8907	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
Epoch: 15	Time: 49.5545	Loss: 186.5174	LearningRate 0.000191
[Ep 16 it 14	 PSNR SIDD: 5.8916	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 16 it 29	 PSNR SIDD: 5.9003	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 16 it 44	 PSNR SIDD: 5.8828	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 16 it 59	 PSNR SIDD: 5.9098	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
Epoch: 16	Time: 50.5704	Loss: 178.2323	LearningRate 0.000190
[Ep 17 it 14	 PSNR SIDD: 5.9050	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 17 it 29	 PSNR SIDD: 5.8513	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 17 it 44	 PSNR SIDD: 5.9218	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 17 it 59	 PSNR SIDD: 5.9307	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
Epoch: 17	Time: 52.2293	Loss: 177.6510	LearningRate 0.000189
[Ep 18 it 14	 PSNR SIDD: 5.9350	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 18 it 29	 PSNR SIDD: 5.9369	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 18 it 44	 PSNR SIDD: 5.9375	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 18 it 59	 PSNR SIDD: 5.9274	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
Epoch: 18	Time: 50.1650	Loss: 176.1680	LearningRate 0.000187
[Ep 19 it 14	 PSNR SIDD: 5.9487	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 19 it 29	 PSNR SIDD: 5.9483	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 19 it 44	 PSNR SIDD: 5.9260	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 19 it 59	 PSNR SIDD: 5.9451	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
Epoch: 19	Time: 50.9412	Loss: 175.6039	LearningRate 0.000185
[Ep 20 it 14	 PSNR SIDD: 5.4982	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 20 it 29	 PSNR SIDD: 5.5047	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 20 it 44	 PSNR SIDD: 5.7129	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 20 it 59	 PSNR SIDD: 5.7726	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
Epoch: 20	Time: 49.8822	Loss: 188.6138	LearningRate 0.000184
[Ep 21 it 14	 PSNR SIDD: 5.9077	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 21 it 29	 PSNR SIDD: 5.9281	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 21 it 44	 PSNR SIDD: 5.9357	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 21 it 59	 PSNR SIDD: 5.7333	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
Epoch: 21	Time: 50.7744	Loss: 180.0683	LearningRate 0.000182
[Ep 22 it 14	 PSNR SIDD: 5.8408	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 22 it 29	 PSNR SIDD: 5.9307	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 22 it 44	 PSNR SIDD: 5.9389	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 22 it 59	 PSNR SIDD: 5.7562	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
Epoch: 22	Time: 50.9634	Loss: 180.6006	LearningRate 0.000180
[Ep 23 it 14	 PSNR SIDD: 5.8389	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 23 it 29	 PSNR SIDD: 5.9177	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 23 it 44	 PSNR SIDD: 5.9479	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 23 it 59	 PSNR SIDD: 5.9512	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
Epoch: 23	Time: 50.4269	Loss: 179.3756	LearningRate 0.000178
[Ep 24 it 14	 PSNR SIDD: 5.7023	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 24 it 29	 PSNR SIDD: 5.7693	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 24 it 44	 PSNR SIDD: 5.8770	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 24 it 59	 PSNR SIDD: 5.8754	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
Epoch: 24	Time: 50.7811	Loss: 184.7411	LearningRate 0.000176
[Ep 25 it 14	 PSNR SIDD: 5.9386	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 25 it 29	 PSNR SIDD: 5.9543	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 25 it 44	 PSNR SIDD: 5.9475	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 25 it 59	 PSNR SIDD: 5.9490	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
Epoch: 25	Time: 50.2833	Loss: 176.2076	LearningRate 0.000174
[Ep 26 it 14	 PSNR SIDD: 5.9152	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 26 it 29	 PSNR SIDD: 5.9431	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 26 it 44	 PSNR SIDD: 5.8756	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 26 it 59	 PSNR SIDD: 5.9486	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
Epoch: 26	Time: 51.4399	Loss: 179.6183	LearningRate 0.000172
[Ep 27 it 14	 PSNR SIDD: 5.9437	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 27 it 29	 PSNR SIDD: 5.9441	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 27 it 44	 PSNR SIDD: 5.8566	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 27 it 59	 PSNR SIDD: 5.9421	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
Epoch: 27	Time: 50.9396	Loss: 178.1026	LearningRate 0.000169
[Ep 28 it 14	 PSNR SIDD: 5.9499	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 28 it 29	 PSNR SIDD: 5.9473	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 28 it 44	 PSNR SIDD: 5.9203	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 28 it 59	 PSNR SIDD: 5.9307	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
Epoch: 28	Time: 50.8950	Loss: 173.7532	LearningRate 0.000167
[Ep 29 it 14	 PSNR SIDD: 5.9395	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 29 it 29	 PSNR SIDD: 5.9189	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 29 it 44	 PSNR SIDD: 5.9164	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 29 it 59	 PSNR SIDD: 5.9257	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
Epoch: 29	Time: 51.3286	Loss: 175.1698	LearningRate 0.000164
[Ep 30 it 14	 PSNR SIDD: 5.9476	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 30 it 29	 PSNR SIDD: 5.9195	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 30 it 44	 PSNR SIDD: 5.9165	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 30 it 59	 PSNR SIDD: 5.9533	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
Epoch: 30	Time: 50.4552	Loss: 175.8061	LearningRate 0.000162
[Ep 31 it 14	 PSNR SIDD: 5.9515	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 31 it 29	 PSNR SIDD: 5.9550	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 31 it 44	 PSNR SIDD: 5.9335	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 31 it 59	 PSNR SIDD: 5.9500	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
Epoch: 31	Time: 51.1678	Loss: 174.2601	LearningRate 0.000159
[Ep 32 it 14	 PSNR SIDD: 5.6421	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 32 it 29	 PSNR SIDD: 5.7933	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 32 it 44	 PSNR SIDD: 5.8684	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 32 it 59	 PSNR SIDD: 5.9399	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
Epoch: 32	Time: 50.9290	Loss: 182.7645	LearningRate 0.000157
[Ep 33 it 14	 PSNR SIDD: 5.0365	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 33 it 29	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 33 it 44	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 33 it 59	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
Epoch: 33	Time: 50.2268	Loss: 184.0819	LearningRate 0.000154
[Ep 34 it 14	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 34 it 29	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 34 it 44	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 34 it 59	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
Epoch: 34	Time: 49.6517	Loss: 186.7910	LearningRate 0.000151
[Ep 35 it 14	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 35 it 29	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 35 it 44	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 35 it 59	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
Epoch: 35	Time: 52.0977	Loss: 186.6803	LearningRate 0.000148
[Ep 36 it 14	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 36 it 29	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 36 it 44	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 36 it 59	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
Epoch: 36	Time: 51.3271	Loss: 186.7379	LearningRate 0.000146
[Ep 37 it 14	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 37 it 29	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 37 it 44	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 37 it 59	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
Epoch: 37	Time: 50.9543	Loss: 187.0429	LearningRate 0.000143
[Ep 38 it 14	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 38 it 29	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 38 it 44	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 38 it 59	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
Epoch: 38	Time: 49.9007	Loss: 186.4475	LearningRate 0.000140
[Ep 39 it 14	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 39 it 29	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 39 it 44	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 39 it 59	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
Epoch: 39	Time: 51.6200	Loss: 186.8855	LearningRate 0.000137
[Ep 40 it 14	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 40 it 29	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 40 it 44	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 40 it 59	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
Epoch: 40	Time: 50.3656	Loss: 186.6683	LearningRate 0.000134
[Ep 41 it 14	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 41 it 29	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 41 it 44	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 41 it 59	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
Epoch: 41	Time: 49.9486	Loss: 186.5642	LearningRate 0.000131
[Ep 42 it 14	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 42 it 29	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 42 it 44	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 42 it 59	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
Epoch: 42	Time: 50.3210	Loss: 186.7182	LearningRate 0.000128
[Ep 43 it 14	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 43 it 29	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 43 it 44	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 43 it 59	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
Epoch: 43	Time: 51.6662	Loss: 186.0259	LearningRate 0.000125
[Ep 44 it 14	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 44 it 29	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 44 it 44	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 44 it 59	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
Epoch: 44	Time: 52.6050	Loss: 187.2468	LearningRate 0.000121
[Ep 45 it 14	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 45 it 29	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 45 it 44	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 45 it 59	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
Epoch: 45	Time: 49.2186	Loss: 186.6881	LearningRate 0.000118
[Ep 46 it 14	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 46 it 29	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
[Ep 46 it 44	 PSNR SIDD: 4.3377	] ----  [best_Ep_SIDD 7 best_it_SIDD 59 Best_PSNR_SIDD 16.2009] 
