Namespace(arch='Uformer', att_se=False, batch_size=32, checkpoint=50, dataset='SIDD', embed_dim=32, env='32_1005_l110tv_31', eval_workers=8, global_skip=False, gpu='0', local_skip=False, lr_initial=0.0002, mode='denoising', nepoch=31, norm_layer='nn.LayerNorm', optimizer='adamw', pretrain_weights='./log/Uformer32/models/model_best.pth', resume=False, save_dir='/home/ma-user/work/deNoTr/log', save_images=False, token_mlp='leff', token_projection='linear', train_dir='../datasets/lol/train', train_ps=128, train_workers=16, val_dir='../datasets/lol/valid', vit_depth=12, vit_dim=256, vit_mlp_dim=512, vit_nheads=8, vit_patch_size=16, vit_share=False, warmup=True, warmup_epochs=3, weight_decay=0.02, win_size=8)
Uformer(
  embed_dim=32, token_projection=linear, token_mlp=leff,win_size=8
  (pos_drop): Dropout(p=0.0, inplace=False)
  (input_proj): InputProj(
    (proj): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): LeakyReLU(negative_slope=0.01, inplace=True)
    )
  )
  (output_proj): OutputProj(
    (proj): Sequential(
      (0): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (encoderlayer_0): BasicUformerLayer(
    dim=32, input_resolution=(128, 128), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=32, input_resolution=(128, 128), num_heads=1, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=32, win_size=(8, 8), num_heads=1
          (qkv): LinearProjection(
            (to_q): Linear(in_features=32, out_features=32, bias=True)
            (to_kv): Linear(in_features=32, out_features=64, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=32, out_features=32, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=32, out_features=128, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=128, out_features=32, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=32, input_resolution=(128, 128), num_heads=1, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=32, win_size=(8, 8), num_heads=1
          (qkv): LinearProjection(
            (to_q): Linear(in_features=32, out_features=32, bias=True)
            (to_kv): Linear(in_features=32, out_features=64, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=32, out_features=32, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=32, out_features=128, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=128, out_features=32, bias=True)
          )
        )
      )
    )
  )
  (dowsample_0): Downsample(
    (conv): Sequential(
      (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (encoderlayer_1): BasicUformerLayer(
    dim=64, input_resolution=(64, 64), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=64, input_resolution=(64, 64), num_heads=2, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=64, win_size=(8, 8), num_heads=2
          (qkv): LinearProjection(
            (to_q): Linear(in_features=64, out_features=64, bias=True)
            (to_kv): Linear(in_features=64, out_features=128, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=64, out_features=256, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=256, out_features=64, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=64, input_resolution=(64, 64), num_heads=2, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=64, win_size=(8, 8), num_heads=2
          (qkv): LinearProjection(
            (to_q): Linear(in_features=64, out_features=64, bias=True)
            (to_kv): Linear(in_features=64, out_features=128, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=64, out_features=256, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=256, out_features=64, bias=True)
          )
        )
      )
    )
  )
  (dowsample_1): Downsample(
    (conv): Sequential(
      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (encoderlayer_2): BasicUformerLayer(
    dim=128, input_resolution=(32, 32), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=128, input_resolution=(32, 32), num_heads=4, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=128, win_size=(8, 8), num_heads=4
          (qkv): LinearProjection(
            (to_q): Linear(in_features=128, out_features=128, bias=True)
            (to_kv): Linear(in_features=128, out_features=256, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=128, out_features=512, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=128, input_resolution=(32, 32), num_heads=4, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=128, win_size=(8, 8), num_heads=4
          (qkv): LinearProjection(
            (to_q): Linear(in_features=128, out_features=128, bias=True)
            (to_kv): Linear(in_features=128, out_features=256, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=128, out_features=512, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
          )
        )
      )
    )
  )
  (dowsample_2): Downsample(
    (conv): Sequential(
      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (encoderlayer_3): BasicUformerLayer(
    dim=256, input_resolution=(16, 16), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=256, input_resolution=(16, 16), num_heads=8, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=256, win_size=(8, 8), num_heads=8
          (qkv): LinearProjection(
            (to_q): Linear(in_features=256, out_features=256, bias=True)
            (to_kv): Linear(in_features=256, out_features=512, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=1024, out_features=256, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=256, input_resolution=(16, 16), num_heads=8, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=256, win_size=(8, 8), num_heads=8
          (qkv): LinearProjection(
            (to_q): Linear(in_features=256, out_features=256, bias=True)
            (to_kv): Linear(in_features=256, out_features=512, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=1024, out_features=256, bias=True)
          )
        )
      )
    )
  )
  (dowsample_3): Downsample(
    (conv): Sequential(
      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (conv): BasicUformerLayer(
    dim=512, input_resolution=(8, 8), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=512, input_resolution=(8, 8), num_heads=16, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=512, win_size=(8, 8), num_heads=16
          (qkv): LinearProjection(
            (to_q): Linear(in_features=512, out_features=512, bias=True)
            (to_kv): Linear(in_features=512, out_features=1024, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=512, out_features=2048, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=512, input_resolution=(8, 8), num_heads=16, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=512, win_size=(8, 8), num_heads=16
          (qkv): LinearProjection(
            (to_q): Linear(in_features=512, out_features=512, bias=True)
            (to_kv): Linear(in_features=512, out_features=1024, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=512, out_features=2048, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
      )
    )
  )
  (upsample_0): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (decoderlayer_0): BasicUformerLayer(
    dim=512, input_resolution=(16, 16), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=512, input_resolution=(16, 16), num_heads=16, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=512, win_size=(8, 8), num_heads=16
          (qkv): LinearProjection(
            (to_q): Linear(in_features=512, out_features=512, bias=True)
            (to_kv): Linear(in_features=512, out_features=1024, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=512, out_features=2048, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=512, input_resolution=(16, 16), num_heads=16, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=512, win_size=(8, 8), num_heads=16
          (qkv): LinearProjection(
            (to_q): Linear(in_features=512, out_features=512, bias=True)
            (to_kv): Linear(in_features=512, out_features=1024, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=512, out_features=2048, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
      )
    )
  )
  (upsample_1): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(512, 128, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (decoderlayer_1): BasicUformerLayer(
    dim=256, input_resolution=(32, 32), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=256, input_resolution=(32, 32), num_heads=8, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=256, win_size=(8, 8), num_heads=8
          (qkv): LinearProjection(
            (to_q): Linear(in_features=256, out_features=256, bias=True)
            (to_kv): Linear(in_features=256, out_features=512, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=1024, out_features=256, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=256, input_resolution=(32, 32), num_heads=8, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=256, win_size=(8, 8), num_heads=8
          (qkv): LinearProjection(
            (to_q): Linear(in_features=256, out_features=256, bias=True)
            (to_kv): Linear(in_features=256, out_features=512, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=1024, out_features=256, bias=True)
          )
        )
      )
    )
  )
  (upsample_2): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (decoderlayer_2): BasicUformerLayer(
    dim=128, input_resolution=(64, 64), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=128, input_resolution=(64, 64), num_heads=4, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=128, win_size=(8, 8), num_heads=4
          (qkv): LinearProjection(
            (to_q): Linear(in_features=128, out_features=128, bias=True)
            (to_kv): Linear(in_features=128, out_features=256, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=128, out_features=512, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=128, input_resolution=(64, 64), num_heads=4, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=128, win_size=(8, 8), num_heads=4
          (qkv): LinearProjection(
            (to_q): Linear(in_features=128, out_features=128, bias=True)
            (to_kv): Linear(in_features=128, out_features=256, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=128, out_features=512, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
          )
        )
      )
    )
  )
  (upsample_3): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(128, 32, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (decoderlayer_3): BasicUformerLayer(
    dim=64, input_resolution=(128, 128), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=64, input_resolution=(128, 128), num_heads=2, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=64, win_size=(8, 8), num_heads=2
          (qkv): LinearProjection(
            (to_q): Linear(in_features=64, out_features=64, bias=True)
            (to_kv): Linear(in_features=64, out_features=128, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=64, out_features=256, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=256, out_features=64, bias=True)
          )
        )
      )
      (1): LeWinTransformerBlock(
        dim=64, input_resolution=(128, 128), num_heads=2, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=64, win_size=(8, 8), num_heads=2
          (qkv): LinearProjection(
            (to_q): Linear(in_features=64, out_features=64, bias=True)
            (to_kv): Linear(in_features=64, out_features=128, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (linear1): Sequential(
            (0): Linear(in_features=64, out_features=256, bias=True)
            (1): GELU()
          )
          (dwconv): Sequential(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            (1): GELU()
          )
          (linear2): Sequential(
            (0): Linear(in_features=256, out_features=64, bias=True)
          )
        )
      )
    )
  )
)
[Ep 1 it 29	 PSNR SIDD: 9.9909	] ----  [best_Ep_SIDD 1 best_it_SIDD 29 Best_PSNR_SIDD 9.9909] 
[Ep 1 it 59	 PSNR SIDD: 10.6156	] ----  [best_Ep_SIDD 1 best_it_SIDD 59 Best_PSNR_SIDD 10.6156] 
[Ep 1 it 89	 PSNR SIDD: 15.2954	] ----  [best_Ep_SIDD 1 best_it_SIDD 89 Best_PSNR_SIDD 15.2954] 
[Ep 1 it 119	 PSNR SIDD: 16.1438	] ----  [best_Ep_SIDD 1 best_it_SIDD 119 Best_PSNR_SIDD 16.1438] 
Epoch: 1	Time: 113.8716	Loss: 27.6316	LearningRate 0.000133
[Ep 2 it 29	 PSNR SIDD: 16.6692	] ----  [best_Ep_SIDD 2 best_it_SIDD 29 Best_PSNR_SIDD 16.6692] 
[Ep 2 it 59	 PSNR SIDD: 17.2133	] ----  [best_Ep_SIDD 2 best_it_SIDD 59 Best_PSNR_SIDD 17.2133] 
[Ep 2 it 89	 PSNR SIDD: 17.6073	] ----  [best_Ep_SIDD 2 best_it_SIDD 89 Best_PSNR_SIDD 17.6073] 
[Ep 2 it 119	 PSNR SIDD: 17.1157	] ----  [best_Ep_SIDD 2 best_it_SIDD 89 Best_PSNR_SIDD 17.6073] 
Epoch: 2	Time: 106.5560	Loss: 18.4181	LearningRate 0.000200
[Ep 3 it 29	 PSNR SIDD: 17.0347	] ----  [best_Ep_SIDD 2 best_it_SIDD 89 Best_PSNR_SIDD 17.6073] 
[Ep 3 it 59	 PSNR SIDD: 17.6058	] ----  [best_Ep_SIDD 2 best_it_SIDD 89 Best_PSNR_SIDD 17.6073] 
[Ep 3 it 89	 PSNR SIDD: 16.3772	] ----  [best_Ep_SIDD 2 best_it_SIDD 89 Best_PSNR_SIDD 17.6073] 
[Ep 3 it 119	 PSNR SIDD: 16.9166	] ----  [best_Ep_SIDD 2 best_it_SIDD 89 Best_PSNR_SIDD 17.6073] 
Epoch: 3	Time: 100.7000	Loss: 17.0450	LearningRate 0.000200
[Ep 4 it 29	 PSNR SIDD: 17.8830	] ----  [best_Ep_SIDD 4 best_it_SIDD 29 Best_PSNR_SIDD 17.8830] 
[Ep 4 it 59	 PSNR SIDD: 17.4819	] ----  [best_Ep_SIDD 4 best_it_SIDD 29 Best_PSNR_SIDD 17.8830] 
[Ep 4 it 89	 PSNR SIDD: 17.4149	] ----  [best_Ep_SIDD 4 best_it_SIDD 29 Best_PSNR_SIDD 17.8830] 
[Ep 4 it 119	 PSNR SIDD: 17.3748	] ----  [best_Ep_SIDD 4 best_it_SIDD 29 Best_PSNR_SIDD 17.8830] 
Epoch: 4	Time: 103.9646	Loss: 16.0777	LearningRate 0.000199
[Ep 5 it 29	 PSNR SIDD: 17.0051	] ----  [best_Ep_SIDD 4 best_it_SIDD 29 Best_PSNR_SIDD 17.8830] 
[Ep 5 it 59	 PSNR SIDD: 17.1703	] ----  [best_Ep_SIDD 4 best_it_SIDD 29 Best_PSNR_SIDD 17.8830] 
[Ep 5 it 89	 PSNR SIDD: 17.6318	] ----  [best_Ep_SIDD 4 best_it_SIDD 29 Best_PSNR_SIDD 17.8830] 
[Ep 5 it 119	 PSNR SIDD: 17.2778	] ----  [best_Ep_SIDD 4 best_it_SIDD 29 Best_PSNR_SIDD 17.8830] 
Epoch: 5	Time: 101.5237	Loss: 15.4620	LearningRate 0.000196
[Ep 6 it 29	 PSNR SIDD: 17.2310	] ----  [best_Ep_SIDD 4 best_it_SIDD 29 Best_PSNR_SIDD 17.8830] 
[Ep 6 it 59	 PSNR SIDD: 17.3812	] ----  [best_Ep_SIDD 4 best_it_SIDD 29 Best_PSNR_SIDD 17.8830] 
[Ep 6 it 89	 PSNR SIDD: 17.3037	] ----  [best_Ep_SIDD 4 best_it_SIDD 29 Best_PSNR_SIDD 17.8830] 
[Ep 6 it 119	 PSNR SIDD: 17.4157	] ----  [best_Ep_SIDD 4 best_it_SIDD 29 Best_PSNR_SIDD 17.8830] 
Epoch: 6	Time: 102.1484	Loss: 12.9101	LearningRate 0.000191
[Ep 7 it 29	 PSNR SIDD: 17.5463	] ----  [best_Ep_SIDD 4 best_it_SIDD 29 Best_PSNR_SIDD 17.8830] 
[Ep 7 it 59	 PSNR SIDD: 17.4058	] ----  [best_Ep_SIDD 4 best_it_SIDD 29 Best_PSNR_SIDD 17.8830] 
[Ep 7 it 89	 PSNR SIDD: 17.5407	] ----  [best_Ep_SIDD 4 best_it_SIDD 29 Best_PSNR_SIDD 17.8830] 
[Ep 7 it 119	 PSNR SIDD: 17.7239	] ----  [best_Ep_SIDD 4 best_it_SIDD 29 Best_PSNR_SIDD 17.8830] 
Epoch: 7	Time: 100.5760	Loss: 12.8643	LearningRate 0.000186
[Ep 8 it 29	 PSNR SIDD: 17.1301	] ----  [best_Ep_SIDD 4 best_it_SIDD 29 Best_PSNR_SIDD 17.8830] 
[Ep 8 it 59	 PSNR SIDD: 17.8576	] ----  [best_Ep_SIDD 4 best_it_SIDD 29 Best_PSNR_SIDD 17.8830] 
[Ep 8 it 89	 PSNR SIDD: 17.5328	] ----  [best_Ep_SIDD 4 best_it_SIDD 29 Best_PSNR_SIDD 17.8830] 
[Ep 8 it 119	 PSNR SIDD: 17.5280	] ----  [best_Ep_SIDD 4 best_it_SIDD 29 Best_PSNR_SIDD 17.8830] 
Epoch: 8	Time: 101.4424	Loss: 12.4112	LearningRate 0.000180
[Ep 9 it 29	 PSNR SIDD: 17.4476	] ----  [best_Ep_SIDD 4 best_it_SIDD 29 Best_PSNR_SIDD 17.8830] 
[Ep 9 it 59	 PSNR SIDD: 17.4815	] ----  [best_Ep_SIDD 4 best_it_SIDD 29 Best_PSNR_SIDD 17.8830] 
[Ep 9 it 89	 PSNR SIDD: 17.6391	] ----  [best_Ep_SIDD 4 best_it_SIDD 29 Best_PSNR_SIDD 17.8830] 
[Ep 9 it 119	 PSNR SIDD: 17.5562	] ----  [best_Ep_SIDD 4 best_it_SIDD 29 Best_PSNR_SIDD 17.8830] 
Epoch: 9	Time: 102.7373	Loss: 12.3663	LearningRate 0.000172
[Ep 10 it 29	 PSNR SIDD: 17.3488	] ----  [best_Ep_SIDD 4 best_it_SIDD 29 Best_PSNR_SIDD 17.8830] 
[Ep 10 it 59	 PSNR SIDD: 17.7887	] ----  [best_Ep_SIDD 4 best_it_SIDD 29 Best_PSNR_SIDD 17.8830] 
[Ep 10 it 89	 PSNR SIDD: 18.0303	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
[Ep 10 it 119	 PSNR SIDD: 17.7348	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
Epoch: 10	Time: 102.7249	Loss: 12.3876	LearningRate 0.000164
[Ep 11 it 29	 PSNR SIDD: 17.4637	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
[Ep 11 it 59	 PSNR SIDD: 17.1505	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
[Ep 11 it 89	 PSNR SIDD: 17.6184	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
[Ep 11 it 119	 PSNR SIDD: 17.4786	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
Epoch: 11	Time: 102.1086	Loss: 12.5759	LearningRate 0.000155
[Ep 12 it 29	 PSNR SIDD: 17.3667	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
[Ep 12 it 59	 PSNR SIDD: 17.2043	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
[Ep 12 it 89	 PSNR SIDD: 17.4226	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
[Ep 12 it 119	 PSNR SIDD: 17.0260	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
Epoch: 12	Time: 101.4012	Loss: 12.3088	LearningRate 0.000145
[Ep 13 it 29	 PSNR SIDD: 17.7244	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
[Ep 13 it 59	 PSNR SIDD: 17.5216	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
[Ep 13 it 89	 PSNR SIDD: 17.5611	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
[Ep 13 it 119	 PSNR SIDD: 17.3873	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
Epoch: 13	Time: 100.5082	Loss: 12.2295	LearningRate 0.000135
[Ep 14 it 29	 PSNR SIDD: 17.6232	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
[Ep 14 it 59	 PSNR SIDD: 17.7071	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
[Ep 14 it 89	 PSNR SIDD: 17.6094	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
[Ep 14 it 119	 PSNR SIDD: 17.5575	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
Epoch: 14	Time: 100.7906	Loss: 12.0580	LearningRate 0.000124
[Ep 15 it 29	 PSNR SIDD: 17.7790	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
[Ep 15 it 59	 PSNR SIDD: 17.3143	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
[Ep 15 it 89	 PSNR SIDD: 17.4181	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
[Ep 15 it 119	 PSNR SIDD: 17.5050	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
Epoch: 15	Time: 101.7841	Loss: 12.1794	LearningRate 0.000113
[Ep 16 it 29	 PSNR SIDD: 17.4976	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
[Ep 16 it 59	 PSNR SIDD: 17.2035	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
[Ep 16 it 89	 PSNR SIDD: 17.6286	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
[Ep 16 it 119	 PSNR SIDD: 17.7113	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
Epoch: 16	Time: 102.1716	Loss: 11.9273	LearningRate 0.000102
[Ep 17 it 29	 PSNR SIDD: 17.6409	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
[Ep 17 it 59	 PSNR SIDD: 17.9004	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
[Ep 17 it 89	 PSNR SIDD: 17.3674	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
[Ep 17 it 119	 PSNR SIDD: 17.6925	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
Epoch: 17	Time: 101.6188	Loss: 12.0179	LearningRate 0.000090
[Ep 18 it 29	 PSNR SIDD: 17.6552	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
[Ep 18 it 59	 PSNR SIDD: 17.7565	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
[Ep 18 it 89	 PSNR SIDD: 17.5205	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
[Ep 18 it 119	 PSNR SIDD: 17.6847	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
Epoch: 18	Time: 100.5255	Loss: 11.8909	LearningRate 0.000079
[Ep 19 it 29	 PSNR SIDD: 17.9271	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
[Ep 19 it 59	 PSNR SIDD: 17.5485	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
[Ep 19 it 89	 PSNR SIDD: 17.5180	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
[Ep 19 it 119	 PSNR SIDD: 17.7784	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
Epoch: 19	Time: 101.2133	Loss: 11.9083	LearningRate 0.000069
[Ep 20 it 29	 PSNR SIDD: 17.8910	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
[Ep 20 it 59	 PSNR SIDD: 17.8497	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
[Ep 20 it 89	 PSNR SIDD: 17.8383	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
[Ep 20 it 119	 PSNR SIDD: 17.8133	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
Epoch: 20	Time: 101.0531	Loss: 11.9426	LearningRate 0.000058
[Ep 21 it 29	 PSNR SIDD: 17.9292	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
[Ep 21 it 59	 PSNR SIDD: 17.8600	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
[Ep 21 it 89	 PSNR SIDD: 17.7824	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
[Ep 21 it 119	 PSNR SIDD: 17.8309	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
Epoch: 21	Time: 101.7234	Loss: 11.8455	LearningRate 0.000049
[Ep 22 it 29	 PSNR SIDD: 17.9607	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
[Ep 22 it 59	 PSNR SIDD: 17.7914	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
[Ep 22 it 89	 PSNR SIDD: 17.9227	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
[Ep 22 it 119	 PSNR SIDD: 17.5211	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
Epoch: 22	Time: 101.6144	Loss: 11.8264	LearningRate 0.000039
[Ep 23 it 29	 PSNR SIDD: 17.8535	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
[Ep 23 it 59	 PSNR SIDD: 17.9208	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
[Ep 23 it 89	 PSNR SIDD: 17.8537	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
[Ep 23 it 119	 PSNR SIDD: 18.0039	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
Epoch: 23	Time: 102.6026	Loss: 11.7270	LearningRate 0.000031
[Ep 24 it 29	 PSNR SIDD: 17.7803	] ----  [best_Ep_SIDD 10 best_it_SIDD 89 Best_PSNR_SIDD 18.0303] 
[Ep 24 it 59	 PSNR SIDD: 18.0581	] ----  [best_Ep_SIDD 24 best_it_SIDD 59 Best_PSNR_SIDD 18.0581] 
[Ep 24 it 89	 PSNR SIDD: 17.8331	] ----  [best_Ep_SIDD 24 best_it_SIDD 59 Best_PSNR_SIDD 18.0581] 
[Ep 24 it 119	 PSNR SIDD: 17.8521	] ----  [best_Ep_SIDD 24 best_it_SIDD 59 Best_PSNR_SIDD 18.0581] 
Epoch: 24	Time: 104.4043	Loss: 11.5438	LearningRate 0.000024
[Ep 25 it 29	 PSNR SIDD: 18.0133	] ----  [best_Ep_SIDD 24 best_it_SIDD 59 Best_PSNR_SIDD 18.0581] 
[Ep 25 it 59	 PSNR SIDD: 17.8867	] ----  [best_Ep_SIDD 24 best_it_SIDD 59 Best_PSNR_SIDD 18.0581] 
[Ep 25 it 89	 PSNR SIDD: 18.0536	] ----  [best_Ep_SIDD 24 best_it_SIDD 59 Best_PSNR_SIDD 18.0581] 
[Ep 25 it 119	 PSNR SIDD: 17.9458	] ----  [best_Ep_SIDD 24 best_it_SIDD 59 Best_PSNR_SIDD 18.0581] 
Epoch: 25	Time: 100.8586	Loss: 11.6584	LearningRate 0.000017
[Ep 26 it 29	 PSNR SIDD: 17.9399	] ----  [best_Ep_SIDD 24 best_it_SIDD 59 Best_PSNR_SIDD 18.0581] 
[Ep 26 it 59	 PSNR SIDD: 17.9699	] ----  [best_Ep_SIDD 24 best_it_SIDD 59 Best_PSNR_SIDD 18.0581] 
[Ep 26 it 89	 PSNR SIDD: 17.8157	] ----  [best_Ep_SIDD 24 best_it_SIDD 59 Best_PSNR_SIDD 18.0581] 
[Ep 26 it 119	 PSNR SIDD: 17.9593	] ----  [best_Ep_SIDD 24 best_it_SIDD 59 Best_PSNR_SIDD 18.0581] 
Epoch: 26	Time: 100.9595	Loss: 11.6048	LearningRate 0.000012
[Ep 27 it 29	 PSNR SIDD: 17.8594	] ----  [best_Ep_SIDD 24 best_it_SIDD 59 Best_PSNR_SIDD 18.0581] 
[Ep 27 it 59	 PSNR SIDD: 17.8669	] ----  [best_Ep_SIDD 24 best_it_SIDD 59 Best_PSNR_SIDD 18.0581] 
[Ep 27 it 89	 PSNR SIDD: 17.9486	] ----  [best_Ep_SIDD 24 best_it_SIDD 59 Best_PSNR_SIDD 18.0581] 
[Ep 27 it 119	 PSNR SIDD: 17.9621	] ----  [best_Ep_SIDD 24 best_it_SIDD 59 Best_PSNR_SIDD 18.0581] 
Epoch: 27	Time: 102.9523	Loss: 11.4583	LearningRate 0.000007
[Ep 28 it 29	 PSNR SIDD: 18.0306	] ----  [best_Ep_SIDD 24 best_it_SIDD 59 Best_PSNR_SIDD 18.0581] 
[Ep 28 it 59	 PSNR SIDD: 17.9535	] ----  [best_Ep_SIDD 24 best_it_SIDD 59 Best_PSNR_SIDD 18.0581] 
[Ep 28 it 89	 PSNR SIDD: 18.0293	] ----  [best_Ep_SIDD 24 best_it_SIDD 59 Best_PSNR_SIDD 18.0581] 
[Ep 28 it 119	 PSNR SIDD: 17.9505	] ----  [best_Ep_SIDD 24 best_it_SIDD 59 Best_PSNR_SIDD 18.0581] 
Epoch: 28	Time: 101.8230	Loss: 11.5625	LearningRate 0.000004
[Ep 29 it 29	 PSNR SIDD: 18.0172	] ----  [best_Ep_SIDD 24 best_it_SIDD 59 Best_PSNR_SIDD 18.0581] 
[Ep 29 it 59	 PSNR SIDD: 18.0271	] ----  [best_Ep_SIDD 24 best_it_SIDD 59 Best_PSNR_SIDD 18.0581] 
[Ep 29 it 89	 PSNR SIDD: 17.9368	] ----  [best_Ep_SIDD 24 best_it_SIDD 59 Best_PSNR_SIDD 18.0581] 
[Ep 29 it 119	 PSNR SIDD: 17.9608	] ----  [best_Ep_SIDD 24 best_it_SIDD 59 Best_PSNR_SIDD 18.0581] 
Epoch: 29	Time: 101.5362	Loss: 11.5231	LearningRate 0.000002
[Ep 30 it 29	 PSNR SIDD: 17.9505	] ----  [best_Ep_SIDD 24 best_it_SIDD 59 Best_PSNR_SIDD 18.0581] 
[Ep 30 it 59	 PSNR SIDD: 17.9760	] ----  [best_Ep_SIDD 24 best_it_SIDD 59 Best_PSNR_SIDD 18.0581] 
[Ep 30 it 89	 PSNR SIDD: 17.9886	] ----  [best_Ep_SIDD 24 best_it_SIDD 59 Best_PSNR_SIDD 18.0581] 
[Ep 30 it 119	 PSNR SIDD: 17.9645	] ----  [best_Ep_SIDD 24 best_it_SIDD 59 Best_PSNR_SIDD 18.0581] 
Epoch: 30	Time: 103.5679	Loss: 11.4947	LearningRate 0.000001
[Ep 31 it 29	 PSNR SIDD: 18.0037	] ----  [best_Ep_SIDD 24 best_it_SIDD 59 Best_PSNR_SIDD 18.0581] 
[Ep 31 it 59	 PSNR SIDD: 18.0038	] ----  [best_Ep_SIDD 24 best_it_SIDD 59 Best_PSNR_SIDD 18.0581] 
[Ep 31 it 89	 PSNR SIDD: 17.9978	] ----  [best_Ep_SIDD 24 best_it_SIDD 59 Best_PSNR_SIDD 18.0581] 
[Ep 31 it 119	 PSNR SIDD: 17.9867	] ----  [best_Ep_SIDD 24 best_it_SIDD 59 Best_PSNR_SIDD 18.0581] 
Epoch: 31	Time: 102.6149	Loss: 11.3563	LearningRate 0.000001
